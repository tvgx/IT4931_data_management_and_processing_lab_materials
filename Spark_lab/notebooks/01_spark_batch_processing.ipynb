{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 1: Spark Batch Processing\n",
        "\n",
        "## üéØ Objectives\n",
        "- Master Spark DataFrame operations\n",
        "- Learn data processing patterns\n",
        "- Understand performance optimization\n",
        "- Practice with real-world datasets\n",
        "\n",
        "## üìã Prerequisites\n",
        "- Spark cluster running\n",
        "- Basic Python knowledge\n",
        "- Understanding of SQL concepts\n",
        "\n",
        "## üèóÔ∏è Architecture Overview\n",
        "```\n",
        "Data Sources ‚Üí Spark DataFrame ‚Üí Transformations ‚Üí Actions ‚Üí Results\n",
        "     ‚Üì              ‚Üì                ‚Üì              ‚Üì\n",
        "   CSV/JSON    Select/Filter    GroupBy/Join    Collect/Write\n",
        "   Parquet     WithColumn      Aggregations    Database/File\n",
        "   Database    Drop/Rename     Window Functions\n",
        "```\n",
        "\n",
        "## üìä Sample Datasets\n",
        "- **Sales Data**: Transaction records with customer, product, timestamp\n",
        "- **Customer Data**: Demographics, preferences, purchase history  \n",
        "- **Product Catalog**: Product details, categories, pricing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (3.5.0)\n",
            "Requirement already satisfied: findspark in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (2.0.1)\n",
            "Requirement already satisfied: pandas in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (1.24.3)\n",
            "Requirement already satisfied: pyarrow in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (14.0.1)\n",
            "Requirement already satisfied: psycopg2-binary in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (2.9.9)\n",
            "Requirement already satisfied: sqlalchemy in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (2.0.23)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from sqlalchemy) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚úÖ Dependencies installed and imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install and Import Dependencies\n",
        "%pip install pyspark findspark pandas numpy pyarrow psycopg2-binary sqlalchemy\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import builtins  # Import builtins ƒë·ªÉ s·ª≠ d·ª•ng Python's built-in round()\n",
        "\n",
        "print(\"‚úÖ Dependencies installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkBatchProcessingLab\") \\\n",
        "    .master(\"spark://spark-master:7077\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/26 07:49:21 WARN Utils: Your hostname, DSAI-TrungTrans-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.20.10.2 instead (on interface en0)\n",
            "25/11/26 07:49:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/11/26 07:49:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Spark Session initialized successfully!\n",
            "üìä Spark Version: 3.5.0\n",
            "üîó Master URL: local\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Set log level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"üöÄ Spark Session initialized successfully!\")\n",
        "print(f\"üìä Spark Version: {spark.version}\")\n",
        "print(f\"üîó Master URL: {spark.sparkContext.master}\")\n",
        "#print(f\"üë• Available Executors: {spark.sparkContext.statusTracker().getExecutorInfos()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Creating sample datasets for Spark Batch Processing Lab...\n",
            "‚úÖ Sample data created:\n",
            "   üìä Sales records: 1000\n",
            "   üë• Customer records: 8\n",
            "   üì¶ Product records: 8\n"
          ]
        }
      ],
      "source": [
        "# Create Sample Data for Spark Lab\n",
        "print(\"üìä Creating sample datasets for Spark Batch Processing Lab...\")\n",
        "\n",
        "# Sample Sales Data\n",
        "sales_data = []\n",
        "products = ['Laptop', 'Phone', 'Tablet', 'Headphones', 'Camera', 'Monitor', 'Keyboard', 'Mouse']\n",
        "customers = ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry']\n",
        "categories = ['Electronics', 'Accessories', 'Computing']\n",
        "\n",
        "for i in range(1000):\n",
        "    sales_data.append({\n",
        "        'sale_id': f'SALE_{i+1:04d}',\n",
        "        'customer_name': random.choice(customers),\n",
        "        'product_name': random.choice(products),\n",
        "        'category': random.choice(categories),\n",
        "        'quantity': random.randint(1, 5),\n",
        "        'unit_price': __builtins__.round(random.uniform(50, 2000), 2),  # S·ª≠ d·ª•ng Python's built-in round()\n",
        "        'sale_date': (datetime.now() - timedelta(days=random.randint(0, 365))).strftime('%Y-%m-%d'),\n",
        "        'region': random.choice(['North', 'South', 'East', 'West', 'Central'])\n",
        "    })\n",
        "\n",
        "# Sample Customer Data\n",
        "customer_data = []\n",
        "for customer in customers:\n",
        "    customer_data.append({\n",
        "        'customer_name': customer,\n",
        "        'age': random.randint(25, 65),\n",
        "        'city': random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']),\n",
        "        'membership_level': random.choice(['Bronze', 'Silver', 'Gold', 'Platinum']),\n",
        "        'join_date': (datetime.now() - timedelta(days=random.randint(30, 1000))).strftime('%Y-%m-%d'),\n",
        "        'total_purchases': random.randint(5, 50)\n",
        "    })\n",
        "\n",
        "# Sample Product Data\n",
        "product_data = []\n",
        "for product in products:\n",
        "    product_data.append({\n",
        "        'product_name': product,\n",
        "        'category': random.choice(categories),\n",
        "        'brand': random.choice(['TechCorp', 'ElectroMax', 'DigitalPro', 'SmartTech']),\n",
        "        'cost_price': __builtins__.round(random.uniform(30, 1500), 2),  # S·ª≠ d·ª•ng Python's built-in round()\n",
        "        'in_stock': random.randint(0, 100),\n",
        "        'supplier': random.choice(['SupplierA', 'SupplierB', 'SupplierC'])\n",
        "    })\n",
        "\n",
        "print(f\"‚úÖ Sample data created:\")\n",
        "print(f\"   üìä Sales records: {len(sales_data)}\")\n",
        "print(f\"   üë• Customer records: {len(customer_data)}\")\n",
        "print(f\"   üì¶ Product records: {len(product_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Creating Spark DataFrames from sample data...\n",
            "\n",
            "üìä Sales DataFrame Schema:\n",
            "root\n",
            " |-- category: string (nullable = true)\n",
            " |-- customer_name: string (nullable = true)\n",
            " |-- product_name: string (nullable = true)\n",
            " |-- quantity: long (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- sale_date: string (nullable = true)\n",
            " |-- sale_id: string (nullable = true)\n",
            " |-- unit_price: double (nullable = true)\n",
            "\n",
            "\n",
            "üìä Sales DataFrame Sample:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------------+------------+--------+-------+----------+---------+----------+\n",
            "|category   |customer_name|product_name|quantity|region |sale_date |sale_id  |unit_price|\n",
            "+-----------+-------------+------------+--------+-------+----------+---------+----------+\n",
            "|Electronics|Diana        |Monitor     |4       |East   |2025-04-12|SALE_0001|1326.25   |\n",
            "|Computing  |Alice        |Headphones  |4       |West   |2025-02-02|SALE_0002|245.87    |\n",
            "|Accessories|Charlie      |Camera      |2       |East   |2025-01-15|SALE_0003|1787.48   |\n",
            "|Computing  |Alice        |Tablet      |5       |Central|2025-05-27|SALE_0004|1036.16   |\n",
            "|Accessories|Diana        |Camera      |5       |West   |2025-06-13|SALE_0005|1028.07   |\n",
            "+-----------+-------------+------------+--------+-------+----------+---------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "üë• Customers DataFrame Schema:\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- customer_name: string (nullable = true)\n",
            " |-- join_date: string (nullable = true)\n",
            " |-- membership_level: string (nullable = true)\n",
            " |-- total_purchases: long (nullable = true)\n",
            "\n",
            "\n",
            "üë• Customers DataFrame Sample:\n",
            "+---+-----------+-------------+----------+----------------+---------------+\n",
            "|age|city       |customer_name|join_date |membership_level|total_purchases|\n",
            "+---+-----------+-------------+----------+----------------+---------------+\n",
            "|36 |Phoenix    |Alice        |2024-04-19|Bronze          |10             |\n",
            "|65 |Chicago    |Bob          |2025-07-02|Bronze          |41             |\n",
            "|47 |Los Angeles|Charlie      |2025-05-09|Silver          |50             |\n",
            "|45 |Phoenix    |Diana        |2024-03-27|Platinum        |50             |\n",
            "|52 |New York   |Eve          |2024-08-01|Bronze          |30             |\n",
            "+---+-----------+-------------+----------+----------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "üì¶ Products DataFrame Schema:\n",
            "root\n",
            " |-- brand: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- cost_price: double (nullable = true)\n",
            " |-- in_stock: long (nullable = true)\n",
            " |-- product_name: string (nullable = true)\n",
            " |-- supplier: string (nullable = true)\n",
            "\n",
            "\n",
            "üì¶ Products DataFrame Sample:\n",
            "+----------+-----------+----------+--------+------------+---------+\n",
            "|brand     |category   |cost_price|in_stock|product_name|supplier |\n",
            "+----------+-----------+----------+--------+------------+---------+\n",
            "|DigitalPro|Accessories|1372.55   |28      |Laptop      |SupplierB|\n",
            "|TechCorp  |Electronics|1275.62   |36      |Phone       |SupplierB|\n",
            "|TechCorp  |Electronics|979.42    |67      |Tablet      |SupplierA|\n",
            "|SmartTech |Computing  |1207.89   |14      |Headphones  |SupplierC|\n",
            "|ElectroMax|Computing  |1144.43   |18      |Camera      |SupplierB|\n",
            "+----------+-----------+----------+--------+------------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "‚úÖ DataFrames created successfully!\n",
            "   üìä Sales: 1000 records\n",
            "   üë• Customers: 8 records\n",
            "   üì¶ Products: 8 records\n"
          ]
        }
      ],
      "source": [
        "# Create Spark DataFrames\n",
        "print(\"üîÑ Creating Spark DataFrames from sample data...\")\n",
        "\n",
        "# Convert to Spark DataFrames\n",
        "sales_df = spark.createDataFrame(sales_data)\n",
        "customers_df = spark.createDataFrame(customer_data)\n",
        "products_df = spark.createDataFrame(product_data)\n",
        "\n",
        "# Show schema and sample data\n",
        "print(\"\\nüìä Sales DataFrame Schema:\")\n",
        "sales_df.printSchema()\n",
        "\n",
        "print(\"\\nüìä Sales DataFrame Sample:\")\n",
        "sales_df.show(5, truncate=False)\n",
        "\n",
        "print(\"\\nüë• Customers DataFrame Schema:\")\n",
        "customers_df.printSchema()\n",
        "\n",
        "print(\"\\nüë• Customers DataFrame Sample:\")\n",
        "customers_df.show(5, truncate=False)\n",
        "\n",
        "print(\"\\nüì¶ Products DataFrame Schema:\")\n",
        "products_df.printSchema()\n",
        "\n",
        "print(\"\\nüì¶ Products DataFrame Sample:\")\n",
        "products_df.show(5, truncate=False)\n",
        "\n",
        "print(f\"\\n‚úÖ DataFrames created successfully!\")\n",
        "print(f\"   üìä Sales: {sales_df.count()} records\")\n",
        "print(f\"   üë• Customers: {customers_df.count()} records\") \n",
        "print(f\"   üì¶ Products: {products_df.count()} records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Basic DataFrame Operations\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Master DataFrame transformations\n",
        "- Learn filtering and selection patterns\n",
        "- Practice column operations\n",
        "- Understand DataFrame caching strategies\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **Transformations**: Lazy operations that build execution plan\n",
        "2. **Actions**: Operations that trigger computation\n",
        "3. **Column Operations**: Working with DataFrame columns\n",
        "4. **Caching**: Optimizing repeated operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Basic DataFrame Operations\n",
        "print(\"üîß Exercise 1: Basic DataFrame Operations\")\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Filtering Operations:\")\n",
        "print(\"   Filter sales with quantity > 2 and unit_price > 500\")\n",
        "\n",
        "filtered_sales = sales_df.filter(\n",
        "    (col(\"quantity\") > 2) & (col(\"unit_price\") > 500)\n",
        ")\n",
        "\n",
        "print(f\"   üìä Filtered records: {filtered_sales.count()}\")\n",
        "filtered_sales.show(5)\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Column Selection and Renaming:\")\n",
        "print(\"   Select specific columns and rename them\")\n",
        "\n",
        "selected_sales = sales_df.select(\n",
        "    col(\"sale_id\").alias(\"transaction_id\"),\n",
        "    col(\"customer_name\").alias(\"customer\"),\n",
        "    col(\"product_name\").alias(\"product\"),\n",
        "    col(\"quantity\"),\n",
        "    col(\"unit_price\").alias(\"price\")\n",
        ")\n",
        "\n",
        "selected_sales.show(5)\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Adding Calculated Columns:\")\n",
        "print(\"   Add total_amount column (quantity * unit_price)\")\n",
        "\n",
        "sales_with_total = sales_df.withColumn(\n",
        "    \"total_amount\", \n",
        "    col(\"quantity\") * col(\"unit_price\")\n",
        ").withColumn(\n",
        "    \"discount_applied\",\n",
        "    when(col(\"total_amount\") > 1000, col(\"total_amount\") * 0.1).otherwise(0)\n",
        ").withColumn(\n",
        "    \"final_amount\",\n",
        "    col(\"total_amount\") - col(\"discount_applied\")\n",
        ")\n",
        "\n",
        "sales_with_total.show(5)\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ Data Type Conversions:\")\n",
        "print(\"   Convert sale_date to proper date type\")\n",
        "\n",
        "sales_with_dates = sales_df.withColumn(\n",
        "    \"sale_date\", \n",
        "    to_date(col(\"sale_date\"), \"yyyy-MM-dd\")\n",
        ").withColumn(\n",
        "    \"sale_year\",\n",
        "    year(col(\"sale_date\"))\n",
        ").withColumn(\n",
        "    \"sale_month\", \n",
        "    month(col(\"sale_date\"))\n",
        ")\n",
        "\n",
        "sales_with_dates.select(\"sale_id\", \"sale_date\", \"sale_year\", \"sale_month\").show(5)\n",
        "\n",
        "print(\"\\n‚úÖ Basic DataFrame operations completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Aggregations and Grouping\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Master groupBy operations\n",
        "- Learn aggregation functions\n",
        "- Practice window functions\n",
        "- Understand data summarization patterns\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **GroupBy**: Grouping data by columns\n",
        "2. **Aggregations**: Sum, count, avg, min, max operations\n",
        "3. **Window Functions**: Advanced analytical functions\n",
        "4. **Pivoting**: Reshaping data for analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2: Aggregations and Grouping\n",
        "print(\"üìä Exercise 2: Aggregations and Grouping\")\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Basic Aggregations:\")\n",
        "print(\"   Calculate total sales by product\")\n",
        "\n",
        "product_sales = sales_df.groupBy(\"product_name\").agg(\n",
        "    sum(\"quantity\").alias(\"total_quantity\"),\n",
        "    sum(col(\"quantity\") * col(\"unit_price\")).alias(\"total_revenue\"),\n",
        "    avg(\"unit_price\").alias(\"avg_price\"),\n",
        "    count(\"*\").alias(\"sale_count\")\n",
        ").orderBy(desc(\"total_revenue\"))\n",
        "\n",
        "product_sales.show()\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Multi-level Grouping:\")\n",
        "print(\"   Sales by region and category\")\n",
        "\n",
        "region_category_sales = sales_df.groupBy(\"region\", \"category\").agg(\n",
        "    sum(col(\"quantity\") * col(\"unit_price\")).alias(\"total_revenue\"),\n",
        "    avg(col(\"quantity\") * col(\"unit_price\")).alias(\"avg_transaction_value\"),\n",
        "    count(\"*\").alias(\"transaction_count\")\n",
        ").orderBy(desc(\"total_revenue\"))\n",
        "\n",
        "region_category_sales.show()\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Window Functions:\")\n",
        "print(\"   Calculate running totals and rankings\")\n",
        "\n",
        "# Define window specification\n",
        "window_spec = Window.partitionBy(\"customer_name\").orderBy(\"sale_date\")\n",
        "\n",
        "# Add running totals and rankings\n",
        "sales_with_window = sales_df.withColumn(\n",
        "    \"running_total\", \n",
        "    sum(col(\"quantity\") * col(\"unit_price\")).over(window_spec)\n",
        ").withColumn(\n",
        "    \"transaction_rank\",\n",
        "    row_number().over(window_spec)\n",
        ").withColumn(\n",
        "    \"customer_avg_transaction\",\n",
        "    avg(col(\"quantity\") * col(\"unit_price\")).over(\n",
        "        Window.partitionBy(\"customer_name\")\n",
        "    )\n",
        ")\n",
        "\n",
        "sales_with_window.select(\n",
        "    \"customer_name\", \"sale_date\", \"quantity\", \"unit_price\", \n",
        "    \"running_total\", \"transaction_rank\", \"customer_avg_transaction\"\n",
        ").show(10)\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ Pivot Operations:\")\n",
        "print(\"   Pivot sales data by region\")\n",
        "\n",
        "pivot_sales = sales_df.groupBy(\"product_name\").pivot(\"region\").agg(\n",
        "    sum(col(\"quantity\") * col(\"unit_price\")).alias(\"revenue\")\n",
        ").fillna(0)\n",
        "\n",
        "pivot_sales.show()\n",
        "\n",
        "print(\"\\n‚úÖ Aggregations and grouping completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Joins and Data Integration\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Master different join types\n",
        "- Learn data integration patterns\n",
        "- Practice complex join operations\n",
        "- Understand join optimization\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **Inner Join**: Matching records from both tables\n",
        "2. **Left/Right Join**: Including all records from one side\n",
        "3. **Outer Join**: Including all records from both sides\n",
        "4. **Join Optimization**: Efficient join strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3: Joins and Data Integration\n",
        "print(\"üîó Exercise 3: Joins and Data Integration\")\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Inner Join:\")\n",
        "print(\"   Join sales with customer data\")\n",
        "\n",
        "sales_customers = sales_df.join(\n",
        "    customers_df, \n",
        "    sales_df.customer_name == customers_df.customer_name, \n",
        "    \"inner\"\n",
        ").select(\n",
        "    sales_df[\"*\"],\n",
        "    customers_df.age.alias(\"customer_age\"),\n",
        "    customers_df.city.alias(\"customer_city\"),\n",
        "    customers_df.membership_level.alias(\"customer_membership\")\n",
        ")\n",
        "\n",
        "print(f\"   üìä Joined records: {sales_customers.count()}\")\n",
        "sales_customers.show(5)\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Left Join:\")\n",
        "print(\"   Join sales with product data (include all sales)\")\n",
        "\n",
        "sales_products = sales_df.join(\n",
        "    products_df,\n",
        "    sales_df.product_name == products_df.product_name,\n",
        "    \"left\"\n",
        ").select(\n",
        "    sales_df[\"*\"],\n",
        "    products_df.brand.alias(\"product_brand\"),\n",
        "    products_df.cost_price.alias(\"product_cost\"),\n",
        "    products_df.in_stock.alias(\"current_stock\")\n",
        ")\n",
        "\n",
        "print(f\"   üìä Joined records: {sales_products.count()}\")\n",
        "sales_products.show(5)\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Complex Multi-table Join:\")\n",
        "print(\"   Join sales, customers, and products\")\n",
        "\n",
        "complete_sales = sales_df.join(\n",
        "    customers_df,\n",
        "    sales_df.customer_name == customers_df.customer_name,\n",
        "    \"inner\"\n",
        ").join(\n",
        "    products_df,\n",
        "    sales_df.product_name == products_df.product_name,\n",
        "    \"inner\"\n",
        ").select(\n",
        "    sales_df.sale_id,\n",
        "    sales_df.customer_name,\n",
        "    sales_df.product_name,\n",
        "    sales_df.quantity,\n",
        "    sales_df.unit_price,\n",
        "    (sales_df.quantity * sales_df.unit_price).alias(\"total_amount\"),\n",
        "    customers_df.age,\n",
        "    customers_df.city,\n",
        "    customers_df.membership_level,\n",
        "    products_df.brand,\n",
        "    products_df.cost_price,\n",
        "    (sales_df.quantity * sales_df.unit_price - sales_df.quantity * products_df.cost_price).alias(\"profit\")\n",
        ")\n",
        "\n",
        "print(f\"   üìä Complete joined records: {complete_sales.count()}\")\n",
        "complete_sales.show(5)\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ Join Analysis:\")\n",
        "print(\"   Analyze profit by customer membership level\")\n",
        "\n",
        "profit_by_membership = complete_sales.groupBy(\"membership_level\").agg(\n",
        "    sum(\"total_amount\").alias(\"total_revenue\"),\n",
        "    sum(\"profit\").alias(\"total_profit\"),\n",
        "    avg(\"profit\").alias(\"avg_profit_per_transaction\"),\n",
        "    count(\"*\").alias(\"transaction_count\")\n",
        ").orderBy(desc(\"total_profit\"))\n",
        "\n",
        "profit_by_membership.show()\n",
        "\n",
        "print(\"\\n‚úÖ Joins and data integration completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Performance Optimization\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Learn DataFrame caching strategies\n",
        "- Understand partitioning concepts\n",
        "- Practice performance monitoring\n",
        "- Master optimization techniques\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **Caching**: Storing DataFrames in memory\n",
        "2. **Partitioning**: Data distribution strategies\n",
        "3. **Broadcast Joins**: Optimizing small table joins\n",
        "4. **Performance Monitoring**: Tracking execution metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4: Performance Optimization\n",
        "print(\"‚ö° Exercise 4: Performance Optimization\")\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ DataFrame Caching:\")\n",
        "print(\"   Cache frequently used DataFrames\")\n",
        "\n",
        "# Cache the complete sales DataFrame for multiple operations\n",
        "complete_sales.cache()\n",
        "print(\"   üìä Cached complete_sales DataFrame\")\n",
        "\n",
        "# Test cache performance\n",
        "import time\n",
        "start_time = time.time()\n",
        "count1 = complete_sales.count()\n",
        "first_run_time = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "count2 = complete_sales.count()\n",
        "second_run_time = time.time() - start_time\n",
        "\n",
        "print(f\"   ‚è±Ô∏è First count: {first_run_time:.3f}s\")\n",
        "print(f\"   ‚è±Ô∏è Second count: {second_run_time:.3f}s\")\n",
        "print(f\"   üìä Records: {count1}\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Broadcast Join:\")\n",
        "print(\"   Use broadcast join for small tables\")\n",
        "\n",
        "# Broadcast the small customers table\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "sales_broadcast = sales_df.join(\n",
        "    broadcast(customers_df),\n",
        "    sales_df.customer_name == customers_df.customer_name,\n",
        "    \"inner\"\n",
        ")\n",
        "\n",
        "print(\"   üìä Used broadcast join for customers table\")\n",
        "sales_broadcast.select(\"sale_id\", \"customer_name\", \"age\", \"city\").show(5)\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Repartitioning:\")\n",
        "print(\"   Optimize data partitioning\")\n",
        "\n",
        "# Check current partitions\n",
        "print(f\"   üìä Current partitions: {sales_df.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Repartition by region for better performance\n",
        "sales_repartitioned = sales_df.repartition(4, \"region\")\n",
        "print(f\"   üìä Repartitioned partitions: {sales_repartitioned.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Coalesce to reduce partitions\n",
        "sales_coalesced = sales_repartitioned.coalesce(2)\n",
        "print(f\"   üìä Coalesced partitions: {sales_coalesced.rdd.getNumPartitions()}\")\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ Performance Monitoring:\")\n",
        "print(\"   Monitor Spark application performance\")\n",
        "\n",
        "# Get Spark context information\n",
        "sc = spark.sparkContext\n",
        "print(f\"   üîß Spark Version: {sc.version}\")\n",
        "print(f\"   üîß Master: {sc.master}\")\n",
        "print(f\"   üîß App Name: {sc.appName}\")\n",
        "\n",
        "# Get executor information\n",
        "executor_infos = sc.statusTracker().getExecutorInfos()\n",
        "print(f\"   üîß Active Executors: {len(executor_infos)}\")\n",
        "\n",
        "for executor in executor_infos:\n",
        "    print(f\"      Executor {executor.executorId}: {executor.host}:{executor.port}\")\n",
        "\n",
        "print(\"\\n5Ô∏è‚É£ Memory Management:\")\n",
        "print(\"   Check DataFrame memory usage\")\n",
        "\n",
        "# Show storage level\n",
        "print(f\"   üìä Storage Level: {complete_sales.storageLevel}\")\n",
        "\n",
        "# Unpersist cached DataFrames\n",
        "complete_sales.unpersist()\n",
        "print(\"   üóëÔ∏è Unpersisted cached DataFrame\")\n",
        "\n",
        "print(\"\\n‚úÖ Performance optimization completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: Data Persistence and Export\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Learn data persistence strategies\n",
        "- Practice different file formats\n",
        "- Understand data export patterns\n",
        "- Master data pipeline completion\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **File Formats**: Parquet, JSON, CSV, Avro\n",
        "2. **Data Persistence**: Saving processed results\n",
        "3. **Partitioned Storage**: Organizing data by partitions\n",
        "4. **Data Export**: Writing to external systems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 5: Data Persistence and Export\n",
        "print(\"üíæ Exercise 5: Data Persistence and Export\")\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Export to Different Formats:\")\n",
        "print(\"   Save processed data in various formats\")\n",
        "\n",
        "# Create output directory\n",
        "output_dir = \"/tmp/spark_lab_output\"\n",
        "print(f\"   üìÅ Output directory: {output_dir}\")\n",
        "\n",
        "# Export to Parquet (recommended for Spark)\n",
        "print(\"\\n   üìä Exporting to Parquet format...\")\n",
        "complete_sales.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(f\"{output_dir}/sales_parquet\")\n",
        "\n",
        "print(\"   ‚úÖ Parquet export completed\")\n",
        "\n",
        "# Export to JSON\n",
        "print(\"\\n   üìä Exporting to JSON format...\")\n",
        "product_sales.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .json(f\"{output_dir}/product_sales_json\")\n",
        "\n",
        "print(\"   ‚úÖ JSON export completed\")\n",
        "\n",
        "# Export to CSV\n",
        "print(\"\\n   üìä Exporting to CSV format...\")\n",
        "profit_by_membership.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(f\"{output_dir}/profit_analysis_csv\")\n",
        "\n",
        "print(\"   ‚úÖ CSV export completed\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Partitioned Storage:\")\n",
        "print(\"   Save data partitioned by region\")\n",
        "\n",
        "# Partition by region for better query performance\n",
        "print(\"\\n   üìä Exporting partitioned data...\")\n",
        "sales_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"region\") \\\n",
        "    .parquet(f\"{output_dir}/sales_partitioned\")\n",
        "\n",
        "print(\"   ‚úÖ Partitioned export completed\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Data Validation:\")\n",
        "print(\"   Verify exported data\")\n",
        "\n",
        "# Read back and validate\n",
        "print(\"\\n   üìä Reading Parquet data...\")\n",
        "parquet_data = spark.read.parquet(f\"{output_dir}/sales_parquet\")\n",
        "print(f\"   üìä Parquet records: {parquet_data.count()}\")\n",
        "\n",
        "print(\"\\n   üìä Reading JSON data...\")\n",
        "json_data = spark.read.json(f\"{output_dir}/product_sales_json\")\n",
        "print(f\"   üìä JSON records: {json_data.count()}\")\n",
        "\n",
        "print(\"\\n   üìä Reading CSV data...\")\n",
        "csv_data = spark.read.option(\"header\", \"true\").csv(f\"{output_dir}/profit_analysis_csv\")\n",
        "print(f\"   üìä CSV records: {csv_data.count()}\")\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ Summary Statistics:\")\n",
        "print(\"   Final data processing summary\")\n",
        "\n",
        "print(f\"\\nüìä Processing Summary:\")\n",
        "print(f\"   üìà Total sales records processed: {sales_df.count()}\")\n",
        "print(f\"   üë• Customer records: {customers_df.count()}\")\n",
        "print(f\"   üì¶ Product records: {products_df.count()}\")\n",
        "print(f\"   üîó Joined records: {complete_sales.count()}\")\n",
        "print(f\"   üìä Product sales analysis: {product_sales.count()}\")\n",
        "print(f\"   üí∞ Profit analysis records: {profit_by_membership.count()}\")\n",
        "\n",
        "print(\"\\n‚úÖ Data persistence and export completed!\")\n",
        "print(\"üéâ Spark Batch Processing Lab completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup and Best Practices\n",
        "print(\"üßπ Cleanup and Best Practices\")\n",
        "\n",
        "print(\"\\nüìã Spark Batch Processing Best Practices:\")\n",
        "print(\"‚úÖ Use appropriate file formats (Parquet for analytics)\")\n",
        "print(\"‚úÖ Cache DataFrames that are used multiple times\")\n",
        "print(\"‚úÖ Use broadcast joins for small tables\")\n",
        "print(\"‚úÖ Partition data by frequently queried columns\")\n",
        "print(\"‚úÖ Monitor Spark UI for performance insights\")\n",
        "print(\"‚úÖ Use appropriate data types to save memory\")\n",
        "print(\"‚úÖ Avoid unnecessary shuffles and repartitions\")\n",
        "print(\"‚úÖ Use column pruning and predicate pushdown\")\n",
        "print(\"‚úÖ Set appropriate batch sizes for streaming\")\n",
        "print(\"‚úÖ Clean up cached DataFrames when done\")\n",
        "\n",
        "print(\"\\nüîß Performance Tips:\")\n",
        "print(\"‚úÖ Enable adaptive query execution (AQE)\")\n",
        "print(\"‚úÖ Use Kryo serializer for better performance\")\n",
        "print(\"‚úÖ Tune executor memory and cores\")\n",
        "print(\"‚úÖ Use appropriate storage levels\")\n",
        "print(\"‚úÖ Monitor and optimize shuffle operations\")\n",
        "\n",
        "print(\"\\nüìä Data Quality Tips:\")\n",
        "print(\"‚úÖ Validate data schemas\")\n",
        "print(\"‚úÖ Handle null values appropriately\")\n",
        "print(\"‚úÖ Use consistent data types\")\n",
        "print(\"‚úÖ Implement data quality checks\")\n",
        "print(\"‚úÖ Document data transformations\")\n",
        "\n",
        "print(\"\\nüéØ Next Steps:\")\n",
        "print(\"üöÄ Try Lab 2: Spark Streaming for real-time processing\")\n",
        "print(\"ü§ñ Try Lab 3: Spark MLlib for machine learning\")\n",
        "print(\"üìà Explore Spark UI for performance monitoring\")\n",
        "print(\"üîç Practice with larger datasets\")\n",
        "\n",
        "print(\"\\n‚úÖ Spark Batch Processing Lab completed!\")\n",
        "print(\"üéâ Ready for Spark Streaming and MLlib labs!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datalab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
