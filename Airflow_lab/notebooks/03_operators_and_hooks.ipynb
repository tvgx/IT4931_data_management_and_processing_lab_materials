{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3: Operators and Hooks - Executing Tasks with Airflow Operators\n",
        "\n",
        "## ðŸŽ¯ Objectives\n",
        "- Understand the role of Operators in Airflow\n",
        "- Use common operators: BashOperator, PythonOperator, SQLExecuteQueryOperator\n",
        "- Learn how to use Hooks to connect to external systems\n",
        "- Compare @task decorator vs Operators\n",
        "- Create data pipeline with multiple operator types\n",
        "\n",
        "## ðŸ“‹ Prerequisites\n",
        "- Completed Lab 1: Airflow Basics\n",
        "- Completed Lab 2: DAGs and Tasks\n",
        "- Airflow cluster is running\n",
        "- Basic Python and SQL knowledge\n",
        "\n",
        "## ðŸ—ï¸ Operators Overview\n",
        "Operators are templates for tasks in Airflow. Each operator defines a specific type of work:\n",
        "- **BashOperator**: Run bash commands\n",
        "- **PythonOperator**: Run Python functions\n",
        "- **SQLExecuteQueryOperator**: Execute SQL queries\n",
        "- **EmailOperator**: Send emails\n",
        "- **HttpOperator**: Make HTTP requests\n",
        "- And many other operators from providers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Airflow operators and hooks\n",
        "from airflow.sdk import DAG\n",
        "from airflow.providers.standard.operators.bash import BashOperator\n",
        "from airflow.providers.standard.operators.python import PythonOperator\n",
        "from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n",
        "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
        "from airflow.hooks.base import BaseHook\n",
        "\n",
        "import pendulum\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "print(\"âœ… Airflow operators vÃ  hooks imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. BashOperator - Run Shell Commands\n",
        "\n",
        "BashOperator allows running bash commands in tasks. Very useful for:\n",
        "- File operations\n",
        "- System commands\n",
        "- Script execution\n",
        "- Data processing with command-line tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG with BashOperator\n",
        "@dag(\n",
        "    dag_id=\"bash_operator_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"operators\", \"bash\"],\n",
        ")\n",
        "def bash_operator_dag():\n",
        "    \"\"\"\n",
        "    ### BashOperator Example\n",
        "    Sample DAG using BashOperator to run shell commands.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Task 1: Simple bash command\n",
        "    print_date = BashOperator(\n",
        "        task_id=\"print_date\",\n",
        "        bash_command=\"date\",\n",
        "    )\n",
        "    \n",
        "    # Task 2: Multi-line bash command\n",
        "    create_directory = BashOperator(\n",
        "        task_id=\"create_directory\",\n",
        "        bash_command=\"\"\"\n",
        "        mkdir -p /tmp/airflow_test\n",
        "        echo \"Directory created at $(date)\" > /tmp/airflow_test/created.txt\n",
        "        cat /tmp/airflow_test/created.txt\n",
        "        \"\"\",\n",
        "    )\n",
        "    \n",
        "    # Task 3: Bash with environment variables\n",
        "    print_env = BashOperator(\n",
        "        task_id=\"print_env\",\n",
        "        bash_command=\"echo 'Python path: $PYTHONPATH' && echo 'User: $USER'\",\n",
        "        env={\"CUSTOM_VAR\": \"custom_value\"},\n",
        "    )\n",
        "    \n",
        "    # Task 4: Bash with Jinja templating\n",
        "    templated_command = BashOperator(\n",
        "        task_id=\"templated_command\",\n",
        "        bash_command=\"\"\"\n",
        "        echo \"Execution date: {{ ds }}\"\n",
        "        echo \"Next execution date: {{ next_ds }}\"\n",
        "        echo \"Previous execution date: {{ prev_ds }}\"\n",
        "        echo \"DAG run ID: {{ dag_run.dag_id }}\"\n",
        "        \"\"\",\n",
        "    )\n",
        "    \n",
        "    # Define dependencies\n",
        "    print_date >> create_directory >> print_env >> templated_command\n",
        "\n",
        "# Create DAG\n",
        "bash_dag = bash_operator_dag()\n",
        "\n",
        "print(\"âœ… BashOperator DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in bash_dag.tasks]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. PythonOperator - Run Python Functions\n",
        "\n",
        "PythonOperator allows running Python functions as tasks. Note:\n",
        "- Function must be defined at top-level or importable\n",
        "- Can pass arguments via `op_args` and `op_kwargs`\n",
        "- Context variables are available in function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions for PythonOperator\n",
        "def print_context(**context):\n",
        "    \"\"\"Print Airflow context variables\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Airflow Context Variables:\")\n",
        "    print(f\"  Execution Date: {context['ds']}\")\n",
        "    print(f\"  Next Execution Date: {context['next_ds']}\")\n",
        "    print(f\"  Previous Execution Date: {context['prev_ds']}\")\n",
        "    print(f\"  DAG Run ID: {context['dag_run'].dag_id}\")\n",
        "    print(f\"  Task Instance: {context['task_instance'].task_id}\")\n",
        "    print(\"=\" * 60)\n",
        "    return \"Context printed successfully\"\n",
        "\n",
        "def process_data(records: int, multiplier: float = 1.5):\n",
        "    \"\"\"Process data with parameters\"\"\"\n",
        "    print(f\"Processing {records} records with multiplier {multiplier}\")\n",
        "    result = records * multiplier\n",
        "    print(f\"Result: {result}\")\n",
        "    return result\n",
        "\n",
        "def generate_report(**context):\n",
        "    \"\"\"Generate report with context\"\"\"\n",
        "    execution_date = context['ds']\n",
        "    dag_id = context['dag_run'].dag_id\n",
        "    \n",
        "    report = f\"\"\"\n",
        "    Report Generated\n",
        "    ================\n",
        "    DAG: {dag_id}\n",
        "    Execution Date: {execution_date}\n",
        "    Status: Success\n",
        "    \"\"\"\n",
        "    print(report)\n",
        "    return report\n",
        "\n",
        "# DAG with PythonOperator\n",
        "@dag(\n",
        "    dag_id=\"python_operator_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"operators\", \"python\"],\n",
        ")\n",
        "def python_operator_dag():\n",
        "    \"\"\"\n",
        "    ### PythonOperator Example\n",
        "    Sample DAG using PythonOperator to run Python functions.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Task 1: Simple Python function\n",
        "    print_context_task = PythonOperator(\n",
        "        task_id=\"print_context\",\n",
        "        python_callable=print_context,\n",
        "    )\n",
        "    \n",
        "    # Task 2: Python function with arguments\n",
        "    process_data_task = PythonOperator(\n",
        "        task_id=\"process_data\",\n",
        "        python_callable=process_data,\n",
        "        op_args=[100],  # Positional arguments\n",
        "        op_kwargs={\"multiplier\": 2.0},  # Keyword arguments\n",
        "    )\n",
        "    \n",
        "    # Task 3: Python function with context\n",
        "    generate_report_task = PythonOperator(\n",
        "        task_id=\"generate_report\",\n",
        "        python_callable=generate_report,\n",
        "    )\n",
        "    \n",
        "    # Define dependencies\n",
        "    print_context_task >> process_data_task >> generate_report_task\n",
        "\n",
        "# Create DAG\n",
        "python_dag = python_operator_dag()\n",
        "\n",
        "print(\"âœ… PythonOperator DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in python_dag.tasks]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG vá»›i SQLExecuteQueryOperator\n",
        "# Note: Cáº§n setup PostgreSQL connection trong Airflow UI trÆ°á»›c\n",
        "# Connection ID: postgres_default\n",
        "# Connection Type: Postgres\n",
        "# Host: postgres\n",
        "# Schema: airflow\n",
        "# Login: airflow\n",
        "# Password: airflow\n",
        "# Port: 5432\n",
        "\n",
        "@dag(\n",
        "    dag_id=\"sql_operator_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"operators\", \"sql\"],\n",
        ")\n",
        "def sql_operator_dag():\n",
        "    \"\"\"\n",
        "    ### SQLExecuteQueryOperator Example\n",
        "    DAG máº«u sá»­ dá»¥ng SQLExecuteQueryOperator Ä‘á»ƒ thá»±c thi SQL queries.\n",
        "    \n",
        "    Note: Cáº§n setup PostgreSQL connection trong Airflow UI:\n",
        "    - Connection ID: postgres_default\n",
        "    - Connection Type: Postgres\n",
        "    - Host: postgres\n",
        "    - Schema: airflow\n",
        "    - Login: airflow\n",
        "    - Password: airflow\n",
        "    - Port: 5432\n",
        "    \"\"\"\n",
        "    \n",
        "    # Task 1: Create table\n",
        "    create_table = SQLExecuteQueryOperator(\n",
        "        task_id=\"create_table\",\n",
        "        conn_id=\"postgres_default\",\n",
        "        sql=\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS airflow_test_table (\n",
        "            id SERIAL PRIMARY KEY,\n",
        "            name VARCHAR(100),\n",
        "            value INTEGER,\n",
        "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "        );\n",
        "        \"\"\",\n",
        "    )\n",
        "    \n",
        "    # Task 2: Insert data vá»›i Jinja templating\n",
        "    insert_data = SQLExecuteQueryOperator(\n",
        "        task_id=\"insert_data\",\n",
        "        conn_id=\"postgres_default\",\n",
        "        sql=\"\"\"\n",
        "        INSERT INTO airflow_test_table (name, value)\n",
        "        VALUES \n",
        "            ('{{ ds }}', 100),\n",
        "            ('test_record', 200),\n",
        "            ('another_record', 300);\n",
        "        \"\"\",\n",
        "    )\n",
        "    \n",
        "    # Task 3: Query data\n",
        "    query_data = SQLExecuteQueryOperator(\n",
        "        task_id=\"query_data\",\n",
        "        conn_id=\"postgres_default\",\n",
        "        sql=\"\"\"\n",
        "        SELECT \n",
        "            COUNT(*) as total_records,\n",
        "            SUM(value) as total_value,\n",
        "            AVG(value) as avg_value\n",
        "        FROM airflow_test_table;\n",
        "        \"\"\",\n",
        "    )\n",
        "    \n",
        "    # Task 4: Cleanup (optional)\n",
        "    drop_table = SQLExecuteQueryOperator(\n",
        "        task_id=\"drop_table\",\n",
        "        conn_id=\"postgres_default\",\n",
        "        sql=\"DROP TABLE IF EXISTS airflow_test_table;\",\n",
        "    )\n",
        "    \n",
        "    # Define dependencies\n",
        "    create_table >> insert_data >> query_data\n",
        "    # Uncomment Ä‘á»ƒ cleanup:\n",
        "    # query_data >> drop_table\n",
        "\n",
        "# Create DAG\n",
        "sql_dag = sql_operator_dag()\n",
        "\n",
        "print(\"âœ… SQLExecuteQueryOperator DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in sql_dag.tasks]}\")\n",
        "print(\"\\nðŸ’¡ Note: Setup PostgreSQL connection in Airflow UI before running this DAG\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG vá»›i Hooks\n",
        "@dag(\n",
        "    dag_id=\"hooks_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"hooks\", \"example\"],\n",
        ")\n",
        "def hooks_dag():\n",
        "    \"\"\"\n",
        "    ### Hooks Example\n",
        "    DAG máº«u sá»­ dá»¥ng Hooks Ä‘á»ƒ káº¿t ná»‘i vá»›i external systems.\n",
        "    \"\"\"\n",
        "    \n",
        "    def use_postgres_hook(**context):\n",
        "        \"\"\"Sá»­ dá»¥ng PostgresHook Ä‘á»ƒ káº¿t ná»‘i database\"\"\"\n",
        "        # Get hook tá»« connection ID\n",
        "        hook = PostgresHook(postgres_conn_id=\"postgres_default\")\n",
        "        \n",
        "        # Execute query\n",
        "        records = hook.get_records(\"\"\"\n",
        "            SELECT COUNT(*) FROM information_schema.tables \n",
        "            WHERE table_schema = 'public'\n",
        "        \"\"\")\n",
        "        \n",
        "        print(f\"Number of tables in public schema: {records[0][0]}\")\n",
        "        \n",
        "        # Get pandas dataframe\n",
        "        df = hook.get_pandas_df(\"SELECT version();\")\n",
        "        print(f\"PostgreSQL version: {df.iloc[0, 0]}\")\n",
        "        \n",
        "        return records[0][0]\n",
        "    \n",
        "    def test_connection(**context):\n",
        "        \"\"\"Test database connection\"\"\"\n",
        "        hook = PostgresHook(postgres_conn_id=\"postgres_default\")\n",
        "        \n",
        "        # Test connection\n",
        "        try:\n",
        "            conn = hook.get_conn()\n",
        "            print(\"âœ… Database connection successful!\")\n",
        "            print(f\"Connection info: {conn.info}\")\n",
        "            conn.close()\n",
        "            return \"Connection OK\"\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Connection failed: {e}\")\n",
        "            raise\n",
        "    \n",
        "    # Tasks sá»­ dá»¥ng hooks\n",
        "    test_conn_task = PythonOperator(\n",
        "        task_id=\"test_connection\",\n",
        "        python_callable=test_connection,\n",
        "    )\n",
        "    \n",
        "    use_hook_task = PythonOperator(\n",
        "        task_id=\"use_postgres_hook\",\n",
        "        python_callable=use_postgres_hook,\n",
        "    )\n",
        "    \n",
        "    # Define dependencies\n",
        "    test_conn_task >> use_hook_task\n",
        "\n",
        "# Create DAG\n",
        "hooks_dag_instance = hooks_dag()\n",
        "\n",
        "print(\"âœ… Hooks DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in hooks_dag_instance.tasks]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. So sÃ¡nh @task Decorator vs Operators\n",
        "\n",
        "Airflow cung cáº¥p 2 cÃ¡ch Ä‘á»ƒ táº¡o tasks:\n",
        "1. **@task decorator** (Task SDK - Modern approach)\n",
        "2. **Operators** (Traditional approach)\n",
        "\n",
        "HÃ£y so sÃ¡nh Æ°u/nhÆ°á»£c Ä‘iá»ƒm cá»§a má»—i cÃ¡ch:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# So sÃ¡nh @task decorator vs PythonOperator\n",
        "\n",
        "from airflow.sdk import task\n",
        "\n",
        "@dag(\n",
        "    dag_id=\"comparison_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"comparison\", \"example\"],\n",
        ")\n",
        "def comparison_dag():\n",
        "    \"\"\"\n",
        "    ### Comparison: @task vs Operators\n",
        "    So sÃ¡nh cÃ¡ch sá»­ dá»¥ng @task decorator vÃ  Operators.\n",
        "    \"\"\"\n",
        "    \n",
        "    # CÃ¡ch 1: Sá»­ dá»¥ng @task decorator (Modern - Task SDK)\n",
        "    @task\n",
        "    def task_sdk_example(data: dict):\n",
        "        \"\"\"Task SDK approach - Clean vÃ  Pythonic\"\"\"\n",
        "        result = data.get(\"value\", 0) * 2\n",
        "        print(f\"Task SDK result: {result}\")\n",
        "        return result\n",
        "    \n",
        "    # CÃ¡ch 2: Sá»­ dá»¥ng PythonOperator (Traditional)\n",
        "    def python_operator_function(**context):\n",
        "        \"\"\"PythonOperator approach - More explicit\"\"\"\n",
        "        data = {\"value\": 100}\n",
        "        result = data.get(\"value\", 0) * 2\n",
        "        print(f\"PythonOperator result: {result}\")\n",
        "        return result\n",
        "    \n",
        "    python_operator_task = PythonOperator(\n",
        "        task_id=\"python_operator_task\",\n",
        "        python_callable=python_operator_function,\n",
        "    )\n",
        "    \n",
        "    # CÃ¡ch 3: Sá»­ dá»¥ng BashOperator (KhÃ´ng cÃ³ @task tÆ°Æ¡ng Ä‘Æ°Æ¡ng)\n",
        "    bash_task = BashOperator(\n",
        "        task_id=\"bash_task\",\n",
        "        bash_command=\"echo 'BashOperator - No @task equivalent'\",\n",
        "    )\n",
        "    \n",
        "    # Task SDK vá»›i data passing\n",
        "    initial_data = {\"value\": 50}\n",
        "    task_sdk_result = task_sdk_example(initial_data)\n",
        "    \n",
        "    # Define dependencies\n",
        "    bash_task >> python_operator_task >> task_sdk_result\n",
        "\n",
        "# Create DAG\n",
        "comparison_dag_instance = comparison_dag()\n",
        "\n",
        "print(\"âœ… Comparison DAG created!\")\n",
        "print(\"\\nðŸ“Š Comparison Summary:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"@task Decorator:\")\n",
        "print(\"  âœ… Clean, Pythonic syntax\")\n",
        "print(\"  âœ… Automatic XCom handling\")\n",
        "print(\"  âœ… Type hints support\")\n",
        "print(\"  âœ… Easier data passing\")\n",
        "print(\"  âŒ Limited to Python functions\")\n",
        "print(\"\\nOperators:\")\n",
        "print(\"  âœ… Support many types (Bash, SQL, HTTP, etc.)\")\n",
        "print(\"  âœ… More explicit vÃ  configurable\")\n",
        "print(\"  âœ… Better for complex operations\")\n",
        "print(\"  âŒ More verbose syntax\")\n",
        "print(\"  âŒ Manual XCom handling\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Mixed Pipeline - Káº¿t há»£p nhiá»u Operators\n",
        "\n",
        "Trong thá»±c táº¿, pipelines thÆ°á»ng sá»­ dá»¥ng nhiá»u loáº¡i operators khÃ¡c nhau.\n",
        "HÃ£y táº¡o má»™t pipeline hoÃ n chá»‰nh vá»›i nhiá»u operators:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mixed Pipeline vá»›i nhiá»u operators\n",
        "@dag(\n",
        "    dag_id=\"mixed_operators_pipeline\",\n",
        "    schedule=\"@daily\",\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"pipeline\", \"mixed-operators\"],\n",
        ")\n",
        "def mixed_pipeline():\n",
        "    \"\"\"\n",
        "    ### Mixed Operators Pipeline\n",
        "    Pipeline máº«u káº¿t há»£p nhiá»u loáº¡i operators.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Step 1: Bash - Prepare environment\n",
        "    prepare_env = BashOperator(\n",
        "        task_id=\"prepare_environment\",\n",
        "        bash_command=\"\"\"\n",
        "        mkdir -p /tmp/airflow_pipeline\n",
        "        echo \"Pipeline started at $(date)\" > /tmp/airflow_pipeline/log.txt\n",
        "        \"\"\",\n",
        "    )\n",
        "    \n",
        "    # Step 2: Python - Extract data\n",
        "    def extract_data(**context):\n",
        "        \"\"\"Extract data from source\"\"\"\n",
        "        import json\n",
        "        \n",
        "        data = {\n",
        "            \"date\": context['ds'],\n",
        "            \"records\": [\n",
        "                {\"id\": 1, \"name\": \"Alice\", \"score\": 95},\n",
        "                {\"id\": 2, \"name\": \"Bob\", \"score\": 87},\n",
        "                {\"id\": 3, \"name\": \"Charlie\", \"score\": 92},\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        # Save to file\n",
        "        output_path = \"/tmp/airflow_pipeline/data.json\"\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "        \n",
        "        print(f\"Extracted {len(data['records'])} records\")\n",
        "        return output_path\n",
        "    \n",
        "    extract_task = PythonOperator(\n",
        "        task_id=\"extract_data\",\n",
        "        python_callable=extract_data,\n",
        "    )\n",
        "    \n",
        "    # Step 3: Python - Transform data\n",
        "    def transform_data(**context):\n",
        "        \"\"\"Transform extracted data\"\"\"\n",
        "        import json\n",
        "        \n",
        "        # Get file path from previous task (via XCom)\n",
        "        ti = context['ti']\n",
        "        file_path = ti.xcom_pull(task_ids='extract_data')\n",
        "        \n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        # Calculate statistics\n",
        "        scores = [record['score'] for record in data['records']]\n",
        "        stats = {\n",
        "            \"total_records\": len(scores),\n",
        "            \"average_score\": sum(scores) / len(scores),\n",
        "            \"max_score\": max(scores),\n",
        "            \"min_score\": min(scores),\n",
        "        }\n",
        "        \n",
        "        # Save transformed data\n",
        "        output_path = \"/tmp/airflow_pipeline/stats.json\"\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(stats, f, indent=2)\n",
        "        \n",
        "        print(f\"Transformed data: {stats}\")\n",
        "        return output_path\n",
        "    \n",
        "    transform_task = PythonOperator(\n",
        "        task_id=\"transform_data\",\n",
        "        python_callable=transform_data,\n",
        "    )\n",
        "    \n",
        "    # Step 4: Bash - Generate report\n",
        "    generate_report = BashOperator(\n",
        "        task_id=\"generate_report\",\n",
        "        bash_command=\"\"\"\n",
        "        echo \"=== Pipeline Report ===\" >> /tmp/airflow_pipeline/log.txt\n",
        "        echo \"Date: {{ ds }}\" >> /tmp/airflow_pipeline/log.txt\n",
        "        echo \"Status: Success\" >> /tmp/airflow_pipeline/log.txt\n",
        "        cat /tmp/airflow_pipeline/log.txt\n",
        "        \"\"\",\n",
        "    )\n",
        "    \n",
        "    # Step 5: Python - Cleanup\n",
        "    def cleanup(**context):\n",
        "        \"\"\"Cleanup temporary files\"\"\"\n",
        "        import os\n",
        "        \n",
        "        files_to_remove = [\n",
        "            \"/tmp/airflow_pipeline/data.json\",\n",
        "            \"/tmp/airflow_pipeline/stats.json\",\n",
        "        ]\n",
        "        \n",
        "        for file_path in files_to_remove:\n",
        "            if os.path.exists(file_path):\n",
        "                os.remove(file_path)\n",
        "                print(f\"Removed: {file_path}\")\n",
        "        \n",
        "        print(\"Cleanup completed\")\n",
        "    \n",
        "    cleanup_task = PythonOperator(\n",
        "        task_id=\"cleanup\",\n",
        "        python_callable=cleanup,\n",
        "        trigger_rule=\"all_done\",  # Run even if previous tasks failed\n",
        "    )\n",
        "    \n",
        "    # Define dependencies\n",
        "    prepare_env >> extract_task >> transform_task >> generate_report >> cleanup_task\n",
        "\n",
        "# Create DAG\n",
        "mixed_pipeline_dag = mixed_pipeline()\n",
        "\n",
        "print(\"âœ… Mixed Operators Pipeline DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in mixed_pipeline_dag.tasks]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Best Practices vÃ  Tips\n",
        "\n",
        "### âœ… Best Practices:\n",
        "\n",
        "1. **Chá»n Ä‘Ãºng Operator:**\n",
        "   - BashOperator: Cho shell commands vÃ  scripts\n",
        "   - PythonOperator: Cho Python logic phá»©c táº¡p\n",
        "   - SQLExecuteQueryOperator: Cho database operations\n",
        "   - @task decorator: Cho Python functions Ä‘Æ¡n giáº£n\n",
        "\n",
        "2. **Connection Management:**\n",
        "   - LuÃ´n sá»­ dá»¥ng Airflow Connections thay vÃ¬ hardcode credentials\n",
        "   - Test connections trÆ°á»›c khi deploy\n",
        "   - Sá»­ dá»¥ng Hooks Ä‘á»ƒ reuse connections\n",
        "\n",
        "3. **Error Handling:**\n",
        "   - Set retries vÃ  retry_delay phÃ¹ há»£p\n",
        "   - Sá»­ dá»¥ng trigger_rule Ä‘á»ƒ handle failures\n",
        "   - Log errors Ä‘áº§y Ä‘á»§\n",
        "\n",
        "4. **Performance:**\n",
        "   - TrÃ¡nh heavy operations trong PythonOperator\n",
        "   - Sá»­ dá»¥ng appropriate executors\n",
        "   - Cache káº¿t quáº£ khi cÃ³ thá»ƒ\n",
        "\n",
        "5. **Code Organization:**\n",
        "   - Äáº·t helper functions á»Ÿ top-level\n",
        "   - Sá»­ dá»¥ng docstrings cho tasks\n",
        "   - Group related tasks vá»›i TaskGroups\n",
        "\n",
        "### âš ï¸ Common Pitfalls:\n",
        "\n",
        "1. **Import errors**: Functions pháº£i import Ä‘Æ°á»£c tá»« DAG file\n",
        "2. **XCom size limits**: KhÃ´ng pass data quÃ¡ lá»›n qua XCom\n",
        "3. **Connection strings**: KhÃ´ng hardcode trong code\n",
        "4. **Timezone issues**: LuÃ´n sá»­ dá»¥ng UTC vÃ  pendulum\n",
        "5. **Task dependencies**: Äáº£m báº£o dependencies Ä‘Ãºng\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. TÃ³m táº¯t vÃ  Next Steps\n",
        "\n",
        "### âœ… Nhá»¯ng gÃ¬ Ä‘Ã£ há»c:\n",
        "1. BashOperator - Cháº¡y shell commands\n",
        "2. PythonOperator - Cháº¡y Python functions\n",
        "3. SQLExecuteQueryOperator - Thá»±c thi SQL queries\n",
        "4. Hooks - Káº¿t ná»‘i vá»›i external systems\n",
        "5. So sÃ¡nh @task decorator vs Operators\n",
        "6. Táº¡o pipeline vá»›i nhiá»u operators\n",
        "7. Best practices vÃ  common pitfalls\n",
        "\n",
        "### ðŸ“š Next Lab:\n",
        "- **Lab 4**: Task Dependencies vÃ  Branching\n",
        "- Bitshift operators (>>, <<)\n",
        "- BranchPythonOperator\n",
        "- Trigger rules\n",
        "- Dynamic task mapping\n",
        "\n",
        "### ðŸ”— Useful Links:\n",
        "- [Airflow Operators](https://airflow.apache.org/docs/apache-airflow/3.1.1/core-operators/index.html)\n",
        "- [Airflow Hooks](https://airflow.apache.org/docs/apache-airflow/3.1.1/core-concepts/hooks.html)\n",
        "- [SQL Operators](https://airflow.apache.org/docs/apache-airflow-providers-common-sql/stable/operators/sql.html)\n",
        "- [Best Practices](https://airflow.apache.org/docs/apache-airflow/3.1.1/best-practices/index.html)\n",
        "\n",
        "### ðŸ’¡ Exercises:\n",
        "1. Táº¡o DAG vá»›i BashOperator Ä‘á»ƒ process files\n",
        "2. Táº¡o DAG vá»›i PythonOperator Ä‘á»ƒ tÃ­nh toÃ¡n statistics\n",
        "3. Táº¡o DAG vá»›i SQLExecuteQueryOperator Ä‘á»ƒ query database\n",
        "4. Káº¿t há»£p táº¥t cáº£ operators trong má»™t pipeline\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
