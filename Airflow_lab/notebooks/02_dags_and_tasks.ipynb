{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: DAGs v√† Tasks - T·∫°o Workflows v·ªõi Task SDK\n",
        "\n",
        "## üéØ Objectives\n",
        "- H·ªçc c√°ch t·∫°o DAGs v·ªõi Task SDK (@dag, @task decorators)\n",
        "- Hi·ªÉu c√°ch ƒë·ªãnh nghƒ©a tasks v√† dependencies\n",
        "- X·ª≠ l√Ω errors v√† retries\n",
        "- T·∫°o ETL pipeline ƒë∆°n gi·∫£n\n",
        "\n",
        "## üìã Prerequisites\n",
        "- Ho√†n th√†nh Lab 1: Airflow Basics\n",
        "- Airflow cluster ƒëang ch·∫°y\n",
        "- Basic Python knowledge\n",
        "\n",
        "## üèóÔ∏è Task SDK Overview\n",
        "Airflow 3.x gi·ªõi thi·ªáu Task SDK v·ªõi decorators:\n",
        "- `@dag`: ƒê·ªãnh nghƒ©a DAG\n",
        "- `@task`: ƒê·ªãnh nghƒ©a Python task\n",
        "- `@task.bash`: ƒê·ªãnh nghƒ©a bash task\n",
        "- `@task.docker`: ƒê·ªãnh nghƒ©a Docker task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Airflow Task SDK\n",
        "from airflow.sdk import DAG, task\n",
        "from airflow.sdk.task import Task\n",
        "import pendulum\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(\"‚úÖ Airflow Task SDK imported successfully!\")\n",
        "print(f\"üì¶ Airflow version: Check Airflow UI or CLI\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. T·∫°o DAG ƒë∆°n gi·∫£n v·ªõi @dag decorator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫°o DAG ƒë∆°n gi·∫£n v·ªõi @dag decorator\n",
        "@dag(\n",
        "    dag_id=\"simple_dag_example\",\n",
        "    schedule=None,  # Manual trigger\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"tutorial\", \"example\"],\n",
        ")\n",
        "def simple_dag():\n",
        "    \"\"\"\n",
        "    ### Simple DAG Example\n",
        "    DAG ƒë∆°n gi·∫£n v·ªõi m·ªôt task duy nh·∫•t.\n",
        "    \"\"\"\n",
        "    \n",
        "    @task\n",
        "    def hello_task():\n",
        "        \"\"\"Print hello message\"\"\"\n",
        "        print(\"Hello from Airflow Task SDK!\")\n",
        "        return \"Task completed\"\n",
        "    \n",
        "    hello_task()\n",
        "\n",
        "# T·∫°o DAG instance\n",
        "dag_instance = simple_dag()\n",
        "\n",
        "print(\"‚úÖ DAG created successfully!\")\n",
        "print(f\"DAG ID: {dag_instance.dag_id}\")\n",
        "print(f\"Tasks: {[task.task_id for task in dag_instance.tasks]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. T·∫°o ETL Pipeline v·ªõi Task Dependencies\n",
        "\n",
        "Ch√∫ng ta s·∫Ω t·∫°o m·ªôt ETL pipeline ƒë∆°n gi·∫£n v·ªõi 3 tasks:\n",
        "1. Extract: L·∫•y d·ªØ li·ªáu\n",
        "2. Transform: Bi·∫øn ƒë·ªïi d·ªØ li·ªáu\n",
        "3. Load: L∆∞u d·ªØ li·ªáu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ETL Pipeline Example\n",
        "@dag(\n",
        "    dag_id=\"etl_pipeline_example\",\n",
        "    schedule=\"@daily\",\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"etl\", \"pipeline\"],\n",
        ")\n",
        "def etl_pipeline():\n",
        "    \"\"\"\n",
        "    ### ETL Pipeline\n",
        "    Extract, Transform, Load pipeline example.\n",
        "    \"\"\"\n",
        "    \n",
        "    @task\n",
        "    def extract():\n",
        "        \"\"\"Extract data from source\"\"\"\n",
        "        print(\"Extracting data...\")\n",
        "        # Simulate data extraction\n",
        "        data = {\n",
        "            \"users\": [\n",
        "                {\"id\": 1, \"name\": \"Alice\", \"age\": 30},\n",
        "                {\"id\": 2, \"name\": \"Bob\", \"age\": 25},\n",
        "                {\"id\": 3, \"name\": \"Charlie\", \"age\": 35},\n",
        "            ],\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "        }\n",
        "        print(f\"Extracted {len(data['users'])} records\")\n",
        "        return data\n",
        "    \n",
        "    @task\n",
        "    def transform(data: dict):\n",
        "        \"\"\"Transform extracted data\"\"\"\n",
        "        print(\"Transforming data...\")\n",
        "        users = data[\"users\"]\n",
        "        \n",
        "        # Calculate statistics\n",
        "        total_age = sum(user[\"age\"] for user in users)\n",
        "        avg_age = total_age / len(users)\n",
        "        \n",
        "        transformed = {\n",
        "            \"total_users\": len(users),\n",
        "            \"average_age\": avg_age,\n",
        "            \"timestamp\": data[\"timestamp\"],\n",
        "        }\n",
        "        print(f\"Transformed: {transformed}\")\n",
        "        return transformed\n",
        "    \n",
        "    @task\n",
        "    def load(data: dict):\n",
        "        \"\"\"Load transformed data to destination\"\"\"\n",
        "        print(\"Loading data...\")\n",
        "        print(f\"Loading {data['total_users']} users with avg age {data['average_age']:.2f}\")\n",
        "        return f\"Loaded {data['total_users']} records successfully\"\n",
        "    \n",
        "    # Define dependencies\n",
        "    extracted_data = extract()\n",
        "    transformed_data = transform(extracted_data)\n",
        "    load(transformed_data)\n",
        "\n",
        "# Create DAG\n",
        "etl_dag = etl_pipeline()\n",
        "\n",
        "print(\"‚úÖ ETL Pipeline DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in etl_dag.tasks]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Error Handling v√† Retries\n",
        "\n",
        "Airflow h·ªó tr·ª£ retry logic ƒë·ªÉ x·ª≠ l√Ω failures t·ª± ƒë·ªông.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG v·ªõi retry logic\n",
        "@dag(\n",
        "    dag_id=\"retry_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    default_args={\n",
        "        \"retries\": 3,  # Retry 3 times\n",
        "        \"retry_delay\": timedelta(minutes=1),  # Wait 1 minute between retries\n",
        "    },\n",
        "    tags=[\"retry\", \"error-handling\"],\n",
        ")\n",
        "def retry_example():\n",
        "    \"\"\"\n",
        "    ### Retry Example\n",
        "    DAG v·ªõi retry logic ƒë·ªÉ x·ª≠ l√Ω failures.\n",
        "    \"\"\"\n",
        "    \n",
        "    @task(retries=2, retry_delay=timedelta(seconds=30))\n",
        "    def unreliable_task():\n",
        "        \"\"\"Task c√≥ th·ªÉ fail\"\"\"\n",
        "        import random\n",
        "        \n",
        "        # Simulate random failure (50% chance)\n",
        "        if random.random() < 0.5:\n",
        "            print(\"Task failed! Will retry...\")\n",
        "            raise Exception(\"Random failure occurred\")\n",
        "        else:\n",
        "            print(\"Task succeeded!\")\n",
        "            return \"Success\"\n",
        "    \n",
        "    @task\n",
        "    def always_succeed():\n",
        "        \"\"\"Task lu√¥n th√†nh c√¥ng\"\"\"\n",
        "        print(\"This task always succeeds\")\n",
        "        return \"Done\"\n",
        "    \n",
        "    unreliable_task() >> always_succeed()\n",
        "\n",
        "# Create DAG\n",
        "retry_dag = retry_example()\n",
        "\n",
        "print(\"‚úÖ Retry Example DAG created!\")\n",
        "print(\"üí° This DAG demonstrates retry logic for error handling\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. T√≥m t·∫Øt v√† Next Steps\n",
        "\n",
        "### ‚úÖ Nh·ªØng g√¨ ƒë√£ h·ªçc:\n",
        "1. T·∫°o DAGs v·ªõi @dag decorator\n",
        "2. T·∫°o tasks v·ªõi @task decorator\n",
        "3. ƒê·ªãnh nghƒ©a task dependencies\n",
        "4. X·ª≠ l√Ω errors v·ªõi retries\n",
        "\n",
        "### üìö Next Lab:\n",
        "- **Lab 3**: Operators v√† Hooks\n",
        "- S·ª≠ d·ª•ng BashOperator, PythonOperator\n",
        "- SQLExecuteQueryOperator\n",
        "- Custom operators v√† hooks\n",
        "\n",
        "### üîó Useful Links:\n",
        "- [Task SDK Documentation](https://airflow.apache.org/docs/apache-airflow/3.1.1/task-sdk/index.html)\n",
        "- [DAG Best Practices](https://airflow.apache.org/docs/apache-airflow/3.1.1/best-practices/index.html)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
