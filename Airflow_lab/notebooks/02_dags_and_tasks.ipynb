{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: DAGs and Tasks - Creating Workflows with Task SDK\n",
        "\n",
        "## üéØ Objectives\n",
        "- Learn how to create DAGs with Task SDK (@dag, @task decorators)\n",
        "- Understand how to define tasks and dependencies\n",
        "- Handle errors and retries\n",
        "- Create a simple ETL pipeline\n",
        "\n",
        "## üìã Prerequisites\n",
        "- Completed Lab 1: Airflow Basics\n",
        "- Airflow cluster is running\n",
        "- Basic Python knowledge\n",
        "\n",
        "## üèóÔ∏è Task SDK Overview\n",
        "Airflow 3.x introduces Task SDK with decorators:\n",
        "- `@dag`: Define DAG\n",
        "- `@task`: Define Python task\n",
        "- `@task.bash`: Define bash task\n",
        "- `@task.docker`: Define Docker task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Airflow Task SDK\n",
        "from airflow.sdk import DAG, task\n",
        "from airflow.sdk.task import Task\n",
        "import pendulum\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(\"‚úÖ Airflow Task SDK imported successfully!\")\n",
        "print(f\"üì¶ Airflow version: Check Airflow UI or CLI\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Simple DAG with @dag Decorator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create simple DAG with @dag decorator\n",
        "@dag(\n",
        "    dag_id=\"simple_dag_example\",\n",
        "    schedule=None,  # Manual trigger\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"tutorial\", \"example\"],\n",
        ")\n",
        "def simple_dag():\n",
        "    \"\"\"\n",
        "    ### Simple DAG Example\n",
        "    Simple DAG with a single task.\n",
        "    \"\"\"\n",
        "    \n",
        "    @task\n",
        "    def hello_task():\n",
        "        \"\"\"Print hello message\"\"\"\n",
        "        print(\"Hello from Airflow Task SDK!\")\n",
        "        return \"Task completed\"\n",
        "    \n",
        "    hello_task()\n",
        "\n",
        "# Create DAG instance\n",
        "dag_instance = simple_dag()\n",
        "\n",
        "print(\"‚úÖ DAG created successfully!\")\n",
        "print(f\"DAG ID: {dag_instance.dag_id}\")\n",
        "print(f\"Tasks: {[task.task_id for task in dag_instance.tasks]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create ETL Pipeline with Task Dependencies\n",
        "\n",
        "We will create a simple ETL pipeline with 3 tasks:\n",
        "1. Extract: Get data\n",
        "2. Transform: Transform data\n",
        "3. Load: Save data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ETL Pipeline Example\n",
        "@dag(\n",
        "    dag_id=\"etl_pipeline_example\",\n",
        "    schedule=\"@daily\",\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"etl\", \"pipeline\"],\n",
        ")\n",
        "def etl_pipeline():\n",
        "    \"\"\"\n",
        "    ### ETL Pipeline\n",
        "    Extract, Transform, Load pipeline example.\n",
        "    \"\"\"\n",
        "    \n",
        "    @task\n",
        "    def extract():\n",
        "        \"\"\"Extract data from source\"\"\"\n",
        "        print(\"Extracting data...\")\n",
        "        # Simulate data extraction\n",
        "        data = {\n",
        "            \"users\": [\n",
        "                {\"id\": 1, \"name\": \"Alice\", \"age\": 30},\n",
        "                {\"id\": 2, \"name\": \"Bob\", \"age\": 25},\n",
        "                {\"id\": 3, \"name\": \"Charlie\", \"age\": 35},\n",
        "            ],\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "        }\n",
        "        print(f\"Extracted {len(data['users'])} records\")\n",
        "        return data\n",
        "    \n",
        "    @task\n",
        "    def transform(data: dict):\n",
        "        \"\"\"Transform extracted data\"\"\"\n",
        "        print(\"Transforming data...\")\n",
        "        users = data[\"users\"]\n",
        "        \n",
        "        # Calculate statistics\n",
        "        total_age = sum(user[\"age\"] for user in users)\n",
        "        avg_age = total_age / len(users)\n",
        "        \n",
        "        transformed = {\n",
        "            \"total_users\": len(users),\n",
        "            \"average_age\": avg_age,\n",
        "            \"timestamp\": data[\"timestamp\"],\n",
        "        }\n",
        "        print(f\"Transformed: {transformed}\")\n",
        "        return transformed\n",
        "    \n",
        "    @task\n",
        "    def load(data: dict):\n",
        "        \"\"\"Load transformed data to destination\"\"\"\n",
        "        print(\"Loading data...\")\n",
        "        print(f\"Loading {data['total_users']} users with avg age {data['average_age']:.2f}\")\n",
        "        return f\"Loaded {data['total_users']} records successfully\"\n",
        "    \n",
        "    # Define dependencies\n",
        "    extracted_data = extract()\n",
        "    transformed_data = transform(extracted_data)\n",
        "    load(transformed_data)\n",
        "\n",
        "# Create DAG\n",
        "etl_dag = etl_pipeline()\n",
        "\n",
        "print(\"‚úÖ ETL Pipeline DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in etl_dag.tasks]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Error Handling and Retries\n",
        "\n",
        "Airflow supports retry logic to automatically handle failures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG with retry logic\n",
        "@dag(\n",
        "    dag_id=\"retry_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    default_args={\n",
        "        \"retries\": 3,  # Retry 3 times\n",
        "        \"retry_delay\": timedelta(minutes=1),  # Wait 1 minute between retries\n",
        "    },\n",
        "    tags=[\"retry\", \"error-handling\"],\n",
        ")\n",
        "def retry_example():\n",
        "    \"\"\"\n",
        "    ### Retry Example\n",
        "    DAG with retry logic to handle failures.\n",
        "    \"\"\"\n",
        "    \n",
        "    @task(retries=2, retry_delay=timedelta(seconds=30))\n",
        "    def unreliable_task():\n",
        "        \"\"\"Task that may fail\"\"\"\n",
        "        import random\n",
        "        \n",
        "        # Simulate random failure (50% chance)\n",
        "        if random.random() < 0.5:\n",
        "            print(\"Task failed! Will retry...\")\n",
        "            raise Exception(\"Random failure occurred\")\n",
        "        else:\n",
        "            print(\"Task succeeded!\")\n",
        "            return \"Success\"\n",
        "    \n",
        "    @task\n",
        "    def always_succeed():\n",
        "        \"\"\"Task that always succeeds\"\"\"\n",
        "        print(\"This task always succeeds\")\n",
        "        return \"Done\"\n",
        "    \n",
        "    unreliable_task() >> always_succeed()\n",
        "\n",
        "# Create DAG\n",
        "retry_dag = retry_example()\n",
        "\n",
        "print(\"‚úÖ Retry Example DAG created!\")\n",
        "print(\"üí° This DAG demonstrates retry logic for error handling\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary and Next Steps\n",
        "\n",
        "### ‚úÖ What we learned:\n",
        "1. Create DAGs with @dag decorator\n",
        "2. Create tasks with @task decorator\n",
        "3. Define task dependencies\n",
        "4. Handle errors with retries\n",
        "\n",
        "### üìö Next Lab:\n",
        "- **Lab 3**: Operators and Hooks\n",
        "- Use BashOperator, PythonOperator\n",
        "- SQLExecuteQueryOperator\n",
        "- Custom operators and hooks\n",
        "\n",
        "### üîó Useful Links:\n",
        "- [Task SDK Documentation](https://airflow.apache.org/docs/apache-airflow/3.1.1/task-sdk/index.html)\n",
        "- [DAG Best Practices](https://airflow.apache.org/docs/apache-airflow/3.1.1/best-practices/index.html)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
