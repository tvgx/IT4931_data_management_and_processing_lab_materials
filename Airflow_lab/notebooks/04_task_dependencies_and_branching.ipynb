{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 4: Task Dependencies v√† Branching - Qu·∫£n l√Ω Workflow Logic\n",
        "\n",
        "## üéØ Objectives\n",
        "- Hi·ªÉu c√°ch ƒë·ªãnh nghƒ©a task dependencies v·ªõi bitshift operators\n",
        "- S·ª≠ d·ª•ng BranchPythonOperator cho conditional logic\n",
        "- √Åp d·ª•ng trigger rules ƒë·ªÉ x·ª≠ l√Ω failures\n",
        "- T·∫°o dynamic tasks v·ªõi task mapping\n",
        "- S·ª≠ d·ª•ng TaskGroups ƒë·ªÉ t·ªï ch·ª©c tasks\n",
        "- X√¢y d·ª±ng conditional workflows ph·ª©c t·∫°p\n",
        "\n",
        "## üìã Prerequisites\n",
        "- Ho√†n th√†nh Lab 1-3\n",
        "- Hi·ªÉu operators v√† tasks\n",
        "- Airflow cluster ƒëang ch·∫°y\n",
        "\n",
        "## üèóÔ∏è Dependencies Overview\n",
        "Task dependencies trong Airflow c√≥ th·ªÉ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a b·∫±ng:\n",
        "- **Bitshift operators**: `>>` (set downstream), `<<` (set upstream)\n",
        "- **Methods**: `set_downstream()`, `set_upstream()`\n",
        "- **Lists**: Multiple dependencies c√πng l√∫c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries v√† Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Airflow dependencies v√† branching\n",
        "from airflow.sdk import DAG, task\n",
        "from airflow.providers.standard.operators.empty import EmptyOperator\n",
        "from airflow.providers.standard.operators.python import BranchPythonOperator, PythonOperator\n",
        "from airflow.providers.standard.operators.bash import BashOperator\n",
        "from airflow.sdk.task_group import TaskGroup\n",
        "\n",
        "import pendulum\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "print(\"‚úÖ Airflow dependencies v√† branching modules imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Bitshift Operators - ƒê·ªãnh nghƒ©a Dependencies\n",
        "\n",
        "Airflow s·ª≠ d·ª•ng bitshift operators (`>>` v√† `<<`) ƒë·ªÉ ƒë·ªãnh nghƒ©a dependencies m·ªôt c√°ch tr·ª±c quan v√† Pythonic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG v·ªõi bitshift operators\n",
        "@dag(\n",
        "    dag_id=\"bitshift_dependencies_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"dependencies\", \"bitshift\"],\n",
        ")\n",
        "def bitshift_dependencies_dag():\n",
        "    \"\"\"\n",
        "    ### Bitshift Operators Example\n",
        "    DAG m·∫´u s·ª≠ d·ª•ng bitshift operators ƒë·ªÉ ƒë·ªãnh nghƒ©a dependencies.\n",
        "    \"\"\"\n",
        "    \n",
        "    # T·∫°o tasks\n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "    task_a = EmptyOperator(task_id=\"task_a\")\n",
        "    task_b = EmptyOperator(task_id=\"task_b\")\n",
        "    task_c = EmptyOperator(task_id=\"task_c\")\n",
        "    task_d = EmptyOperator(task_id=\"task_d\")\n",
        "    end = EmptyOperator(task_id=\"end\")\n",
        "    \n",
        "    # C√°ch 1: Single dependency v·ªõi >>\n",
        "    # start >> task_a c√≥ nghƒ©a l√† task_a ph·ª• thu·ªôc v√†o start\n",
        "    start >> task_a\n",
        "    \n",
        "    # C√°ch 2: Multiple downstream tasks\n",
        "    # task_a ch·∫°y tr∆∞·ªõc task_b v√† task_c\n",
        "    task_a >> [task_b, task_c]\n",
        "    \n",
        "    # C√°ch 3: Multiple upstream tasks\n",
        "    # task_d ph·ª• thu·ªôc v√†o c·∫£ task_b v√† task_c\n",
        "    [task_b, task_c] >> task_d\n",
        "    \n",
        "    # C√°ch 4: Chain dependencies\n",
        "    task_d >> end\n",
        "    \n",
        "    # Ho·∫∑c vi·∫øt g·ªçn h∆°n:\n",
        "    # start >> task_a >> [task_b, task_c] >> task_d >> end\n",
        "\n",
        "# Create DAG\n",
        "bitshift_dag = bitshift_dependencies_dag()\n",
        "\n",
        "print(\"‚úÖ Bitshift Dependencies DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in bitshift_dag.tasks]}\")\n",
        "print(\"\\nüìä Dependency Flow:\")\n",
        "print(\"start ‚Üí task_a ‚Üí [task_b, task_c] ‚Üí task_d ‚Üí end\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. BranchPythonOperator - Conditional Branching\n",
        "\n",
        "BranchPythonOperator cho ph√©p ch·ªçn nh√°nh n√†o s·∫Ω ch·∫°y d·ª±a tr√™n ƒëi·ªÅu ki·ªán. Function ph·∫£i return task_id(s) c·ªßa task(s) ti·∫øp theo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG v·ªõi BranchPythonOperator\n",
        "@dag(\n",
        "    dag_id=\"branching_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"branching\", \"conditional\"],\n",
        ")\n",
        "def branching_dag():\n",
        "    \"\"\"\n",
        "    ### Branching Example\n",
        "    DAG m·∫´u s·ª≠ d·ª•ng BranchPythonOperator cho conditional logic.\n",
        "    \"\"\"\n",
        "    \n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "    \n",
        "    # Branch function - return task_id(s) c·ªßa task(s) s·∫Ω ch·∫°y\n",
        "    def choose_branch(**context):\n",
        "        \"\"\"Ch·ªçn branch d·ª±a tr√™n ƒëi·ªÅu ki·ªán\"\"\"\n",
        "        execution_date = context['data_interval_start']\n",
        "        day_of_week = execution_date.weekday()\n",
        "        \n",
        "        # Th·ª© 2 (0) ho·∫∑c th·ª© 6 (4): ch·∫°y c·∫£ 2 branches\n",
        "        if day_of_week == 0 or day_of_week == 4:\n",
        "            return [\"branch_a\", \"branch_b\"]\n",
        "        # Cu·ªëi tu·∫ßn: ch·ªâ ch·∫°y branch_a\n",
        "        elif day_of_week >= 5:\n",
        "            return \"branch_a\"\n",
        "        # Ng√†y th∆∞·ªùng: ch·ªâ ch·∫°y branch_b\n",
        "        else:\n",
        "            return \"branch_b\"\n",
        "    \n",
        "    branching = BranchPythonOperator(\n",
        "        task_id=\"branching\",\n",
        "        python_callable=choose_branch,\n",
        "    )\n",
        "    \n",
        "    # C√°c branch tasks\n",
        "    branch_a = EmptyOperator(task_id=\"branch_a\")\n",
        "    branch_b = EmptyOperator(task_id=\"branch_b\")\n",
        "    \n",
        "    # Join task - ch·∫°y sau khi c√°c branches ho√†n th√†nh\n",
        "    join = EmptyOperator(\n",
        "        task_id=\"join\",\n",
        "        trigger_rule=\"none_failed_min_one_success\",  # Ch·∫°y n·∫øu √≠t nh·∫•t 1 branch th√†nh c√¥ng\n",
        "    )\n",
        "    \n",
        "    end = EmptyOperator(task_id=\"end\")\n",
        "    \n",
        "    # Define dependencies\n",
        "    start >> branching\n",
        "    branching >> branch_a >> join\n",
        "    branching >> branch_b >> join\n",
        "    join >> end\n",
        "\n",
        "# Create DAG\n",
        "branching_dag_instance = branching_dag()\n",
        "\n",
        "print(\"‚úÖ Branching DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in branching_dag_instance.tasks]}\")\n",
        "print(\"\\nüí° Branch logic:\")\n",
        "print(\"  - Monday/Friday: Run both branches\")\n",
        "print(\"  - Weekend: Run branch_a only\")\n",
        "print(\"  - Weekday: Run branch_b only\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Trigger Rules - X·ª≠ l√Ω Task Failures\n",
        "\n",
        "Trigger rules x√°c ƒë·ªãnh khi n√†o m·ªôt task s·∫Ω ch·∫°y d·ª±a tr√™n tr·∫°ng th√°i c·ªßa upstream tasks. C√°c trigger rules ph·ªï bi·∫øn:\n",
        "- `all_success` (default): T·∫•t c·∫£ upstream tasks ph·∫£i th√†nh c√¥ng\n",
        "- `all_failed`: T·∫•t c·∫£ upstream tasks ph·∫£i failed\n",
        "- `all_done`: Ch·∫°y khi t·∫•t c·∫£ upstream tasks ho√†n th√†nh (b·∫•t k·ªÉ status)\n",
        "- `one_failed`: Ch·∫°y khi √≠t nh·∫•t 1 upstream task failed\n",
        "- `one_success`: Ch·∫°y khi √≠t nh·∫•t 1 upstream task th√†nh c√¥ng\n",
        "- `none_failed`: Ch·∫°y khi kh√¥ng c√≥ upstream task n√†o failed (success ho·∫∑c skipped)\n",
        "- `none_failed_min_one_success`: Ch·∫°y khi kh√¥ng c√≥ failed v√† √≠t nh·∫•t 1 success\n",
        "- `none_skipped`: Ch·∫°y khi kh√¥ng c√≥ upstream task n√†o skipped\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG v·ªõi Trigger Rules\n",
        "@dag(\n",
        "    dag_id=\"trigger_rules_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"trigger-rules\", \"error-handling\"],\n",
        ")\n",
        "def trigger_rules_dag():\n",
        "    \"\"\"\n",
        "    ### Trigger Rules Example\n",
        "    DAG m·∫´u minh h·ªça c√°c trigger rules kh√°c nhau.\n",
        "    \"\"\"\n",
        "    \n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "    \n",
        "    # Task c√≥ th·ªÉ fail\n",
        "    def task_that_might_fail(**context):\n",
        "        \"\"\"Task c√≥ th·ªÉ fail d·ª±a tr√™n ƒëi·ªÅu ki·ªán\"\"\"\n",
        "        import random\n",
        "        if random.random() < 0.5:\n",
        "            raise Exception(\"Random failure occurred\")\n",
        "        return \"Success\"\n",
        "    \n",
        "    might_fail = PythonOperator(\n",
        "        task_id=\"might_fail\",\n",
        "        python_callable=task_that_might_fail,\n",
        "    )\n",
        "    \n",
        "    # Task lu√¥n th√†nh c√¥ng\n",
        "    always_succeed = EmptyOperator(task_id=\"always_succeed\")\n",
        "    \n",
        "    # Task v·ªõi trigger_rule=\"all_done\" - ch·∫°y b·∫•t k·ªÉ upstream status\n",
        "    cleanup_all_done = EmptyOperator(\n",
        "        task_id=\"cleanup_all_done\",\n",
        "        trigger_rule=\"all_done\",\n",
        "    )\n",
        "    \n",
        "    # Task v·ªõi trigger_rule=\"one_failed\" - ch·∫°y n·∫øu c√≥ task failed\n",
        "    handle_failure = EmptyOperator(\n",
        "        task_id=\"handle_failure\",\n",
        "        trigger_rule=\"one_failed\",\n",
        "    )\n",
        "    \n",
        "    # Task v·ªõi trigger_rule=\"none_failed_min_one_success\" - ch·∫°y n·∫øu kh√¥ng failed v√† c√≥ success\n",
        "    final_task = EmptyOperator(\n",
        "        task_id=\"final_task\",\n",
        "        trigger_rule=\"none_failed_min_one_success\",\n",
        "    )\n",
        "    \n",
        "    end = EmptyOperator(\n",
        "        task_id=\"end\",\n",
        "        trigger_rule=\"all_done\",  # Lu√¥n ch·∫°y\n",
        "    )\n",
        "    \n",
        "    # Define dependencies\n",
        "    start >> [might_fail, always_succeed]\n",
        "    [might_fail, always_succeed] >> cleanup_all_done\n",
        "    might_fail >> handle_failure\n",
        "    [cleanup_all_done, handle_failure] >> final_task >> end\n",
        "\n",
        "# Create DAG\n",
        "trigger_rules_dag_instance = trigger_rules_dag()\n",
        "\n",
        "print(\"‚úÖ Trigger Rules DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in trigger_rules_dag_instance.tasks]}\")\n",
        "print(\"\\nüìä Trigger Rules:\")\n",
        "print(\"  - cleanup_all_done: all_done (ch·∫°y b·∫•t k·ªÉ status)\")\n",
        "print(\"  - handle_failure: one_failed (ch·∫°y n·∫øu c√≥ failure)\")\n",
        "print(\"  - final_task: none_failed_min_one_success (ch·∫°y n·∫øu kh√¥ng failed v√† c√≥ success)\")\n",
        "print(\"  - end: all_done (lu√¥n ch·∫°y)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Dynamic Task Mapping - T·∫°o Tasks ƒê·ªông\n",
        "\n",
        "Dynamic Task Mapping cho ph√©p t·∫°o nhi·ªÅu task instances t·ª´ m·ªôt task definition d·ª±a tr√™n input data. R·∫•t h·ªØu √≠ch cho:\n",
        "- Processing multiple files\n",
        "- Batch operations\n",
        "- Parallel processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG v·ªõi Dynamic Task Mapping\n",
        "@dag(\n",
        "    dag_id=\"dynamic_task_mapping_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"dynamic\", \"mapping\"],\n",
        ")\n",
        "def dynamic_mapping_dag():\n",
        "    \"\"\"\n",
        "    ### Dynamic Task Mapping Example\n",
        "    DAG m·∫´u s·ª≠ d·ª•ng dynamic task mapping ƒë·ªÉ t·∫°o tasks ƒë·ªông.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Task t·∫°o list c√°c items c·∫ßn process\n",
        "    @task\n",
        "    def get_items_to_process():\n",
        "        \"\"\"Tr·∫£ v·ªÅ list c√°c items c·∫ßn process\"\"\"\n",
        "        items = [\n",
        "            {\"id\": 1, \"name\": \"item_1\", \"value\": 100},\n",
        "            {\"id\": 2, \"name\": \"item_2\", \"value\": 200},\n",
        "            {\"id\": 3, \"name\": \"item_3\", \"value\": 300},\n",
        "        ]\n",
        "        print(f\"Generated {len(items)} items to process\")\n",
        "        return items\n",
        "    \n",
        "    # Task v·ªõi dynamic mapping - s·∫Ω t·∫°o nhi·ªÅu task instances\n",
        "    @task\n",
        "    def process_item(item: dict):\n",
        "        \"\"\"Process m·ªôt item - s·∫Ω ƒë∆∞·ª£c map cho m·ªói item\"\"\"\n",
        "        print(f\"Processing {item['name']} with value {item['value']}\")\n",
        "        result = item['value'] * 2\n",
        "        print(f\"Result: {result}\")\n",
        "        return {\"item_id\": item['id'], \"result\": result}\n",
        "    \n",
        "    # Task t·ªïng h·ª£p k·∫øt qu·∫£\n",
        "    @task\n",
        "    def aggregate_results(results: list):\n",
        "        \"\"\"T·ªïng h·ª£p k·∫øt qu·∫£ t·ª´ t·∫•t c·∫£ tasks\"\"\"\n",
        "        total = sum(r['result'] for r in results)\n",
        "        print(f\"Aggregated total: {total}\")\n",
        "        return total\n",
        "    \n",
        "    # Define workflow v·ªõi dynamic mapping\n",
        "    items = get_items_to_process()\n",
        "    # .expand() t·∫°o dynamic task instances cho m·ªói item\n",
        "    processed_items = process_item.expand(item=items)\n",
        "    aggregate_results(processed_items)\n",
        "\n",
        "# Create DAG\n",
        "dynamic_mapping_dag_instance = dynamic_mapping_dag()\n",
        "\n",
        "print(\"‚úÖ Dynamic Task Mapping DAG created!\")\n",
        "print(\"\\nüí° Dynamic mapping s·∫Ω t·∫°o:\")\n",
        "print(\"  - 1 task instance cho get_items_to_process\")\n",
        "print(\"  - 3 task instances cho process_item (m·ªôt cho m·ªói item)\")\n",
        "print(\"  - 1 task instance cho aggregate_results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. TaskGroups - T·ªï ch·ª©c Tasks\n",
        "\n",
        "TaskGroups cho ph√©p nh√≥m c√°c tasks l·∫°i v·ªõi nhau ƒë·ªÉ:\n",
        "- C·∫£i thi·ªán visualization trong UI\n",
        "- T·ªï ch·ª©c code t·ªët h∆°n\n",
        "- √Åp d·ª•ng settings cho nh√≥m tasks\n",
        "- T·∫°o sub-workflows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG v·ªõi TaskGroups\n",
        "@dag(\n",
        "    dag_id=\"taskgroup_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"taskgroup\", \"organization\"],\n",
        ")\n",
        "def taskgroup_dag():\n",
        "    \"\"\"\n",
        "    ### TaskGroup Example\n",
        "    DAG m·∫´u s·ª≠ d·ª•ng TaskGroups ƒë·ªÉ t·ªï ch·ª©c tasks.\n",
        "    \"\"\"\n",
        "    \n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "    \n",
        "    # TaskGroup 1: Data Extraction\n",
        "    with TaskGroup(\"extract_group\") as extract_group:\n",
        "        extract_a = EmptyOperator(task_id=\"extract_source_a\")\n",
        "        extract_b = EmptyOperator(task_id=\"extract_source_b\")\n",
        "        extract_c = EmptyOperator(task_id=\"extract_source_c\")\n",
        "        \n",
        "        # Dependencies trong group\n",
        "        extract_a >> [extract_b, extract_c]\n",
        "    \n",
        "    # TaskGroup 2: Data Transformation\n",
        "    with TaskGroup(\"transform_group\") as transform_group:\n",
        "        transform_a = EmptyOperator(task_id=\"transform_a\")\n",
        "        transform_b = EmptyOperator(task_id=\"transform_b\")\n",
        "        \n",
        "        transform_a >> transform_b\n",
        "    \n",
        "    # TaskGroup 3: Data Loading\n",
        "    with TaskGroup(\"load_group\") as load_group:\n",
        "        load_a = EmptyOperator(task_id=\"load_destination_a\")\n",
        "        load_b = EmptyOperator(task_id=\"load_destination_b\")\n",
        "        \n",
        "        # Parallel loading\n",
        "        [load_a, load_b]\n",
        "    \n",
        "    end = EmptyOperator(task_id=\"end\")\n",
        "    \n",
        "    # Define dependencies gi·ªØa groups\n",
        "    start >> extract_group >> transform_group >> load_group >> end\n",
        "\n",
        "# Create DAG\n",
        "taskgroup_dag_instance = taskgroup_dag()\n",
        "\n",
        "print(\"‚úÖ TaskGroup DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in taskgroup_dag_instance.tasks]}\")\n",
        "print(\"\\nüìä Task Groups:\")\n",
        "print(\"  - extract_group: extract_source_a ‚Üí [extract_source_b, extract_source_c]\")\n",
        "print(\"  - transform_group: transform_a ‚Üí transform_b\")\n",
        "print(\"  - load_group: [load_destination_a, load_destination_b] (parallel)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Complex Branching Scenario - Real-world Example\n",
        "\n",
        "T·∫°o m·ªôt workflow ph·ª©c t·∫°p v·ªõi nhi·ªÅu branches v√† conditions ƒë·ªÉ minh h·ªça c√°ch s·ª≠ d·ª•ng branching trong th·ª±c t·∫ø.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complex Branching Scenario\n",
        "@dag(\n",
        "    dag_id=\"complex_branching_scenario\",\n",
        "    schedule=\"@daily\",\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"complex\", \"branching\", \"real-world\"],\n",
        ")\n",
        "def complex_branching_dag():\n",
        "    \"\"\"\n",
        "    ### Complex Branching Scenario\n",
        "    DAG m·∫´u v·ªõi branching logic ph·ª©c t·∫°p cho use case th·ª±c t·∫ø.\n",
        "    \"\"\"\n",
        "    \n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "    \n",
        "    # Check data quality\n",
        "    def check_data_quality(**context):\n",
        "        \"\"\"Ki·ªÉm tra ch·∫•t l∆∞·ª£ng data\"\"\"\n",
        "        # Simulate data quality check\n",
        "        import random\n",
        "        quality_score = random.uniform(0.7, 1.0)\n",
        "        \n",
        "        if quality_score >= 0.9:\n",
        "            return \"high_quality\"\n",
        "        elif quality_score >= 0.8:\n",
        "            return \"medium_quality\"\n",
        "        else:\n",
        "            return \"low_quality\"\n",
        "    \n",
        "    quality_check = BranchPythonOperator(\n",
        "        task_id=\"check_data_quality\",\n",
        "        python_callable=check_data_quality,\n",
        "    )\n",
        "    \n",
        "    # Different processing paths based on quality\n",
        "    high_quality_process = EmptyOperator(task_id=\"high_quality_process\")\n",
        "    medium_quality_process = EmptyOperator(task_id=\"medium_quality_process\")\n",
        "    low_quality_process = EmptyOperator(task_id=\"low_quality_process\")\n",
        "    \n",
        "    # Data cleaning for low quality\n",
        "    clean_data = EmptyOperator(task_id=\"clean_data\")\n",
        "    low_quality_process >> clean_data\n",
        "    \n",
        "    # Join after processing\n",
        "    join_processing = EmptyOperator(\n",
        "        task_id=\"join_processing\",\n",
        "        trigger_rule=\"none_failed_min_one_success\",\n",
        "    )\n",
        "    \n",
        "    # Final validation\n",
        "    validate_output = EmptyOperator(task_id=\"validate_output\")\n",
        "    \n",
        "    # Notification based on result\n",
        "    def send_notification(**context):\n",
        "        \"\"\"G·ª≠i notification d·ª±a tr√™n k·∫øt qu·∫£\"\"\"\n",
        "        ti = context['ti']\n",
        "        # Check if validation passed\n",
        "        print(\"Sending notification...\")\n",
        "        return \"notification_sent\"\n",
        "    \n",
        "    send_notif = PythonOperator(\n",
        "        task_id=\"send_notification\",\n",
        "        python_callable=send_notification,\n",
        "    )\n",
        "    \n",
        "    end = EmptyOperator(\n",
        "        task_id=\"end\",\n",
        "        trigger_rule=\"all_done\",\n",
        "    )\n",
        "    \n",
        "    # Define dependencies\n",
        "    start >> quality_check\n",
        "    quality_check >> high_quality_process >> join_processing\n",
        "    quality_check >> medium_quality_process >> join_processing\n",
        "    quality_check >> low_quality_process\n",
        "    clean_data >> join_processing\n",
        "    join_processing >> validate_output >> send_notif >> end\n",
        "\n",
        "# Create DAG\n",
        "complex_branching_dag_instance = complex_branching_dag()\n",
        "\n",
        "print(\"‚úÖ Complex Branching DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in complex_branching_dag_instance.tasks]}\")\n",
        "print(\"\\nüìä Workflow:\")\n",
        "print(\"  start ‚Üí check_data_quality ‚Üí [high/medium/low_quality_process]\")\n",
        "print(\"  low_quality_process ‚Üí clean_data ‚Üí join_processing\")\n",
        "print(\"  join_processing ‚Üí validate_output ‚Üí send_notification ‚Üí end\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. T√≥m t·∫Øt v√† Next Steps\n",
        "\n",
        "### ‚úÖ Nh·ªØng g√¨ ƒë√£ h·ªçc:\n",
        "1. Bitshift operators (>>, <<) - ƒê·ªãnh nghƒ©a dependencies\n",
        "2. BranchPythonOperator - Conditional branching\n",
        "3. Trigger rules - X·ª≠ l√Ω failures v√† skipped tasks\n",
        "4. Dynamic Task Mapping - T·∫°o tasks ƒë·ªông\n",
        "5. TaskGroups - T·ªï ch·ª©c tasks\n",
        "6. Complex branching scenarios - Use cases th·ª±c t·∫ø\n",
        "7. Best practices v√† common pitfalls\n",
        "\n",
        "### üìö Next Lab:\n",
        "- **Lab 5**: XCom v√† Data Sharing\n",
        "- XCom push/pull\n",
        "- Task return values\n",
        "- Custom XCom backends\n",
        "- Data passing best practices\n",
        "\n",
        "### üîó Useful Links:\n",
        "- [Task Dependencies](https://airflow.apache.org/docs/apache-airflow/3.1.1/core-concepts/dags.html#task-dependencies)\n",
        "- [Branching](https://airflow.apache.org/docs/apache-airflow/3.1.1/core-concepts/dags.html#branching)\n",
        "- [Trigger Rules](https://airflow.apache.org/docs/apache-airflow/3.1.1/core-concepts/tasks.html#trigger-rules)\n",
        "- [Dynamic Task Mapping](https://airflow.apache.org/docs/apache-airflow/3.1.1/core-concepts/dynamic-task-mapping.html)\n",
        "- [TaskGroups](https://airflow.apache.org/docs/apache-airflow/3.1.1/core-concepts/taskgroup.html)\n",
        "\n",
        "### üí° Exercises:\n",
        "1. T·∫°o DAG v·ªõi branching d·ª±a tr√™n data quality\n",
        "2. S·ª≠ d·ª•ng dynamic mapping ƒë·ªÉ process multiple files\n",
        "3. T·∫°o TaskGroups cho ETL pipeline\n",
        "4. Implement error handling v·ªõi trigger rules\n",
        "5. X√¢y d·ª±ng complex workflow v·ªõi nhi·ªÅu branches\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
