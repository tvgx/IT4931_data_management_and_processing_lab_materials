{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 4: Task Dependencies and Branching - Managing Workflow Logic\n",
        "\n",
        "## ðŸŽ¯ Objectives\n",
        "- Understand how to define task dependencies with bitshift operators\n",
        "- Use BranchPythonOperator for conditional logic\n",
        "- Apply trigger rules to handle failures\n",
        "- Create dynamic tasks with task mapping\n",
        "- Use TaskGroups to organize tasks\n",
        "- Build complex conditional workflows\n",
        "\n",
        "## ðŸ“‹ Prerequisites\n",
        "- Completed Lab 1-3\n",
        "- Understand operators and tasks\n",
        "- Airflow cluster is running\n",
        "\n",
        "## ðŸ—ï¸ Dependencies Overview\n",
        "Task dependencies in Airflow can be defined using:\n",
        "- **Bitshift operators**: `>>` (set downstream), `<<` (set upstream)\n",
        "- **Methods**: `set_downstream()`, `set_upstream()`\n",
        "- **Lists**: Multiple dependencies at once\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Airflow dependencies and branching\n",
        "from airflow.sdk import DAG, task\n",
        "from airflow.providers.standard.operators.empty import EmptyOperator\n",
        "from airflow.providers.standard.operators.python import BranchPythonOperator, PythonOperator\n",
        "from airflow.providers.standard.operators.bash import BashOperator\n",
        "from airflow.sdk.task_group import TaskGroup\n",
        "\n",
        "import pendulum\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "print(\"âœ… Airflow dependencies vÃ  branching modules imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Bitshift Operators - Defining Dependencies\n",
        "\n",
        "Airflow uses bitshift operators (`>>` and `<<`) to define dependencies in a visual and Pythonic way.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG with bitshift operators\n",
        "@dag(\n",
        "    dag_id=\"bitshift_dependencies_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"dependencies\", \"bitshift\"],\n",
        ")\n",
        "def bitshift_dependencies_dag():\n",
        "    \"\"\"\n",
        "    ### Bitshift Operators Example\n",
        "    Sample DAG using bitshift operators to define dependencies.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create tasks\n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "    task_a = EmptyOperator(task_id=\"task_a\")\n",
        "    task_b = EmptyOperator(task_id=\"task_b\")\n",
        "    task_c = EmptyOperator(task_id=\"task_c\")\n",
        "    task_d = EmptyOperator(task_id=\"task_d\")\n",
        "    end = EmptyOperator(task_id=\"end\")\n",
        "    \n",
        "    # Approach 1: Single dependency with >>\n",
        "    # start >> task_a means task_a depends on start\n",
        "    start >> task_a\n",
        "    \n",
        "    # Approach 2: Multiple downstream tasks\n",
        "    # task_a runs before task_b and task_c\n",
        "    task_a >> [task_b, task_c]\n",
        "    \n",
        "    # Approach 3: Multiple upstream tasks\n",
        "    # task_d depends on both task_b and task_c\n",
        "    [task_b, task_c] >> task_d\n",
        "    \n",
        "    # Approach 4: Chain dependencies\n",
        "    task_d >> end\n",
        "    \n",
        "    # Or write more concisely:\n",
        "    # start >> task_a >> [task_b, task_c] >> task_d >> end\n",
        "\n",
        "# Create DAG\n",
        "bitshift_dag = bitshift_dependencies_dag()\n",
        "\n",
        "print(\"âœ… Bitshift Dependencies DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in bitshift_dag.tasks]}\")\n",
        "print(\"\\nðŸ“Š Dependency Flow:\")\n",
        "print(\"start â†’ task_a â†’ [task_b, task_c] â†’ task_d â†’ end\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. BranchPythonOperator - Conditional Branching\n",
        "\n",
        "BranchPythonOperator allows selecting which branch will run based on conditions. Function must return task_id(s) of the next task(s).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG with BranchPythonOperator\n",
        "@dag(\n",
        "    dag_id=\"branching_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"branching\", \"conditional\"],\n",
        ")\n",
        "def branching_dag():\n",
        "    \"\"\"\n",
        "    ### Branching Example\n",
        "    Sample DAG using BranchPythonOperator for conditional logic.\n",
        "    \"\"\"\n",
        "    \n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "    \n",
        "    # Branch function - return task_id(s) of task(s) that will run\n",
        "    def choose_branch(**context):\n",
        "        \"\"\"Choose branch based on condition\"\"\"\n",
        "        execution_date = context['data_interval_start']\n",
        "        day_of_week = execution_date.weekday()\n",
        "        \n",
        "        # Monday (0) or Friday (4): run both branches\n",
        "        if day_of_week == 0 or day_of_week == 4:\n",
        "            return [\"branch_a\", \"branch_b\"]\n",
        "        # Weekend: only run branch_a\n",
        "        elif day_of_week >= 5:\n",
        "            return \"branch_a\"\n",
        "        # Weekday: only run branch_b\n",
        "        else:\n",
        "            return \"branch_b\"\n",
        "    \n",
        "    branching = BranchPythonOperator(\n",
        "        task_id=\"branching\",\n",
        "        python_callable=choose_branch,\n",
        "    )\n",
        "    \n",
        "    # Branch tasks\n",
        "    branch_a = EmptyOperator(task_id=\"branch_a\")\n",
        "    branch_b = EmptyOperator(task_id=\"branch_b\")\n",
        "    \n",
        "    # Join task - runs after branches complete\n",
        "    join = EmptyOperator(\n",
        "        task_id=\"join\",\n",
        "        trigger_rule=\"none_failed_min_one_success\",  # Run if at least 1 branch succeeds\n",
        "    )\n",
        "    \n",
        "    end = EmptyOperator(task_id=\"end\")\n",
        "    \n",
        "    # Define dependencies\n",
        "    start >> branching\n",
        "    branching >> branch_a >> join\n",
        "    branching >> branch_b >> join\n",
        "    join >> end\n",
        "\n",
        "# Create DAG\n",
        "branching_dag_instance = branching_dag()\n",
        "\n",
        "print(\"âœ… Branching DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in branching_dag_instance.tasks]}\")\n",
        "print(\"\\nðŸ’¡ Branch logic:\")\n",
        "print(\"  - Monday/Friday: Run both branches\")\n",
        "print(\"  - Weekend: Run branch_a only\")\n",
        "print(\"  - Weekday: Run branch_b only\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Trigger Rules - Handling Task Failures\n",
        "\n",
        "Trigger rules determine when a task will run based on the status of upstream tasks. Common trigger rules:\n",
        "- `all_success` (default): All upstream tasks must succeed\n",
        "- `all_failed`: All upstream tasks must fail\n",
        "- `all_done`: Run when all upstream tasks complete (regardless of status)\n",
        "- `one_failed`: Run when at least 1 upstream task failed\n",
        "- `one_success`: Run when at least 1 upstream task succeeds\n",
        "- `none_failed`: Run when no upstream tasks failed (success or skipped)\n",
        "- `none_failed_min_one_success`: Run when no failed and at least 1 success\n",
        "- `none_skipped`: Run when no upstream tasks are skipped\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG with Trigger Rules\n",
        "@dag(\n",
        "    dag_id=\"trigger_rules_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"trigger-rules\", \"error-handling\"],\n",
        ")\n",
        "def trigger_rules_dag():\n",
        "    \"\"\"\n",
        "    ### Trigger Rules Example\n",
        "    Sample DAG demonstrating different trigger rules.\n",
        "    \"\"\"\n",
        "    \n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "    \n",
        "    # Task that might fail\n",
        "    def task_that_might_fail(**context):\n",
        "        \"\"\"Task that may fail based on condition\"\"\"\n",
        "        import random\n",
        "        if random.random() < 0.5:\n",
        "            raise Exception(\"Random failure occurred\")\n",
        "        return \"Success\"\n",
        "    \n",
        "    might_fail = PythonOperator(\n",
        "        task_id=\"might_fail\",\n",
        "        python_callable=task_that_might_fail,\n",
        "    )\n",
        "    \n",
        "    # Task that always succeeds\n",
        "    always_succeed = EmptyOperator(task_id=\"always_succeed\")\n",
        "    \n",
        "    # Task with trigger_rule=\"all_done\" - runs regardless of upstream status\n",
        "    cleanup_all_done = EmptyOperator(\n",
        "        task_id=\"cleanup_all_done\",\n",
        "        trigger_rule=\"all_done\",\n",
        "    )\n",
        "    \n",
        "    # Task with trigger_rule=\"one_failed\" - runs if any task failed\n",
        "    handle_failure = EmptyOperator(\n",
        "        task_id=\"handle_failure\",\n",
        "        trigger_rule=\"one_failed\",\n",
        "    )\n",
        "    \n",
        "    # Task with trigger_rule=\"none_failed_min_one_success\" - runs if no failed and has success\n",
        "    final_task = EmptyOperator(\n",
        "        task_id=\"final_task\",\n",
        "        trigger_rule=\"none_failed_min_one_success\",\n",
        "    )\n",
        "    \n",
        "    end = EmptyOperator(\n",
        "        task_id=\"end\",\n",
        "        trigger_rule=\"all_done\",  # Always runs\n",
        "    )\n",
        "    \n",
        "    # Define dependencies\n",
        "    start >> [might_fail, always_succeed]\n",
        "    [might_fail, always_succeed] >> cleanup_all_done\n",
        "    might_fail >> handle_failure\n",
        "    [cleanup_all_done, handle_failure] >> final_task >> end\n",
        "\n",
        "# Create DAG\n",
        "trigger_rules_dag_instance = trigger_rules_dag()\n",
        "\n",
        "print(\"âœ… Trigger Rules DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in trigger_rules_dag_instance.tasks]}\")\n",
        "print(\"\\nðŸ“Š Trigger Rules:\")\n",
        "print(\"  - cleanup_all_done: all_done (runs regardless of status)\")\n",
        "print(\"  - handle_failure: one_failed (runs if there's a failure)\")\n",
        "print(\"  - final_task: none_failed_min_one_success (runs if no failed and has success)\")\n",
        "print(\"  - end: all_done (always runs)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Dynamic Task Mapping - Creating Dynamic Tasks\n",
        "\n",
        "Dynamic Task Mapping allows creating multiple task instances from one task definition based on input data. Very useful for:\n",
        "- Processing multiple files\n",
        "- Batch operations\n",
        "- Parallel processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG with Dynamic Task Mapping\n",
        "@dag(\n",
        "    dag_id=\"dynamic_task_mapping_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"dynamic\", \"mapping\"],\n",
        ")\n",
        "def dynamic_mapping_dag():\n",
        "    \"\"\"\n",
        "    ### Dynamic Task Mapping Example\n",
        "    Sample DAG using dynamic task mapping to create dynamic tasks.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Task that creates list of items to process\n",
        "    @task\n",
        "    def get_items_to_process():\n",
        "        \"\"\"Return list of items to process\"\"\"\n",
        "        items = [\n",
        "            {\"id\": 1, \"name\": \"item_1\", \"value\": 100},\n",
        "            {\"id\": 2, \"name\": \"item_2\", \"value\": 200},\n",
        "            {\"id\": 3, \"name\": \"item_3\", \"value\": 300},\n",
        "        ]\n",
        "        print(f\"Generated {len(items)} items to process\")\n",
        "        return items\n",
        "    \n",
        "    # Task with dynamic mapping - will create multiple task instances\n",
        "    @task\n",
        "    def process_item(item: dict):\n",
        "        \"\"\"Process one item - will be mapped for each item\"\"\"\n",
        "        print(f\"Processing {item['name']} with value {item['value']}\")\n",
        "        result = item['value'] * 2\n",
        "        print(f\"Result: {result}\")\n",
        "        return {\"item_id\": item['id'], \"result\": result}\n",
        "    \n",
        "    # Task to aggregate results\n",
        "    @task\n",
        "    def aggregate_results(results: list):\n",
        "        \"\"\"Aggregate results from all tasks\"\"\"\n",
        "        total = sum(r['result'] for r in results)\n",
        "        print(f\"Aggregated total: {total}\")\n",
        "        return total\n",
        "    \n",
        "    # Define workflow with dynamic mapping\n",
        "    items = get_items_to_process()\n",
        "    # .expand() creates dynamic task instances for each item\n",
        "    processed_items = process_item.expand(item=items)\n",
        "    aggregate_results(processed_items)\n",
        "\n",
        "# Create DAG\n",
        "dynamic_mapping_dag_instance = dynamic_mapping_dag()\n",
        "\n",
        "print(\"âœ… Dynamic Task Mapping DAG created!\")\n",
        "print(\"\\nðŸ’¡ Dynamic mapping sáº½ táº¡o:\")\n",
        "print(\"  - 1 task instance for get_items_to_process\")\n",
        "print(\"  - 3 task instances for process_item (one for each item)\")\n",
        "print(\"  - 1 task instance for aggregate_results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. TaskGroups - Organizing Tasks\n",
        "\n",
        "TaskGroups allow grouping tasks together to:\n",
        "- Improve visualization in UI\n",
        "- Better organize code\n",
        "- Apply settings to groups of tasks\n",
        "- Create sub-workflows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAG with TaskGroups\n",
        "@dag(\n",
        "    dag_id=\"taskgroup_example\",\n",
        "    schedule=None,\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"taskgroup\", \"organization\"],\n",
        ")\n",
        "def taskgroup_dag():\n",
        "    \"\"\"\n",
        "    ### TaskGroup Example\n",
        "    Sample DAG using TaskGroups to organize tasks.\n",
        "    \"\"\"\n",
        "    \n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "    \n",
        "    # TaskGroup 1: Data Extraction\n",
        "    with TaskGroup(\"extract_group\") as extract_group:\n",
        "        extract_a = EmptyOperator(task_id=\"extract_source_a\")\n",
        "        extract_b = EmptyOperator(task_id=\"extract_source_b\")\n",
        "        extract_c = EmptyOperator(task_id=\"extract_source_c\")\n",
        "        \n",
        "        # Dependencies within group\n",
        "        extract_a >> [extract_b, extract_c]\n",
        "    \n",
        "    # TaskGroup 2: Data Transformation\n",
        "    with TaskGroup(\"transform_group\") as transform_group:\n",
        "        transform_a = EmptyOperator(task_id=\"transform_a\")\n",
        "        transform_b = EmptyOperator(task_id=\"transform_b\")\n",
        "        \n",
        "        transform_a >> transform_b\n",
        "    \n",
        "    # TaskGroup 3: Data Loading\n",
        "    with TaskGroup(\"load_group\") as load_group:\n",
        "        load_a = EmptyOperator(task_id=\"load_destination_a\")\n",
        "        load_b = EmptyOperator(task_id=\"load_destination_b\")\n",
        "        \n",
        "        # Parallel loading\n",
        "        [load_a, load_b]\n",
        "    \n",
        "    end = EmptyOperator(task_id=\"end\")\n",
        "    \n",
        "    # Define dependencies between groups\n",
        "    start >> extract_group >> transform_group >> load_group >> end\n",
        "\n",
        "# Create DAG\n",
        "taskgroup_dag_instance = taskgroup_dag()\n",
        "\n",
        "print(\"âœ… TaskGroup DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in taskgroup_dag_instance.tasks]}\")\n",
        "print(\"\\nðŸ“Š Task Groups:\")\n",
        "print(\"  - extract_group: extract_source_a â†’ [extract_source_b, extract_source_c]\")\n",
        "print(\"  - transform_group: transform_a â†’ transform_b\")\n",
        "print(\"  - load_group: [load_destination_a, load_destination_b] (parallel)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Complex Branching Scenario - Real-world Example\n",
        "\n",
        "Create a complex workflow with multiple branches and conditions to demonstrate how to use branching in practice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complex Branching Scenario\n",
        "@dag(\n",
        "    dag_id=\"complex_branching_scenario\",\n",
        "    schedule=\"@daily\",\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"complex\", \"branching\", \"real-world\"],\n",
        ")\n",
        "def complex_branching_dag():\n",
        "    \"\"\"\n",
        "    ### Complex Branching Scenario\n",
        "    Sample DAG with complex branching logic for real-world use case.\n",
        "    \"\"\"\n",
        "    \n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "    \n",
        "    # Check data quality\n",
        "    def check_data_quality(**context):\n",
        "        \"\"\"Check data quality\"\"\"\n",
        "        # Simulate data quality check\n",
        "        import random\n",
        "        quality_score = random.uniform(0.7, 1.0)\n",
        "        \n",
        "        if quality_score >= 0.9:\n",
        "            return \"high_quality\"\n",
        "        elif quality_score >= 0.8:\n",
        "            return \"medium_quality\"\n",
        "        else:\n",
        "            return \"low_quality\"\n",
        "    \n",
        "    quality_check = BranchPythonOperator(\n",
        "        task_id=\"check_data_quality\",\n",
        "        python_callable=check_data_quality,\n",
        "    )\n",
        "    \n",
        "    # Different processing paths based on quality\n",
        "    high_quality_process = EmptyOperator(task_id=\"high_quality_process\")\n",
        "    medium_quality_process = EmptyOperator(task_id=\"medium_quality_process\")\n",
        "    low_quality_process = EmptyOperator(task_id=\"low_quality_process\")\n",
        "    \n",
        "    # Data cleaning for low quality\n",
        "    clean_data = EmptyOperator(task_id=\"clean_data\")\n",
        "    low_quality_process >> clean_data\n",
        "    \n",
        "    # Join after processing\n",
        "    join_processing = EmptyOperator(\n",
        "        task_id=\"join_processing\",\n",
        "        trigger_rule=\"none_failed_min_one_success\",\n",
        "    )\n",
        "    \n",
        "    # Final validation\n",
        "    validate_output = EmptyOperator(task_id=\"validate_output\")\n",
        "    \n",
        "    # Notification based on result\n",
        "    def send_notification(**context):\n",
        "        \"\"\"Send notification based on result\"\"\"\n",
        "        ti = context['ti']\n",
        "        # Check if validation passed\n",
        "        print(\"Sending notification...\")\n",
        "        return \"notification_sent\"\n",
        "    \n",
        "    send_notif = PythonOperator(\n",
        "        task_id=\"send_notification\",\n",
        "        python_callable=send_notification,\n",
        "    )\n",
        "    \n",
        "    end = EmptyOperator(\n",
        "        task_id=\"end\",\n",
        "        trigger_rule=\"all_done\",\n",
        "    )\n",
        "    \n",
        "    # Define dependencies\n",
        "    start >> quality_check\n",
        "    quality_check >> high_quality_process >> join_processing\n",
        "    quality_check >> medium_quality_process >> join_processing\n",
        "    quality_check >> low_quality_process\n",
        "    clean_data >> join_processing\n",
        "    join_processing >> validate_output >> send_notif >> end\n",
        "\n",
        "# Create DAG\n",
        "complex_branching_dag_instance = complex_branching_dag()\n",
        "\n",
        "print(\"âœ… Complex Branching DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in complex_branching_dag_instance.tasks]}\")\n",
        "print(\"\\nðŸ“Š Workflow:\")\n",
        "print(\"  start â†’ check_data_quality â†’ [high/medium/low_quality_process]\")\n",
        "print(\"  low_quality_process â†’ clean_data â†’ join_processing\")\n",
        "print(\"  join_processing â†’ validate_output â†’ send_notification â†’ end\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Next Steps\n",
        "\n",
        "### âœ… What we learned:\n",
        "1. Bitshift operators (>>, <<) - Define dependencies\n",
        "2. BranchPythonOperator - Conditional branching\n",
        "3. Trigger rules - Handle failures and skipped tasks\n",
        "4. Dynamic Task Mapping - Create dynamic tasks\n",
        "5. TaskGroups - Organize tasks\n",
        "6. Complex branching scenarios - Real-world use cases\n",
        "7. Best practices and common pitfalls\n",
        "\n",
        "### ðŸ“š Next Lab:\n",
        "- **Lab 5**: XCom and Data Sharing\n",
        "- XCom push/pull\n",
        "- Task return values\n",
        "- Custom XCom backends\n",
        "- Data passing best practices\n",
        "\n",
        "### ðŸ”— Useful Links:\n",
        "- [Task Dependencies](https://airflow.apache.org/docs/apache-airflow/3.1.1/core-concepts/dags.html#task-dependencies)\n",
        "- [Branching](https://airflow.apache.org/docs/apache-airflow/3.1.1/core-concepts/dags.html#branching)\n",
        "- [Trigger Rules](https://airflow.apache.org/docs/apache-airflow/3.1.1/core-concepts/tasks.html#trigger-rules)\n",
        "- [Dynamic Task Mapping](https://airflow.apache.org/docs/apache-airflow/3.1.1/core-concepts/dynamic-task-mapping.html)\n",
        "- [TaskGroups](https://airflow.apache.org/docs/apache-airflow/3.1.1/core-concepts/taskgroup.html)\n",
        "\n",
        "### ðŸ’¡ Exercises:\n",
        "1. Create DAG with branching based on data quality\n",
        "2. Use dynamic mapping to process multiple files\n",
        "3. Create TaskGroups for ETL pipeline\n",
        "4. Implement error handling with trigger rules\n",
        "5. Build complex workflow with multiple branches\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
