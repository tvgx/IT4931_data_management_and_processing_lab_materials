{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 7: End-to-End Pipeline Integration - Integrating Airflow with Data Ecosystem\n",
        "\n",
        "## üéØ Objectives\n",
        "- Integrate Airflow with Kafka to ingest streaming data\n",
        "- Orchestrate Spark jobs with Airflow\n",
        "- Connect to databases (PostgreSQL, MongoDB)\n",
        "- Build complete data pipeline from source to destination\n",
        "- Implement error handling and recovery strategies\n",
        "- Monitoring and alerting for pipelines\n",
        "- Best practices for production pipelines\n",
        "\n",
        "## üìã Prerequisites\n",
        "- Completed Lab 1-6\n",
        "- Kafka Lab is running (optional)\n",
        "- Spark Lab is running (optional)\n",
        "- Understand concepts from previous labs\n",
        "\n",
        "## üèóÔ∏è Pipeline Architecture Overview\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Kafka  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Airflow ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Spark  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇDatabase  ‚îÇ\n",
        "‚îÇ (Source)‚îÇ     ‚îÇ(Orchestr)‚îÇ     ‚îÇ(Process)‚îÇ     ‚îÇ(Dest)    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "     ‚îÇ                ‚îÇ                ‚îÇ                ‚îÇ\n",
        "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                    Airflow Orchestrates All\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Airflow and integration modules\n",
        "from airflow.sdk import DAG, task\n",
        "from airflow.providers.standard.operators.python import PythonOperator\n",
        "from airflow.providers.standard.operators.bash import BashOperator\n",
        "from airflow.providers.standard.operators.empty import EmptyOperator\n",
        "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
        "from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n",
        "\n",
        "# Kafka integration\n",
        "try:\n",
        "    from kafka import KafkaConsumer, KafkaProducer\n",
        "    KAFKA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    KAFKA_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è  kafka-python not available. Install with: pip install kafka-python\")\n",
        "\n",
        "# Spark integration\n",
        "try:\n",
        "    from pyspark.sql import SparkSession\n",
        "    SPARK_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SPARK_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è  pyspark not available. Install with: pip install pyspark\")\n",
        "\n",
        "import pendulum\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(\"‚úÖ Integration modules imported successfully!\")\n",
        "print(f\"Kafka available: {KAFKA_AVAILABLE}\")\n",
        "print(f\"Spark available: {SPARK_AVAILABLE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ETL Pipeline with Multiple Data Sources\n",
        "\n",
        "Create a complete ETL pipeline with multiple data sources and destinations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete ETL Pipeline\n",
        "@dag(\n",
        "    dag_id=\"complete_etl_pipeline\",\n",
        "    schedule=\"@daily\",\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    default_args={\n",
        "        \"retries\": 2,\n",
        "        \"retry_delay\": timedelta(minutes=5),\n",
        "    },\n",
        "    tags=[\"etl\", \"pipeline\", \"integration\"],\n",
        ")\n",
        "def complete_etl_pipeline():\n",
        "    \"\"\"\n",
        "    ### Complete ETL Pipeline\n",
        "    End-to-end ETL pipeline with multiple sources and destinations.\n",
        "    \"\"\"\n",
        "    \n",
        "    @task\n",
        "    def extract_from_api(**context):\n",
        "        \"\"\"Extract data from API\"\"\"\n",
        "        execution_date = context['ds']\n",
        "        print(f\"Extracting data from API for date: {execution_date}\")\n",
        "        \n",
        "        # Simulate API call\n",
        "        data = {\n",
        "            \"date\": execution_date,\n",
        "            \"records\": [\n",
        "                {\"id\": 1, \"name\": \"Product A\", \"sales\": 1000},\n",
        "                {\"id\": 2, \"name\": \"Product B\", \"sales\": 2000},\n",
        "                {\"id\": 3, \"name\": \"Product C\", \"sales\": 1500},\n",
        "            ],\n",
        "            \"source\": \"api\",\n",
        "        }\n",
        "        \n",
        "        # Save to temporary file\n",
        "        output_path = f\"/tmp/airflow_data/api_data_{execution_date}.json\"\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "        \n",
        "        print(f\"Extracted {len(data['records'])} records\")\n",
        "        return output_path\n",
        "    \n",
        "    @task\n",
        "    def extract_from_file(**context):\n",
        "        \"\"\"Extract data from file system\"\"\"\n",
        "        execution_date = context['ds']\n",
        "        print(f\"Extracting data from files for date: {execution_date}\")\n",
        "        \n",
        "        # Simulate file extraction\n",
        "        data = {\n",
        "            \"date\": execution_date,\n",
        "            \"records\": [\n",
        "                {\"id\": 4, \"name\": \"Product D\", \"sales\": 800},\n",
        "                {\"id\": 5, \"name\": \"Product E\", \"sales\": 1200},\n",
        "            ],\n",
        "            \"source\": \"file\",\n",
        "        }\n",
        "        \n",
        "        output_path = f\"/tmp/airflow_data/file_data_{execution_date}.json\"\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "        \n",
        "        print(f\"Extracted {len(data['records'])} records\")\n",
        "        return output_path\n",
        "    \n",
        "    @task\n",
        "    def transform_data(api_path: str, file_path: str):\n",
        "        \"\"\"Transform and merge data from multiple sources\"\"\"\n",
        "        print(\"Transforming and merging data...\")\n",
        "        \n",
        "        # Load data from both sources\n",
        "        with open(api_path, 'r') as f:\n",
        "            api_data = json.load(f)\n",
        "        \n",
        "        with open(file_path, 'r') as f:\n",
        "            file_data = json.load(f)\n",
        "        \n",
        "        # Merge and transform\n",
        "        all_records = api_data['records'] + file_data['records']\n",
        "        \n",
        "        # Calculate statistics\n",
        "        total_sales = sum(record['sales'] for record in all_records)\n",
        "        avg_sales = total_sales / len(all_records) if all_records else 0\n",
        "        \n",
        "        transformed = {\n",
        "            \"date\": api_data['date'],\n",
        "            \"total_records\": len(all_records),\n",
        "            \"total_sales\": total_sales,\n",
        "            \"average_sales\": avg_sales,\n",
        "            \"records\": all_records,\n",
        "        }\n",
        "        \n",
        "        # Save transformed data\n",
        "        output_path = f\"/tmp/airflow_data/transformed_{api_data['date']}.json\"\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(transformed, f, indent=2)\n",
        "        \n",
        "        print(f\"Transformed {transformed['total_records']} records\")\n",
        "        print(f\"Total sales: {transformed['total_sales']}\")\n",
        "        return output_path\n",
        "    \n",
        "    @task\n",
        "    def load_to_database(transformed_path: str, **context):\n",
        "        \"\"\"Load transformed data to database\"\"\"\n",
        "        execution_date = context['ds']\n",
        "        print(f\"Loading data to database for date: {execution_date}\")\n",
        "        \n",
        "        # Load transformed data\n",
        "        with open(transformed_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        # In practice, would insert into PostgreSQL\n",
        "        print(f\"Would insert {data['total_records']} records to database\")\n",
        "        print(f\"Total sales: {data['total_sales']}\")\n",
        "        \n",
        "        # Simulate database insert\n",
        "        return {\n",
        "            \"status\": \"loaded\",\n",
        "            \"records_inserted\": data['total_records'],\n",
        "            \"date\": execution_date,\n",
        "        }\n",
        "    \n",
        "    @task\n",
        "    def generate_report(load_result: dict):\n",
        "        \"\"\"Generate summary report\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"ETL Pipeline Report\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Date: {load_result['date']}\")\n",
        "        print(f\"Status: {load_result['status']}\")\n",
        "        print(f\"Records Inserted: {load_result['records_inserted']}\")\n",
        "        print(\"=\" * 60)\n",
        "        return \"Report generated\"\n",
        "    \n",
        "    # Define workflow\n",
        "    api_data = extract_from_api()\n",
        "    file_data = extract_from_file()\n",
        "    transformed_data = transform_data(api_data, file_data)\n",
        "    load_result = load_to_database(transformed_data)\n",
        "    generate_report(load_result)\n",
        "\n",
        "# Create DAG\n",
        "complete_etl_pipeline_instance = complete_etl_pipeline()\n",
        "\n",
        "print(\"‚úÖ Complete ETL Pipeline DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in complete_etl_pipeline_instance.tasks]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Kafka Integration - Stream Data Ingestion\n",
        "\n",
        "Integrate Airflow with Kafka to ingest streaming data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kafka Integration Pipeline\n",
        "if KAFKA_AVAILABLE:\n",
        "    @dag(\n",
        "        dag_id=\"kafka_integration_pipeline\",\n",
        "        schedule=\"@hourly\",\n",
        "        start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "        catchup=False,\n",
        "        tags=[\"kafka\", \"streaming\", \"integration\"],\n",
        "    )\n",
        "    def kafka_integration_pipeline():\n",
        "        \"\"\"\n",
        "        ### Kafka Integration Pipeline\n",
        "        Ingest data t·ª´ Kafka v√† process v·ªõi Airflow.\n",
        "        \"\"\"\n",
        "        \n",
        "        def consume_kafka_messages(**context):\n",
        "            \"\"\"Consume messages t·ª´ Kafka topic\"\"\"\n",
        "            kafka_bootstrap_servers = \"localhost:9092\"\n",
        "            topic_name = \"stock-data\"\n",
        "            \n",
        "            print(f\"Consuming messages from Kafka topic: {topic_name}\")\n",
        "            \n",
        "            try:\n",
        "                consumer = KafkaConsumer(\n",
        "                    topic_name,\n",
        "                    bootstrap_servers=kafka_bootstrap_servers,\n",
        "                    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
        "                    auto_offset_reset='latest',\n",
        "                    consumer_timeout_ms=5000,  # Timeout sau 5 gi√¢y\n",
        "                )\n",
        "                \n",
        "                messages = []\n",
        "                for message in consumer:\n",
        "                    messages.append(message.value)\n",
        "                    if len(messages) >= 10:  # Limit s·ªë messages\n",
        "                        break\n",
        "                \n",
        "                consumer.close()\n",
        "                \n",
        "                print(f\"Consumed {len(messages)} messages from Kafka\")\n",
        "                \n",
        "                # Save messages\n",
        "                output_path = \"/tmp/airflow_data/kafka_messages.json\"\n",
        "                os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "                with open(output_path, 'w') as f:\n",
        "                    json.dump(messages, f, indent=2)\n",
        "                \n",
        "                return output_path\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Kafka consumption failed: {e}\")\n",
        "                print(\"üí° Make sure Kafka is running: docker compose up -d (in Kafka_lab)\")\n",
        "                # Return empty file path ƒë·ªÉ pipeline kh√¥ng fail\n",
        "                return None\n",
        "        \n",
        "        def process_kafka_data(file_path: str):\n",
        "            \"\"\"Process data t·ª´ Kafka\"\"\"\n",
        "            if not file_path or not os.path.exists(file_path):\n",
        "                print(\"No Kafka data to process\")\n",
        "                return {\"status\": \"skipped\", \"records\": 0}\n",
        "            \n",
        "            with open(file_path, 'r') as f:\n",
        "                messages = json.load(f)\n",
        "            \n",
        "            # Process messages\n",
        "            total_volume = sum(msg.get('volume', 0) for msg in messages)\n",
        "            avg_price = sum(msg.get('close', 0) for msg in messages) / len(messages) if messages else 0\n",
        "            \n",
        "            result = {\n",
        "                \"status\": \"processed\",\n",
        "                \"total_messages\": len(messages),\n",
        "                \"total_volume\": total_volume,\n",
        "                \"average_price\": avg_price,\n",
        "            }\n",
        "            \n",
        "            print(f\"Processed Kafka data: {result}\")\n",
        "            return result\n",
        "        \n",
        "        # Tasks\n",
        "        consume_task = PythonOperator(\n",
        "            task_id=\"consume_kafka_messages\",\n",
        "            python_callable=consume_kafka_messages,\n",
        "        )\n",
        "        \n",
        "        process_task = PythonOperator(\n",
        "            task_id=\"process_kafka_data\",\n",
        "            python_callable=process_kafka_data,\n",
        "            op_args=[consume_task.output],  # Pass output t·ª´ previous task\n",
        "        )\n",
        "        \n",
        "        consume_task >> process_task\n",
        "    \n",
        "    kafka_pipeline = kafka_integration_pipeline()\n",
        "    print(\"‚úÖ Kafka Integration Pipeline DAG created!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Kafka integration DAG not created (kafka-python not available)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Database Integration Pipeline\n",
        "@dag(\n",
        "    dag_id=\"database_integration_pipeline\",\n",
        "    schedule=\"@daily\",\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"database\", \"postgresql\", \"integration\"],\n",
        ")\n",
        "def database_integration_pipeline():\n",
        "    \"\"\"\n",
        "    ### Database Integration Pipeline\n",
        "    T√≠ch h·ª£p v·ªõi PostgreSQL ƒë·ªÉ th·ª±c hi·ªán database operations.\n",
        "    \n",
        "    Note: Setup PostgreSQL connection trong Airflow UI:\n",
        "    - Connection ID: postgres_default\n",
        "    - Connection Type: Postgres\n",
        "    - Host: postgres\n",
        "    - Schema: airflow\n",
        "    - Login: airflow\n",
        "    - Password: airflow\n",
        "    - Port: 5432\n",
        "    \"\"\"\n",
        "    \n",
        "    # Task 1: Create table\n",
        "    create_table = SQLExecuteQueryOperator(\n",
        "        task_id=\"create_sales_table\",\n",
        "        conn_id=\"postgres_default\",\n",
        "        sql=\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS daily_sales (\n",
        "            id SERIAL PRIMARY KEY,\n",
        "            date DATE NOT NULL,\n",
        "            product_name VARCHAR(100),\n",
        "            sales_amount DECIMAL(10, 2),\n",
        "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "            UNIQUE(date, product_name)\n",
        "        );\n",
        "        \"\"\",\n",
        "    )\n",
        "    \n",
        "    # Task 2: Insert data v·ªõi Jinja templating\n",
        "    def insert_sales_data(**context):\n",
        "        \"\"\"Insert sales data v√†o database\"\"\"\n",
        "        execution_date = context['ds']\n",
        "        \n",
        "        # Sample data\n",
        "        sales_data = [\n",
        "            {\"product\": \"Product A\", \"amount\": 1000.00},\n",
        "            {\"product\": \"Product B\", \"amount\": 2000.00},\n",
        "            {\"product\": \"Product C\", \"amount\": 1500.00},\n",
        "        ]\n",
        "        \n",
        "        hook = PostgresHook(postgres_conn_id=\"postgres_default\")\n",
        "        \n",
        "        for sale in sales_data:\n",
        "            sql = f\"\"\"\n",
        "            INSERT INTO daily_sales (date, product_name, sales_amount)\n",
        "            VALUES ('{execution_date}', '{sale['product']}', {sale['amount']})\n",
        "            ON CONFLICT (date, product_name) DO UPDATE\n",
        "            SET sales_amount = EXCLUDED.sales_amount;\n",
        "            \"\"\"\n",
        "            hook.run(sql)\n",
        "        \n",
        "        print(f\"Inserted {len(sales_data)} records for date: {execution_date}\")\n",
        "        return len(sales_data)\n",
        "    \n",
        "    insert_task = PythonOperator(\n",
        "        task_id=\"insert_sales_data\",\n",
        "        python_callable=insert_sales_data,\n",
        "    )\n",
        "    \n",
        "    # Task 3: Generate report t·ª´ database\n",
        "    generate_report = SQLExecuteQueryOperator(\n",
        "        task_id=\"generate_sales_report\",\n",
        "        conn_id=\"postgres_default\",\n",
        "        sql=\"\"\"\n",
        "        SELECT \n",
        "            date,\n",
        "            COUNT(*) as product_count,\n",
        "            SUM(sales_amount) as total_sales,\n",
        "            AVG(sales_amount) as avg_sales\n",
        "        FROM daily_sales\n",
        "        WHERE date = '{{ ds }}'\n",
        "        GROUP BY date;\n",
        "        \"\"\",\n",
        "    )\n",
        "    \n",
        "    # Define dependencies\n",
        "    create_table >> insert_task >> generate_report\n",
        "\n",
        "# Create DAG\n",
        "database_pipeline = database_integration_pipeline()\n",
        "\n",
        "print(\"‚úÖ Database Integration Pipeline DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in database_pipeline.tasks]}\")\n",
        "print(\"\\nüí° Note: Setup PostgreSQL connection in Airflow UI before running\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Error Handling v√† Recovery Strategies\n",
        "\n",
        "Implement error handling v√† recovery strategies cho production pipelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline v·ªõi Error Handling\n",
        "@dag(\n",
        "    dag_id=\"error_handling_pipeline\",\n",
        "    schedule=\"@daily\",\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    default_args={\n",
        "        \"retries\": 3,\n",
        "        \"retry_delay\": timedelta(minutes=5),\n",
        "        \"on_failure_callback\": None,  # C√≥ th·ªÉ add callback function\n",
        "    },\n",
        "    tags=[\"error-handling\", \"recovery\"],\n",
        ")\n",
        "def error_handling_pipeline():\n",
        "    \"\"\"\n",
        "    ### Error Handling Pipeline\n",
        "    Pipeline v·ªõi comprehensive error handling v√† recovery.\n",
        "    \"\"\"\n",
        "    \n",
        "    from airflow.providers.standard.operators.empty import EmptyOperator\n",
        "    \n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "    \n",
        "    def extract_with_retry(**context):\n",
        "        \"\"\"Extract v·ªõi retry logic\"\"\"\n",
        "        import random\n",
        "        \n",
        "        # Simulate potential failure\n",
        "        if random.random() < 0.3:  # 30% chance of failure\n",
        "            raise Exception(\"Extraction failed - will retry\")\n",
        "        \n",
        "        print(\"Extraction successful\")\n",
        "        return {\"status\": \"extracted\", \"records\": 100}\n",
        "    \n",
        "    extract_task = PythonOperator(\n",
        "        task_id=\"extract_data\",\n",
        "        python_callable=extract_with_retry,\n",
        "        retries=3,\n",
        "        retry_delay=timedelta(minutes=2),\n",
        "    )\n",
        "    \n",
        "    def validate_data(**context):\n",
        "        \"\"\"Validate extracted data\"\"\"\n",
        "        ti = context['ti']\n",
        "        extracted = ti.xcom_pull(task_ids='extract_data')\n",
        "        \n",
        "        if not extracted or extracted.get('records', 0) == 0:\n",
        "            raise ValueError(\"No data extracted - validation failed\")\n",
        "        \n",
        "        print(f\"Validation passed: {extracted['records']} records\")\n",
        "        return extracted\n",
        "    \n",
        "    validate_task = PythonOperator(\n",
        "        task_id=\"validate_data\",\n",
        "        python_callable=validate_data,\n",
        "    )\n",
        "    \n",
        "    def process_data(**context):\n",
        "        \"\"\"Process data\"\"\"\n",
        "        ti = context['ti']\n",
        "        data = ti.xcom_pull(task_ids='validate_data')\n",
        "        print(f\"Processing {data['records']} records\")\n",
        "        return {\"status\": \"processed\"}\n",
        "    \n",
        "    process_task = PythonOperator(\n",
        "        task_id=\"process_data\",\n",
        "        python_callable=process_data,\n",
        "    )\n",
        "    \n",
        "    # Error handling task\n",
        "    def handle_failure(**context):\n",
        "        \"\"\"Handle failures\"\"\"\n",
        "        print(\"Handling failure - sending alerts, cleaning up...\")\n",
        "        return \"Failure handled\"\n",
        "    \n",
        "    handle_failure_task = PythonOperator(\n",
        "        task_id=\"handle_failure\",\n",
        "        python_callable=handle_failure,\n",
        "        trigger_rule=\"one_failed\",  # Ch·∫°y n·∫øu c√≥ task failed\n",
        "    )\n",
        "    \n",
        "    # Cleanup task - lu√¥n ch·∫°y\n",
        "    def cleanup(**context):\n",
        "        \"\"\"Cleanup resources\"\"\"\n",
        "        print(\"Cleaning up temporary files and resources...\")\n",
        "        return \"Cleanup completed\"\n",
        "    \n",
        "    cleanup_task = PythonOperator(\n",
        "        task_id=\"cleanup\",\n",
        "        python_callable=cleanup,\n",
        "        trigger_rule=\"all_done\",  # Lu√¥n ch·∫°y\n",
        "    )\n",
        "    \n",
        "    end = EmptyOperator(\n",
        "        task_id=\"end\",\n",
        "        trigger_rule=\"all_done\",\n",
        "    )\n",
        "    \n",
        "    # Define dependencies\n",
        "    start >> extract_task >> validate_task >> process_task >> end\n",
        "    [extract_task, validate_task, process_task] >> handle_failure_task >> cleanup_task\n",
        "    cleanup_task >> end\n",
        "\n",
        "# Create DAG\n",
        "error_handling_pipeline_instance = error_handling_pipeline()\n",
        "\n",
        "print(\"‚úÖ Error Handling Pipeline DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in error_handling_pipeline_instance.tasks]}\")\n",
        "print(\"\\nüí° Error Handling Strategies:\")\n",
        "print(\"  - Retries v·ªõi exponential backoff\")\n",
        "print(\"  - Validation tasks ƒë·ªÉ catch errors early\")\n",
        "print(\"  - Failure handling tasks v·ªõi trigger_rule='one_failed'\")\n",
        "print(\"  - Cleanup tasks v·ªõi trigger_rule='all_done'\")\n",
        "print(\"  - Callbacks cho notifications\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Monitoring v√† Alerting\n",
        "\n",
        "Implement monitoring v√† alerting cho pipelines ƒë·ªÉ track health v√† performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline v·ªõi Monitoring\n",
        "@dag(\n",
        "    dag_id=\"monitoring_pipeline\",\n",
        "    schedule=\"@daily\",\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    tags=[\"monitoring\", \"alerting\"],\n",
        ")\n",
        "def monitoring_pipeline():\n",
        "    \"\"\"\n",
        "    ### Monitoring Pipeline\n",
        "    Pipeline v·ªõi monitoring v√† alerting capabilities.\n",
        "    \"\"\"\n",
        "    \n",
        "    @task\n",
        "    def track_start(**context):\n",
        "        \"\"\"Track pipeline start\"\"\"\n",
        "        execution_date = context['ds']\n",
        "        dag_run_id = context['dag_run'].dag_id\n",
        "        \n",
        "        metrics = {\n",
        "            \"dag_run_id\": dag_run_id,\n",
        "            \"execution_date\": execution_date,\n",
        "            \"start_time\": datetime.now().isoformat(),\n",
        "            \"status\": \"started\",\n",
        "        }\n",
        "        \n",
        "        print(f\"Pipeline started: {metrics}\")\n",
        "        return metrics\n",
        "    \n",
        "    @task\n",
        "    def process_with_metrics(**context):\n",
        "        \"\"\"Process v·ªõi metrics tracking\"\"\"\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Simulate processing\n",
        "        records_processed = 1000\n",
        "        processing_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        metrics = {\n",
        "            \"records_processed\": records_processed,\n",
        "            \"processing_time_seconds\": processing_time,\n",
        "            \"throughput\": records_processed / processing_time if processing_time > 0 else 0,\n",
        "        }\n",
        "        \n",
        "        print(f\"Processing metrics: {metrics}\")\n",
        "        return metrics\n",
        "    \n",
        "    @task\n",
        "    def check_health(**context):\n",
        "        \"\"\"Health check\"\"\"\n",
        "        ti = context['ti']\n",
        "        metrics = ti.xcom_pull(task_ids='process_with_metrics')\n",
        "        \n",
        "        # Health checks\n",
        "        health_status = {\n",
        "            \"status\": \"healthy\",\n",
        "            \"checks\": {\n",
        "                \"records_processed\": metrics['records_processed'] > 0,\n",
        "                \"processing_time\": metrics['processing_time_seconds'] < 60,\n",
        "                \"throughput\": metrics['throughput'] > 10,\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Check if all health checks pass\n",
        "        if not all(health_status['checks'].values()):\n",
        "            health_status['status'] = \"unhealthy\"\n",
        "            print(f\"‚ö†Ô∏è  Health check failed: {health_status}\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Health check passed: {health_status}\")\n",
        "        \n",
        "        return health_status\n",
        "    \n",
        "    @task\n",
        "    def send_alerts(health_status: dict, **context):\n",
        "        \"\"\"Send alerts n·∫øu c√≥ v·∫•n ƒë·ªÅ\"\"\"\n",
        "        if health_status['status'] == \"unhealthy\":\n",
        "            print(\"üö® Sending alert: Pipeline unhealthy!\")\n",
        "            print(f\"Failed checks: {[k for k, v in health_status['checks'].items() if not v]}\")\n",
        "            # In th·ª±c t·∫ø, s·∫Ω g·ª≠i email/Slack/PagerDuty\n",
        "        else:\n",
        "            print(\"‚úÖ No alerts needed - pipeline healthy\")\n",
        "        \n",
        "        return \"Alerts sent\"\n",
        "    \n",
        "    # Define workflow\n",
        "    start_metrics = track_start()\n",
        "    process_metrics = process_with_metrics()\n",
        "    health_status = check_health()\n",
        "    send_alerts(health_status)\n",
        "\n",
        "# Create DAG\n",
        "monitoring_pipeline_instance = monitoring_pipeline()\n",
        "\n",
        "print(\"‚úÖ Monitoring Pipeline DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in monitoring_pipeline_instance.tasks]}\")\n",
        "print(\"\\nüí° Monitoring Features:\")\n",
        "print(\"  - Track pipeline start/end times\")\n",
        "print(\"  - Collect processing metrics\")\n",
        "print(\"  - Health checks\")\n",
        "print(\"  - Alerting cho failures\")\n",
        "print(\"  - Performance monitoring\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Complete Data Pipeline - End-to-End Example\n",
        "\n",
        "T·∫°o m·ªôt complete data pipeline t√≠ch h·ª£p t·∫•t c·∫£ components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete End-to-End Pipeline\n",
        "@dag(\n",
        "    dag_id=\"complete_data_pipeline\",\n",
        "    schedule=\"@daily\",\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    default_args={\n",
        "        \"retries\": 2,\n",
        "        \"retry_delay\": timedelta(minutes=5),\n",
        "    },\n",
        "    tags=[\"complete\", \"end-to-end\", \"production\"],\n",
        ")\n",
        "def complete_data_pipeline():\n",
        "    \"\"\"\n",
        "    ### Complete Data Pipeline\n",
        "    End-to-end data pipeline t√≠ch h·ª£p t·∫•t c·∫£ components:\n",
        "    - Data extraction t·ª´ multiple sources\n",
        "    - Data transformation\n",
        "    - Data validation\n",
        "    - Data loading\n",
        "    - Monitoring v√† alerting\n",
        "    \"\"\"\n",
        "    \n",
        "    from airflow.sdk.task_group import TaskGroup\n",
        "    \n",
        "    # Start\n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "    \n",
        "    # Extraction Group\n",
        "    with TaskGroup(\"extraction_group\") as extraction_group:\n",
        "        @task\n",
        "        def extract_api():\n",
        "            \"\"\"Extract t·ª´ API\"\"\"\n",
        "            print(\"Extracting from API...\")\n",
        "            return {\"source\": \"api\", \"records\": 100}\n",
        "        \n",
        "        @task\n",
        "        def extract_files():\n",
        "            \"\"\"Extract t·ª´ files\"\"\"\n",
        "            print(\"Extracting from files...\")\n",
        "            return {\"source\": \"files\", \"records\": 50}\n",
        "        \n",
        "        api_data = extract_api()\n",
        "        file_data = extract_files()\n",
        "    \n",
        "    # Transformation Group\n",
        "    with TaskGroup(\"transformation_group\") as transformation_group:\n",
        "        @task\n",
        "        def merge_data(api_data: dict, file_data: dict):\n",
        "            \"\"\"Merge data t·ª´ nhi·ªÅu sources\"\"\"\n",
        "            total_records = api_data['records'] + file_data['records']\n",
        "            print(f\"Merged {total_records} records\")\n",
        "            return {\"total_records\": total_records}\n",
        "        \n",
        "        @task\n",
        "        def transform_data(merged_data: dict):\n",
        "            \"\"\"Transform data\"\"\"\n",
        "            print(f\"Transforming {merged_data['total_records']} records\")\n",
        "            return {\"status\": \"transformed\", **merged_data}\n",
        "        \n",
        "        merged = merge_data(api_data, file_data)\n",
        "        transformed = transform_data(merged)\n",
        "    \n",
        "    # Validation\n",
        "    @task\n",
        "    def validate_data(transformed_data: dict):\n",
        "        \"\"\"Validate transformed data\"\"\"\n",
        "        if transformed_data['total_records'] > 0:\n",
        "            print(\"‚úÖ Validation passed\")\n",
        "            return transformed_data\n",
        "        else:\n",
        "            raise ValueError(\"Validation failed: No records\")\n",
        "    \n",
        "    # Loading Group\n",
        "    with TaskGroup(\"loading_group\") as loading_group:\n",
        "        @task\n",
        "        def load_to_database(validated_data: dict):\n",
        "            \"\"\"Load v√†o database\"\"\"\n",
        "            print(f\"Loading {validated_data['total_records']} records to database\")\n",
        "            return {\"status\": \"loaded\"}\n",
        "        \n",
        "        @task\n",
        "        def load_to_file(validated_data: dict):\n",
        "            \"\"\"Load v√†o file\"\"\"\n",
        "            print(f\"Loading {validated_data['total_records']} records to file\")\n",
        "            return {\"status\": \"saved\"}\n",
        "        \n",
        "        # Load tasks - validated s·∫Ω ƒë∆∞·ª£c pass t·ª´ outside\n",
        "        db_result = load_to_database(validated)\n",
        "        file_result = load_to_file(validated)\n",
        "    \n",
        "    # Monitoring\n",
        "    @task\n",
        "    def generate_report(db_result: dict, file_result: dict, **context):\n",
        "        \"\"\"Generate final report\"\"\"\n",
        "        execution_date = context['ds']\n",
        "        print(\"=\" * 60)\n",
        "        print(\"Pipeline Execution Report\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Date: {execution_date}\")\n",
        "        print(f\"Database Status: {db_result['status']}\")\n",
        "        print(f\"File Status: {file_result['status']}\")\n",
        "        print(\"=\" * 60)\n",
        "        return \"Report generated\"\n",
        "    \n",
        "    # End\n",
        "    end = EmptyOperator(\n",
        "        task_id=\"end\",\n",
        "        trigger_rule=\"all_done\",\n",
        "    )\n",
        "    \n",
        "    # Define workflow\n",
        "    # transformed l√† output t·ª´ transformation_group TaskGroup\n",
        "    validated = validate_data(transformed)\n",
        "    start >> extraction_group >> transformation_group >> validated >> loading_group >> generate_report(db_result, file_result) >> end\n",
        "\n",
        "# Create DAG\n",
        "complete_pipeline_instance = complete_data_pipeline()\n",
        "\n",
        "print(\"‚úÖ Complete Data Pipeline DAG created!\")\n",
        "print(f\"Tasks: {[task.task_id for task in complete_pipeline_instance.tasks]}\")\n",
        "print(\"\\nüìä Pipeline Structure:\")\n",
        "print(\"  start ‚Üí extraction_group ‚Üí transformation_group ‚Üí validate ‚Üí loading_group ‚Üí report ‚Üí end\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Best Practices cho Production Pipelines\n",
        "\n",
        "### ‚úÖ Production Best Practices:\n",
        "\n",
        "1. **Error Handling:**\n",
        "   - Set retries v√† retry_delay ph√π h·ª£p\n",
        "   - Implement validation tasks\n",
        "   - Use trigger rules cho cleanup\n",
        "   - Add failure callbacks\n",
        "\n",
        "2. **Monitoring:**\n",
        "   - Track metrics v√† performance\n",
        "   - Implement health checks\n",
        "   - Set up alerting\n",
        "   - Monitor resource usage\n",
        "\n",
        "3. **Data Quality:**\n",
        "   - Validate data ·ªü m·ªói stage\n",
        "   - Check data completeness\n",
        "   - Verify data schema\n",
        "   - Handle missing data\n",
        "\n",
        "4. **Performance:**\n",
        "   - Use appropriate executors\n",
        "   - Optimize task dependencies\n",
        "   - Parallel processing khi c√≥ th·ªÉ\n",
        "   - Cache intermediate results\n",
        "\n",
        "5. **Security:**\n",
        "   - Use Airflow Connections cho credentials\n",
        "   - Don't hardcode secrets\n",
        "   - Use Variables cho config\n",
        "   - Implement access controls\n",
        "\n",
        "6. **Maintainability:**\n",
        "   - Use TaskGroups ƒë·ªÉ organize\n",
        "   - Add docstrings v√† comments\n",
        "   - Version control DAGs\n",
        "   - Document dependencies\n",
        "\n",
        "### ‚ö†Ô∏è Common Production Issues:\n",
        "\n",
        "1. **Resource exhaustion**: Too many concurrent tasks\n",
        "2. **Data quality issues**: Missing validation\n",
        "3. **Long-running tasks**: Blocking scheduler\n",
        "4. **Memory issues**: Large data trong XCom\n",
        "5. **Network issues**: External service failures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. T√≥m t·∫Øt v√† K·∫øt lu·∫≠n\n",
        "\n",
        "### ‚úÖ Nh·ªØng g√¨ ƒë√£ h·ªçc trong to√†n b·ªô Airflow Lab Series:\n",
        "\n",
        "**Lab 1: Airflow Basics**\n",
        "- Ki·∫øn tr√∫c Airflow\n",
        "- Web UI v√† CLI\n",
        "- REST API\n",
        "\n",
        "**Lab 2: DAGs v√† Tasks**\n",
        "- Task SDK (@dag, @task decorators)\n",
        "- Task dependencies\n",
        "- Error handling\n",
        "\n",
        "**Lab 3: Operators v√† Hooks**\n",
        "- BashOperator, PythonOperator, SQLExecuteQueryOperator\n",
        "- Hooks cho external systems\n",
        "- So s√°nh @task vs Operators\n",
        "\n",
        "**Lab 4: Task Dependencies v√† Branching**\n",
        "- Bitshift operators\n",
        "- BranchPythonOperator\n",
        "- Trigger rules\n",
        "- Dynamic task mapping\n",
        "- TaskGroups\n",
        "\n",
        "**Lab 5: XCom v√† Data Sharing**\n",
        "- XCom push/pull\n",
        "- Task return values\n",
        "- Data passing best practices\n",
        "\n",
        "**Lab 6: Scheduling v√† Timetables**\n",
        "- Cron expressions\n",
        "- Timedelta scheduling\n",
        "- Custom timetables\n",
        "- Catchup v√† data intervals\n",
        "\n",
        "**Lab 7: End-to-End Pipeline Integration**\n",
        "- Kafka integration\n",
        "- Database operations\n",
        "- Error handling v√† recovery\n",
        "- Monitoring v√† alerting\n",
        "- Complete production pipelines\n",
        "\n",
        "### üéØ Key Takeaways:\n",
        "\n",
        "1. **Airflow l√† orchestration tool**, kh√¥ng ph·∫£i processing engine\n",
        "2. **Task SDK** l√† modern approach, nh∆∞ng **Operators** v·∫´n quan tr·ªçng\n",
        "3. **XCom** ch·ªâ cho small data, d√πng file storage cho large data\n",
        "4. **Scheduling** ph·ª©c t·∫°p h∆°n cron - hi·ªÉu data intervals\n",
        "5. **Error handling** l√† critical cho production\n",
        "6. **Monitoring** gi√∫p maintain healthy pipelines\n",
        "\n",
        "### üìö Next Steps:\n",
        "\n",
        "1. **Deploy to Production:**\n",
        "   - Setup Airflow tr√™n production environment\n",
        "   - Configure executors (Celery, Kubernetes)\n",
        "   - Setup monitoring v√† alerting\n",
        "\n",
        "2. **Advanced Topics:**\n",
        "   - Custom operators v√† hooks\n",
        "   - Airflow plugins\n",
        "   - Dynamic DAG generation\n",
        "   - Multi-tenant deployments\n",
        "\n",
        "3. **Integration:**\n",
        "   - Cloud services (AWS, GCP, Azure)\n",
        "   - Data quality tools (Great Expectations)\n",
        "   - Transformation tools (dbt)\n",
        "   - ML pipelines (MLflow, Kubeflow)\n",
        "\n",
        "### üîó Useful Resources:\n",
        "\n",
        "- [Airflow Documentation](https://airflow.apache.org/docs/apache-airflow/3.1.1/)\n",
        "- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/3.1.1/best-practices/)\n",
        "- [Airflow Providers](https://airflow.apache.org/docs/apache-airflow-providers/)\n",
        "- [Airflow GitHub](https://github.com/apache/airflow)\n",
        "- [Airflow Slack Community](https://apache-airflow.slack.com)\n",
        "\n",
        "### üéâ Congratulations!\n",
        "\n",
        "B·∫°n ƒë√£ ho√†n th√†nh to√†n b·ªô Airflow Lab Series! B√¢y gi·ªù b·∫°n c√≥ ƒë·ªß ki·∫øn th·ª©c ƒë·ªÉ:\n",
        "- Thi·∫øt k·∫ø v√† implement data pipelines v·ªõi Airflow\n",
        "- T√≠ch h·ª£p Airflow v·ªõi c√°c h·ªá th·ªëng kh√°c\n",
        "- Deploy v√† maintain production pipelines\n",
        "- Troubleshoot v√† optimize pipelines\n",
        "\n",
        "**Happy Orchestrating! üöÄ**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
