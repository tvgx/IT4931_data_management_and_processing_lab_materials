# Airflow Lab - Summary

## ğŸ‰ Airflow Lab Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng!

### ğŸ“ Cáº¥u trÃºc thÆ° má»¥c:
```
Airflow_lab/
â”œâ”€â”€ docker-compose.yml          # Airflow 3.1.1 vá»›i PostgreSQL
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ setup_airflow_lab.sh       # Setup script
â”œâ”€â”€ README.md                  # HÆ°á»›ng dáº«n chi tiáº¿t
â”œâ”€â”€ SUMMARY.md                 # File nÃ y
â”œâ”€â”€ dags/                      # DAGs máº«u
â”‚   â”œâ”€â”€ hello_world_dag.py
â”‚   â”œâ”€â”€ tutorial_dag.py
â”‚   â”œâ”€â”€ branching_dag.py
â”‚   â””â”€â”€ xcom_dag.py
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â””â”€â”€ 01_airflow_basics.ipynb
â”œâ”€â”€ logs/                      # Airflow logs
â”œâ”€â”€ plugins/                   # Custom plugins
â”œâ”€â”€ config/                    # Airflow config
â””â”€â”€ data/                      # Sample data
```

### ğŸš€ CÃ¡ch sá»­ dá»¥ng:

#### 1. Setup Environment:
```bash
cd Airflow_lab
./setup_airflow_lab.sh
```

#### 2. Initialize Airflow:
```bash
# Set AIRFLOW_UID
export AIRFLOW_UID=$(id -u)
echo "AIRFLOW_UID=$AIRFLOW_UID" > .env

# Initialize database
docker compose up airflow-init
```

#### 3. Start Airflow Services:
```bash
docker compose up -d
```

#### 4. Access Airflow UI:
- **URL**: http://localhost:8080
- **Username**: airflow
- **Password**: airflow

#### 5. Start Jupyter Lab:
```bash
conda activate datalab
jupyter lab
```

#### 6. Run Labs:
- Má»Ÿ `notebooks/01_airflow_basics.ipynb`
- Cháº¡y tá»«ng cell Ä‘á»ƒ há»c Airflow fundamentals

### ğŸŒ Web UIs:
- **Airflow UI**: http://localhost:8080
- **PostgreSQL**: localhost:5432 (airflow/airflow)

### ğŸ“Š Lab Content:

#### **Lab 1: Airflow Basics** âœ…
- Kiáº¿n trÃºc Airflow
- Web UI vÃ  CLI
- REST API
- Trigger vÃ  monitor DAGs

#### **Lab 2: DAGs vÃ  Tasks** (Coming soon)
- Task SDK (@dag, @task decorators)
- Task dependencies
- Error handling

#### **Lab 3: Operators vÃ  Hooks** (Coming soon)
- BashOperator, PythonOperator
- SQLExecuteQueryOperator
- Custom operators

#### **Lab 4: Task Dependencies vÃ  Branching** (Coming soon)
- Bitshift operators
- BranchPythonOperator
- Trigger rules

#### **Lab 5: XCom vÃ  Data Sharing** (Coming soon)
- XCom push/pull
- Task return values
- Data passing

#### **Lab 6: Scheduling vÃ  Timetables** (Coming soon)
- Cron expressions
- Custom timetables
- Catchup vÃ  data intervals

#### **Lab 7: End-to-End Pipeline** (Coming soon)
- Kafka integration
- Spark integration
- Database operations

### ğŸ¯ Learning Outcomes:
Sau khi hoÃ n thÃ nh lab series nÃ y, sinh viÃªn sáº½ cÃ³ thá»ƒ:

1. **Airflow Fundamentals**: Hiá»ƒu kiáº¿n trÃºc vÃ  components
2. **DAG Development**: Táº¡o DAGs vá»›i Task SDK
3. **Operators & Hooks**: Sá»­ dá»¥ng vÃ  táº¡o custom operators
4. **Data Management**: Chia sáº» data vá»›i XCom
5. **Scheduling**: Cáº¥u hÃ¬nh scheduling phá»©c táº¡p
6. **Pipeline Integration**: TÃ­ch há»£p vá»›i cÃ¡c há»‡ thá»‘ng khÃ¡c

### ğŸ”§ Tech Stack:
- **Apache Airflow**: 3.1.1 (latest stable)
- **PostgreSQL**: Metadata database
- **Docker Compose**: Containerized environment
- **Python**: Task SDK, operators, hooks
- **Jupyter**: Interactive learning

### ğŸ“ˆ Use Case: E-commerce Data Pipeline
- **Data Sources**: Kafka streams, databases
- **Processing**: Spark transformations
- **Destination**: Data warehouse, analytics
- **Orchestration**: Airflow workflows

### ğŸ”— Integration vá»›i Labs KhÃ¡c:
- **Kafka Lab**: Stream data ingestion
- **Spark Lab**: Data processing
- **NoSQL Lab**: Database operations
- **PyIceberg Lab**: Data lake operations

---

**Airflow Lab Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ sá»­ dá»¥ng! ğŸš€**

