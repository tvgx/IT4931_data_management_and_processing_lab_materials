# Airflow Lab - Data Pipeline Orchestration vá»›i Apache Airflow

## ğŸ¯ Overview

Lab nÃ y cung cáº¥p kiáº¿n thá»©c thá»±c hÃ nh vá» **Apache Airflow 3.1.1** - cÃ´ng cá»¥ orchestration hÃ ng Ä‘áº§u cho data pipelines. Sinh viÃªn sáº½ há»c cÃ¡ch thiáº¿t káº¿, láº­p lá»‹ch vÃ  giÃ¡m sÃ¡t cÃ¡c workflow phá»©c táº¡p trong thá»±c táº¿.

## ğŸ“š Lab Structure

### **Lab 1: Airflow Basics**
- **Focus**: Giá»›i thiá»‡u Airflow, cÃ i Ä‘áº·t vÃ  cáº¥u hÃ¬nh
- **Skills**: Hiá»ƒu kiáº¿n trÃºc Airflow, Web UI, CLI commands
- **Use Case**: Setup mÃ´i trÆ°á»ng vÃ  cháº¡y DAG Ä‘áº§u tiÃªn

### **Lab 2: DAGs vÃ  Tasks**
- **Focus**: Táº¡o DAGs vá»›i Task SDK (@dag, @task decorators)
- **Skills**: Äá»‹nh nghÄ©a workflows, tasks, dependencies
- **Use Case**: ETL pipeline Ä‘Æ¡n giáº£n vá»›i Python tasks

### **Lab 3: Operators vÃ  Hooks**
- **Focus**: Sá»­ dá»¥ng cÃ¡c operators phá»• biáº¿n (Bash, Python, SQL)
- **Skills**: BashOperator, PythonOperator, SQLExecuteQueryOperator
- **Use Case**: Data extraction vÃ  transformation vá»›i nhiá»u loáº¡i operators

### **Lab 4: Task Dependencies vÃ  Branching**
- **Focus**: Quáº£n lÃ½ dependencies, branching logic, trigger rules
- **Skills**: Bitshift operators (>>, <<), BranchPythonOperator
- **Use Case**: Conditional workflows vÃ  error handling

### **Lab 5: XCom vÃ  Data Sharing**
- **Focus**: Chia sáº» dá»¯ liá»‡u giá»¯a cÃ¡c tasks vá»›i XCom
- **Skills**: Task return values, XCom push/pull, custom XCom backends
- **Use Case**: Data pipeline vá»›i data passing giá»¯a tasks

### **Lab 6: Scheduling vÃ  Timetables**
- **Focus**: Láº­p lá»‹ch DAGs vá»›i cron, timedelta, custom timetables
- **Skills**: Schedule intervals, catchup, data intervals
- **Use Case**: Daily, hourly, vÃ  custom scheduling patterns

### **Lab 7: End-to-End Pipeline Integration**
- **Focus**: TÃ­ch há»£p Airflow vá»›i Kafka, Spark, Databases
- **Skills**: Multi-service orchestration, monitoring, error recovery
- **Use Case**: Complete data pipeline tá»« source â†’ processing â†’ destination

## ğŸš€ Quick Start

### 1. Setup Environment
```bash
# Install dependencies
./setup_airflow_lab.sh

# Set AIRFLOW_UID (Linux/Mac)
export AIRFLOW_UID=$(id -u)
echo "AIRFLOW_UID=$AIRFLOW_UID" > .env

# Or set manually (Windows)
echo "AIRFLOW_UID=50000" > .env
```

### 2. Initialize Airflow Database
```bash
# Initialize database and create admin user
docker compose up airflow-init

# Wait for initialization to complete
```

### 3. Start Airflow Services
```bash
# Start all services
docker compose up -d

# Check status
docker compose ps

# View logs
docker compose logs -f airflow-webserver
```

### 4. Access Airflow UI
- **URL**: http://localhost:8080
- **Username**: `airflow`
- **Password**: `airflow`

### 5. Start Jupyter Lab
```bash
# Activate conda environment
conda activate datalab

# Start Jupyter Lab
jupyter lab
```

### 6. Run Labs
Má»Ÿ notebooks theo thá»© tá»±:
1. `notebooks/01_airflow_basics.ipynb`
2. `notebooks/02_dags_and_tasks.ipynb`
3. `notebooks/03_operators_and_hooks.ipynb`
4. `notebooks/04_task_dependencies.ipynb`
5. `notebooks/05_xcom_data_sharing.ipynb`
6. `notebooks/06_scheduling_timetables.ipynb`
7. `notebooks/07_end_to_end_pipeline.ipynb`

## ğŸ—ï¸ Architecture

### **Services Included:**
- **Airflow Webserver (API Server)**: Port 8080 (Web UI vÃ  REST API)
- **Airflow Scheduler**: Láº­p lá»‹ch vÃ  trigger DAGs
- **Airflow DAG Processor**: Parse vÃ  load DAGs
- **Airflow Triggerer**: Xá»­ lÃ½ deferrable operators
- **PostgreSQL**: Port 5432 (Metadata database)

### **Components:**
- **DAGs**: Workflow definitions trong `dags/`
- **Plugins**: Custom operators/hooks trong `plugins/`
- **Logs**: Task execution logs trong `logs/`
- **Config**: Airflow configuration trong `config/`

### **Data Flow:**
```
Source Data â†’ Airflow DAG â†’ Task 1 â†’ Task 2 â†’ ... â†’ Destination
                â†“
            Scheduler monitors
                â†“
            Web UI displays
```

## ğŸ“Š Sample DAGs

### **Basic DAGs:**
- `hello_world_dag.py`: DAG Ä‘Æ¡n giáº£n nháº¥t
- `tutorial_dag.py`: DAG tutorial vá»›i nhiá»u tasks
- `etl_pipeline_dag.py`: ETL pipeline máº«u

### **Advanced DAGs:**
- `branching_dag.py`: Conditional branching
- `dynamic_dag.py`: Dynamic task generation
- `xcom_dag.py`: Data sharing vá»›i XCom
- `scheduled_dag.py`: Custom scheduling

### **Integration DAGs:**
- `kafka_spark_dag.py`: Kafka â†’ Spark â†’ Database
- `data_quality_dag.py`: Data validation pipeline
- `ml_pipeline_dag.py`: Machine learning pipeline

## ğŸ”§ Configuration

### **Environment Variables:**
```bash
# Airflow UID (set in .env)
AIRFLOW_UID=50000

# Admin user (optional)
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow
```

### **Airflow Configuration:**
- Executor: `LocalExecutor` (single machine)
- Database: PostgreSQL
- Load Examples: `false` (clean environment)
- DAGs Paused at Creation: `true`

## ğŸ› Troubleshooting

### **Common Issues:**

1. **Airflow won't start:**
   ```bash
   # Check logs
   docker compose logs airflow-webserver
   docker compose logs airflow-scheduler
   
   # Restart services
   docker compose restart
   ```

2. **Permission errors:**
   ```bash
   # Set correct AIRFLOW_UID
   export AIRFLOW_UID=$(id -u)
   echo "AIRFLOW_UID=$AIRFLOW_UID" > .env
   
   # Fix permissions
   sudo chown -R $AIRFLOW_UID:0 dags logs plugins config
   ```

3. **DAGs not appearing:**
   ```bash
   # Check DAG processor logs
   docker compose logs airflow-dag-processor
   
   # Test DAG parsing
   docker compose run airflow-cli airflow dags list
   ```

4. **Database connection errors:**
   ```bash
   # Check PostgreSQL health
   docker compose ps postgres
   
   # Restart database
   docker compose restart postgres
   ```

### **Performance Tuning:**
- Increase Docker memory allocation (minimum 4GB)
- Adjust scheduler settings in `config/airflow.cfg`
- Use appropriate executor for your use case

## ğŸ“š Learning Resources

- [Apache Airflow Documentation](https://airflow.apache.org/docs/apache-airflow/3.1.1/)
- [Airflow Task SDK](https://airflow.apache.org/docs/apache-airflow/3.1.1/task-sdk/index.html)
- [Airflow Concepts](https://airflow.apache.org/docs/apache-airflow/3.1.1/core-concepts/index.html)
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/3.1.1/best-practices/index.html)

## ğŸ¯ Learning Outcomes

Sau khi hoÃ n thÃ nh lab series nÃ y, sinh viÃªn sáº½ cÃ³ thá»ƒ:

1. **Airflow Fundamentals**:
   - Hiá»ƒu kiáº¿n trÃºc vÃ  components cá»§a Airflow
   - Sá»­ dá»¥ng Web UI vÃ  CLI Ä‘á»ƒ quáº£n lÃ½ DAGs
   - CÃ i Ä‘áº·t vÃ  cáº¥u hÃ¬nh Airflow environment

2. **DAG Development**:
   - Táº¡o DAGs vá»›i Task SDK (@dag, @task decorators)
   - Äá»‹nh nghÄ©a task dependencies vÃ  workflows
   - Implement branching vÃ  conditional logic

3. **Operators & Hooks**:
   - Sá»­ dá»¥ng cÃ¡c operators phá»• biáº¿n
   - Táº¡o custom operators vÃ  hooks
   - Integrate vá»›i external systems

4. **Data Management**:
   - Chia sáº» data giá»¯a tasks vá»›i XCom
   - Handle data passing trong pipelines
   - Implement data validation

5. **Scheduling**:
   - Cáº¥u hÃ¬nh scheduling vá»›i cron vÃ  timetables
   - Hiá»ƒu catchup vÃ  data intervals
   - Implement custom scheduling logic

6. **Pipeline Integration**:
   - TÃ­ch há»£p Airflow vá»›i Kafka, Spark, Databases
   - Build end-to-end data pipelines
   - Monitor vÃ  troubleshoot pipelines

## ğŸ“‹ Assessment Criteria

### **Beginner Level**:
- HoÃ n thÃ nh Lab 1 vÃ  2
- Hiá»ƒu basic concepts cá»§a Airflow
- Táº¡o Ä‘Æ°á»£c DAG Ä‘Æ¡n giáº£n

### **Intermediate Level**:
- HoÃ n thÃ nh Labs 1-5
- Hiá»ƒu operators, dependencies, XCom
- Implement Ä‘Æ°á»£c branching logic

### **Advanced Level**:
- HoÃ n thÃ nh táº¥t cáº£ labs
- Build Ä‘Æ°á»£c end-to-end pipeline
- TÃ­ch há»£p vá»›i multiple systems
- Optimize performance vÃ  reliability

## ğŸ”— Integration vá»›i Labs KhÃ¡c

Lab nÃ y tÃ­ch há»£p vá»›i:
- **Kafka Lab**: Stream data ingestion
- **Spark Lab**: Data processing
- **NoSQL Lab**: Database operations
- **PyIceberg Lab**: Data lake operations

## ğŸ‰ Next Steps

Sau khi hoÃ n thÃ nh Airflow Lab, báº¡n cÃ³ thá»ƒ:
1. TÃ­ch há»£p vá»›i dbt Lab (transformation)
2. ThÃªm Data Quality Lab (Great Expectations)
3. Implement CI/CD cho data pipelines
4. Deploy lÃªn production environment

---

**Happy Orchestrating! ğŸš€**

