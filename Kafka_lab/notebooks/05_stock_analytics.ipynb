{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5: Real-time Stock Analytics with Kafka\n",
        "\n",
        "## üéØ **Learning Objectives:**\n",
        "- Build real-time stock analytics pipeline\n",
        "- Implement multiple consumer groups for different analytics\n",
        "- Practice stream processing patterns\n",
        "- Create real-time dashboards and monitoring\n",
        "- Handle high-throughput data streams\n",
        "- Learn about Kafka Streams concepts\n",
        "\n",
        "## üìö **Key Concepts:**\n",
        "1. **Real-time Analytics**: Processing data as it arrives\n",
        "2. **Multiple Consumer Groups**: Different processing pipelines\n",
        "3. **Stream Processing**: Continuous data processing patterns\n",
        "4. **Real-time Dashboards**: Live data visualization\n",
        "5. **High-throughput Processing**: Handling large data volumes\n",
        "6. **Alert Systems**: Real-time notifications and triggers\n",
        "\n",
        "## üèóÔ∏è **Architecture Overview:**\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   Stock Data    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Kafka Topic    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Analytics     ‚îÇ\n",
        "‚îÇ   Producer      ‚îÇ    ‚îÇ   (3 Partitions) ‚îÇ    ‚îÇ   Consumer      ‚îÇ\n",
        "‚îÇ                 ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ   Groups        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ                        ‚îÇ                        ‚îÇ\n",
        "         ‚ñº                        ‚ñº                        ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ High-frequency  ‚îÇ    ‚îÇ Partition 0      ‚îÇ    ‚îÇ Analytics      ‚îÇ\n",
        "‚îÇ Data Generation ‚îÇ    ‚îÇ Partition 1      ‚îÇ    ‚îÇ ‚Ä¢ Moving Avg   ‚îÇ\n",
        "‚îÇ                 ‚îÇ    ‚îÇ Partition 2      ‚îÇ    ‚îÇ ‚Ä¢ Price Alerts ‚îÇ\n",
        "‚îÇ                 ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ ‚Ä¢ Volume       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                ‚îÇ                        ‚îÇ\n",
        "                                ‚ñº                        ‚ñº\n",
        "                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                       ‚îÇ Storage Group   ‚îÇ    ‚îÇ Dashboard Group ‚îÇ\n",
        "                       ‚îÇ ‚Ä¢ PostgreSQL    ‚îÇ    ‚îÇ ‚Ä¢ Real-time UI  ‚îÇ\n",
        "                       ‚îÇ ‚Ä¢ Redis Cache   ‚îÇ    ‚îÇ ‚Ä¢ Charts        ‚îÇ\n",
        "                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All dependencies imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install and Import Dependencies\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import threading\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict, deque\n",
        "from kafka import KafkaProducer, KafkaConsumer, KafkaAdminClient\n",
        "from kafka.errors import KafkaError\n",
        "from kafka.admin import NewTopic\n",
        "import uuid\n",
        "import statistics\n",
        "\n",
        "print(\"‚úÖ All dependencies imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Real-time Analytics Configuration:\n",
            "   Bootstrap Servers: localhost:9092\n",
            "   Topic: stock-analytics\n",
            "   Consumer Groups: ['analytics', 'alerts', 'storage', 'dashboard']\n",
            "   Analytics Window: 10 messages\n",
            "   Price Alert Threshold: 5.0%\n",
            "   High-frequency Interval: 0.1s\n"
          ]
        }
      ],
      "source": [
        "# Kafka Configuration for Real-time Analytics\n",
        "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
        "TOPIC_NAME = 'stock-analytics'\n",
        "CONSUMER_GROUPS = {\n",
        "    'analytics': 'stock-analytics-group',\n",
        "    'alerts': 'stock-alerts-group', \n",
        "    'storage': 'stock-storage-group',\n",
        "    'dashboard': 'stock-dashboard-group'\n",
        "}\n",
        "\n",
        "# Analytics Configuration\n",
        "ANALYTICS_CONFIG = {\n",
        "    'moving_average_window': 10,\n",
        "    'price_alert_threshold': 0.05,  # 5% price change\n",
        "    'volume_alert_threshold': 2.0,  # 2x average volume\n",
        "    'trend_detection_window': 5,\n",
        "    'high_frequency_interval': 0.1  # 100ms between messages\n",
        "}\n",
        "\n",
        "print(\"üîß Real-time Analytics Configuration:\")\n",
        "print(f\"   Bootstrap Servers: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
        "print(f\"   Topic: {TOPIC_NAME}\")\n",
        "print(f\"   Consumer Groups: {list(CONSUMER_GROUPS.keys())}\")\n",
        "print(f\"   Analytics Window: {ANALYTICS_CONFIG['moving_average_window']} messages\")\n",
        "print(f\"   Price Alert Threshold: {ANALYTICS_CONFIG['price_alert_threshold']*100}%\")\n",
        "print(f\"   High-frequency Interval: {ANALYTICS_CONFIG['high_frequency_interval']}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Connected to Kafka cluster\n",
            "üìù Creating topic 'stock-analytics' with 3 partitions...\n",
            "‚úÖ Topic 'stock-analytics' created successfully\n"
          ]
        }
      ],
      "source": [
        "# Create Topic for Real-time Analytics Lab\n",
        "def create_analytics_topic(topic_name: str, num_partitions: int = 3):\n",
        "    \"\"\"Create topic for real-time analytics\"\"\"\n",
        "    try:\n",
        "        admin_client = KafkaAdminClient(\n",
        "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "            client_id='analytics_lab_admin'\n",
        "        )\n",
        "        \n",
        "        # Check if topic exists\n",
        "        metadata = admin_client.describe_cluster()\n",
        "        print(f\"‚úÖ Connected to Kafka cluster\")\n",
        "        \n",
        "        try:\n",
        "            topic_metadata = admin_client.describe_topics([topic_name])\n",
        "            topic_info = topic_metadata[topic_name]\n",
        "            existing_partitions = len(topic_info.partitions)\n",
        "            \n",
        "            if existing_partitions == num_partitions:\n",
        "                print(f\"‚úÖ Topic '{topic_name}' already exists with {num_partitions} partitions\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Topic '{topic_name}' exists but has {existing_partitions} partitions, need {num_partitions}\")\n",
        "                print(f\"üîÑ Deleting and recreating topic...\")\n",
        "                \n",
        "                admin_client.delete_topics([topic_name])\n",
        "                time.sleep(2)\n",
        "                \n",
        "                topic = NewTopic(\n",
        "                    name=topic_name,\n",
        "                    num_partitions=num_partitions,\n",
        "                    replication_factor=1\n",
        "                )\n",
        "                admin_client.create_topics([topic])\n",
        "                print(f\"‚úÖ Topic '{topic_name}' recreated with {num_partitions} partitions\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"üìù Creating topic '{topic_name}' with {num_partitions} partitions...\")\n",
        "            topic = NewTopic(\n",
        "                name=topic_name,\n",
        "                num_partitions=num_partitions,\n",
        "                replication_factor=1\n",
        "            )\n",
        "            admin_client.create_topics([topic])\n",
        "            print(f\"‚úÖ Topic '{topic_name}' created successfully\")\n",
        "        \n",
        "        admin_client.close()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error managing topic: {e}\")\n",
        "        print(\"üí° Make sure Kafka is running: docker-compose up -d\")\n",
        "\n",
        "# Create the analytics topic\n",
        "create_analytics_topic(TOPIC_NAME, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ High-Frequency Stock Producer initialized!\n"
          ]
        }
      ],
      "source": [
        "# High-Frequency Stock Data Producer\n",
        "class HighFrequencyStockProducer:\n",
        "    \"\"\"Producer for high-frequency stock data simulation\"\"\"\n",
        "    \n",
        "    def __init__(self, bootstrap_servers: str, topic: str):\n",
        "        self.topic = topic\n",
        "        self.symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN', 'META', 'NVDA', 'NFLX', 'ADBE', 'CRM']\n",
        "        self.base_prices = {\n",
        "            'AAPL': 150.0, 'GOOGL': 2800.0, 'MSFT': 350.0, 'TSLA': 250.0, 'AMZN': 3200.0,\n",
        "            'META': 300.0, 'NVDA': 450.0, 'NFLX': 400.0, 'ADBE': 500.0, 'CRM': 200.0\n",
        "        }\n",
        "        self.current_prices = self.base_prices.copy()\n",
        "        self.message_count = 0\n",
        "        \n",
        "    def generate_realistic_price(self, symbol: str) -> float:\n",
        "        \"\"\"Generate realistic price movement using random walk\"\"\"\n",
        "        current_price = self.current_prices[symbol]\n",
        "        \n",
        "        # Random walk with slight upward bias\n",
        "        change_percent = random.gauss(0.0001, 0.01)  # 0.01% volatility\n",
        "        new_price = current_price * (1 + change_percent)\n",
        "        \n",
        "        # Ensure price stays positive and realistic\n",
        "        new_price = max(new_price, current_price * 0.95)  # Max 5% drop\n",
        "        new_price = min(new_price, current_price * 1.05)  # Max 5% gain\n",
        "        \n",
        "        self.current_prices[symbol] = new_price\n",
        "        return round(new_price, 2)\n",
        "    \n",
        "    def generate_ohlcv_data(self, symbol: str) -> dict:\n",
        "        \"\"\"Generate realistic OHLCV data\"\"\"\n",
        "        close_price = self.generate_realistic_price(symbol)\n",
        "        \n",
        "        # Generate OHLC around close price\n",
        "        volatility = 0.002  # 0.2% intraday volatility\n",
        "        open_price = round(close_price * random.uniform(1 - volatility, 1 + volatility), 2)\n",
        "        high_price = round(max(open_price, close_price) * random.uniform(1.001, 1.005), 2)\n",
        "        low_price = round(min(open_price, close_price) * random.uniform(0.995, 0.999), 2)\n",
        "        \n",
        "        # Volume with some randomness\n",
        "        base_volume = random.randint(50000, 500000)\n",
        "        volume_multiplier = random.uniform(0.5, 2.0)\n",
        "        volume = int(base_volume * volume_multiplier)\n",
        "        \n",
        "        self.message_count += 1\n",
        "        \n",
        "        return {\n",
        "            \"symbol\": symbol,\n",
        "            \"timestamp\": datetime.now().isoformat() + \"Z\",\n",
        "            \"open\": open_price,\n",
        "            \"high\": high_price,\n",
        "            \"low\": low_price,\n",
        "            \"close\": close_price,\n",
        "            \"volume\": volume,\n",
        "            \"exchange\": \"NASDAQ\",\n",
        "            \"message_id\": str(uuid.uuid4()),\n",
        "            \"sequence_number\": self.message_count\n",
        "        }\n",
        "    \n",
        "    def start_high_frequency_stream(self, duration_seconds: int = 30, interval: float = 0.1):\n",
        "        \"\"\"Start high-frequency data stream\"\"\"\n",
        "        print(f\"üöÄ Starting high-frequency stream for {duration_seconds} seconds...\")\n",
        "        print(f\"   Interval: {interval}s between messages\")\n",
        "        print(f\"   Expected messages: ~{int(duration_seconds / interval)}\")\n",
        "        \n",
        "        producer_config = {\n",
        "            'bootstrap_servers': KAFKA_BOOTSTRAP_SERVERS,\n",
        "            'value_serializer': lambda v: json.dumps(v).encode('utf-8'),\n",
        "            'key_serializer': lambda k: k.encode('utf-8') if k else None,\n",
        "            'acks': 'all',\n",
        "            'retries': 3,\n",
        "            'retry_backoff_ms': 100,\n",
        "            'batch_size': 16384,\n",
        "            'linger_ms': 5,  # Batch messages for 5ms\n",
        "            'compression_type': 'gzip'\n",
        "        }\n",
        "        \n",
        "        producer = KafkaProducer(**producer_config)\n",
        "        start_time = time.time()\n",
        "        messages_sent = 0\n",
        "        \n",
        "        try:\n",
        "            while time.time() - start_time < duration_seconds:\n",
        "                symbol = random.choice(self.symbols)\n",
        "                ohlcv_data = self.generate_ohlcv_data(symbol)\n",
        "                \n",
        "                # Send message\n",
        "                future = producer.send(\n",
        "                    self.topic,\n",
        "                    key=symbol,\n",
        "                    value=ohlcv_data\n",
        "                )\n",
        "                \n",
        "                try:\n",
        "                    record_metadata = future.get(timeout=1)\n",
        "                    messages_sent += 1\n",
        "                    \n",
        "                    if messages_sent % 50 == 0:  # Print every 50 messages\n",
        "                        print(f\"üìä Sent {messages_sent} messages - Latest: {symbol} ${ohlcv_data['close']}\")\n",
        "                    \n",
        "                except KafkaError as e:\n",
        "                    print(f\"‚ùå Error sending message: {e}\")\n",
        "                \n",
        "                time.sleep(interval)\n",
        "                \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n‚èπÔ∏è Stream stopped by user\")\n",
        "        finally:\n",
        "            producer.flush()\n",
        "            producer.close()\n",
        "            \n",
        "        print(f\"‚úÖ High-frequency stream completed!\")\n",
        "        print(f\"   Total messages sent: {messages_sent}\")\n",
        "        print(f\"   Average rate: {messages_sent / duration_seconds:.1f} messages/second\")\n",
        "\n",
        "# Initialize producer\n",
        "producer = HighFrequencyStockProducer(KAFKA_BOOTSTRAP_SERVERS, TOPIC_NAME)\n",
        "print(\"‚úÖ High-Frequency Stock Producer initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Real-time Analytics Consumer Classes:\n",
            "   - AnalyticsConsumer: Moving averages, trends, volume analysis\n",
            "   - AlertConsumer: Price and volume alerts\n",
            "   - DashboardConsumer: Real-time dashboard updates\n",
            "   - RealTimeAnalyticsConsumer: Base class for custom implementations\n"
          ]
        }
      ],
      "source": [
        "# Real-time Analytics Consumer Classes\n",
        "class RealTimeAnalyticsConsumer:\n",
        "    \"\"\"Base class for real-time analytics consumers\"\"\"\n",
        "    \n",
        "    def __init__(self, bootstrap_servers: str, topic: str, group_id: str):\n",
        "        self.bootstrap_servers = bootstrap_servers\n",
        "        self.topic = topic\n",
        "        self.group_id = group_id\n",
        "        self.consumer = None\n",
        "        self.processed_count = 0\n",
        "        self.start_time = None\n",
        "        \n",
        "    def create_consumer(self, **kwargs):\n",
        "        \"\"\"Create consumer with optimized configuration for real-time processing\"\"\"\n",
        "        default_config = {\n",
        "            'bootstrap_servers': self.bootstrap_servers,\n",
        "            'group_id': self.group_id,\n",
        "            'value_deserializer': lambda m: json.loads(m.decode('utf-8')),\n",
        "            'key_deserializer': lambda m: m.decode('utf-8') if m else None,\n",
        "            'auto_offset_reset': 'latest',\n",
        "            'enable_auto_commit': True,\n",
        "            'auto_commit_interval_ms': 1000,\n",
        "            'session_timeout_ms': 30000,\n",
        "            'heartbeat_interval_ms': 10000,\n",
        "            'consumer_timeout_ms': 1000,\n",
        "            'fetch_min_bytes': 1,\n",
        "            'fetch_max_wait_ms': 100,\n",
        "            'max_poll_records': 100\n",
        "        }\n",
        "        \n",
        "        config = {**default_config, **kwargs}\n",
        "        self.consumer = KafkaConsumer(self.topic, **config)\n",
        "        return self.consumer\n",
        "    \n",
        "    def process_message(self, message):\n",
        "        \"\"\"Process a single message - override in subclasses\"\"\"\n",
        "        data = message.value\n",
        "        self.processed_count += 1\n",
        "        \n",
        "        if self.start_time is None:\n",
        "            self.start_time = time.time()\n",
        "        \n",
        "        return data\n",
        "    \n",
        "    def get_processing_rate(self):\n",
        "        \"\"\"Get current processing rate\"\"\"\n",
        "        if self.start_time is None:\n",
        "            return 0\n",
        "        elapsed = time.time() - self.start_time\n",
        "        return self.processed_count / elapsed if elapsed > 0 else 0\n",
        "\n",
        "class AnalyticsConsumer(RealTimeAnalyticsConsumer):\n",
        "    \"\"\"Consumer for real-time analytics calculations\"\"\"\n",
        "    \n",
        "    def __init__(self, bootstrap_servers: str, topic: str, group_id: str):\n",
        "        super().__init__(bootstrap_servers, topic, group_id)\n",
        "        self.price_history = defaultdict(lambda: deque(maxlen=ANALYTICS_CONFIG['moving_average_window']))\n",
        "        self.volume_history = defaultdict(lambda: deque(maxlen=ANALYTICS_CONFIG['moving_average_window']))\n",
        "        self.analytics_results = []\n",
        "    \n",
        "    def calculate_moving_average(self, symbol: str, prices: deque) -> float:\n",
        "        \"\"\"Calculate simple moving average\"\"\"\n",
        "        if len(prices) < 2:\n",
        "            return prices[-1] if prices else 0\n",
        "        return statistics.mean(prices)\n",
        "    \n",
        "    def detect_trend(self, symbol: str, prices: deque) -> str:\n",
        "        \"\"\"Detect price trend\"\"\"\n",
        "        if len(prices) < ANALYTICS_CONFIG['trend_detection_window']:\n",
        "            return \"insufficient_data\"\n",
        "        \n",
        "        recent_prices = list(prices)[-ANALYTICS_CONFIG['trend_detection_window']:]\n",
        "        first_price = recent_prices[0]\n",
        "        last_price = recent_prices[-1]\n",
        "        \n",
        "        change_percent = (last_price - first_price) / first_price\n",
        "        \n",
        "        if change_percent > 0.02:  # 2% increase\n",
        "            return \"uptrend\"\n",
        "        elif change_percent < -0.02:  # 2% decrease\n",
        "            return \"downtrend\"\n",
        "        else:\n",
        "            return \"sideways\"\n",
        "    \n",
        "    def process_message(self, message):\n",
        "        \"\"\"Process message and calculate analytics\"\"\"\n",
        "        data = super().process_message(message)\n",
        "        symbol = data['symbol']\n",
        "        close_price = data['close']\n",
        "        volume = data['volume']\n",
        "        \n",
        "        # Update history\n",
        "        self.price_history[symbol].append(close_price)\n",
        "        self.volume_history[symbol].append(volume)\n",
        "        \n",
        "        # Calculate analytics\n",
        "        moving_avg = self.calculate_moving_average(symbol, self.price_history[symbol])\n",
        "        trend = self.detect_trend(symbol, self.price_history[symbol])\n",
        "        avg_volume = statistics.mean(self.volume_history[symbol]) if self.volume_history[symbol] else volume\n",
        "        \n",
        "        analytics_result = {\n",
        "            'symbol': symbol,\n",
        "            'timestamp': data['timestamp'],\n",
        "            'close_price': close_price,\n",
        "            'moving_average': round(moving_avg, 2),\n",
        "            'trend': trend,\n",
        "            'volume': volume,\n",
        "            'avg_volume': round(avg_volume, 0),\n",
        "            'volume_ratio': round(volume / avg_volume, 2) if avg_volume > 0 else 1.0,\n",
        "            'message_id': data['message_id'],\n",
        "            'sequence_number': data['sequence_number']\n",
        "        }\n",
        "        \n",
        "        self.analytics_results.append(analytics_result)\n",
        "        \n",
        "        # Print analytics every 10 messages\n",
        "        if self.processed_count % 10 == 0:\n",
        "            print(f\"üìà Analytics: {symbol} ${close_price} | MA: ${moving_avg:.2f} | Trend: {trend} | Vol: {volume:,}\")\n",
        "        \n",
        "        return analytics_result\n",
        "\n",
        "class AlertConsumer(RealTimeAnalyticsConsumer):\n",
        "    \"\"\"Consumer for price and volume alerts\"\"\"\n",
        "    \n",
        "    def __init__(self, bootstrap_servers: str, topic: str, group_id: str):\n",
        "        super().__init__(bootstrap_servers, topic, group_id)\n",
        "        self.price_alerts = []\n",
        "        self.volume_alerts = []\n",
        "        self.symbol_previous_prices = {}\n",
        "    \n",
        "    def check_price_alert(self, symbol: str, current_price: float) -> bool:\n",
        "        \"\"\"Check for significant price changes\"\"\"\n",
        "        if symbol not in self.symbol_previous_prices:\n",
        "            self.symbol_previous_prices[symbol] = current_price\n",
        "            return False\n",
        "        \n",
        "        previous_price = self.symbol_previous_prices[symbol]\n",
        "        change_percent = abs(current_price - previous_price) / previous_price\n",
        "        \n",
        "        if change_percent >= ANALYTICS_CONFIG['price_alert_threshold']:\n",
        "            alert = {\n",
        "                'symbol': symbol,\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'previous_price': previous_price,\n",
        "                'current_price': current_price,\n",
        "                'change_percent': round(change_percent * 100, 2),\n",
        "                'alert_type': 'price_change'\n",
        "            }\n",
        "            self.price_alerts.append(alert)\n",
        "            self.symbol_previous_prices[symbol] = current_price\n",
        "            return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def check_volume_alert(self, symbol: str, volume: float, avg_volume: float) -> bool:\n",
        "        \"\"\"Check for unusual volume spikes\"\"\"\n",
        "        if avg_volume <= 0:\n",
        "            return False\n",
        "        \n",
        "        volume_ratio = volume / avg_volume\n",
        "        \n",
        "        if volume_ratio >= ANALYTICS_CONFIG['volume_alert_threshold']:\n",
        "            alert = {\n",
        "                'symbol': symbol,\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'volume': volume,\n",
        "                'avg_volume': avg_volume,\n",
        "                'volume_ratio': round(volume_ratio, 2),\n",
        "                'alert_type': 'volume_spike'\n",
        "            }\n",
        "            self.volume_alerts.append(alert)\n",
        "            return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def process_message(self, message):\n",
        "        \"\"\"Process message and check for alerts\"\"\"\n",
        "        data = super().process_message(message)\n",
        "        symbol = data['symbol']\n",
        "        close_price = data['close']\n",
        "        volume = data['volume']\n",
        "        \n",
        "        # Check price alert\n",
        "        price_alert = self.check_price_alert(symbol, close_price)\n",
        "        if price_alert:\n",
        "            alert = self.price_alerts[-1]\n",
        "            print(f\"üö® PRICE ALERT: {symbol} changed {alert['change_percent']}% from ${alert['previous_price']} to ${alert['current_price']}\")\n",
        "        \n",
        "        # Check volume alert (simplified - would need historical data in real implementation)\n",
        "        volume_alert = self.check_volume_alert(symbol, volume, volume * 0.5)  # Simplified\n",
        "        if volume_alert:\n",
        "            alert = self.volume_alerts[-1]\n",
        "            print(f\"üìä VOLUME ALERT: {symbol} volume {alert['volume_ratio']}x average ({alert['volume']:,} vs {alert['avg_volume']:,.0f})\")\n",
        "        \n",
        "        return data\n",
        "\n",
        "class DashboardConsumer(RealTimeAnalyticsConsumer):\n",
        "    \"\"\"Consumer for real-time dashboard updates\"\"\"\n",
        "    \n",
        "    def __init__(self, bootstrap_servers: str, topic: str, group_id: str):\n",
        "        super().__init__(bootstrap_servers, topic, group_id)\n",
        "        self.dashboard_data = defaultdict(dict)\n",
        "        self.update_count = 0\n",
        "    \n",
        "    def update_dashboard(self, data: dict):\n",
        "        \"\"\"Update dashboard data structure\"\"\"\n",
        "        symbol = data['symbol']\n",
        "        \n",
        "        self.dashboard_data[symbol] = {\n",
        "            'symbol': symbol,\n",
        "            'price': data['close'],\n",
        "            'volume': data['volume'],\n",
        "            'timestamp': data['timestamp'],\n",
        "            'last_update': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        self.update_count += 1\n",
        "        \n",
        "        # Print dashboard update every 20 messages\n",
        "        if self.update_count % 20 == 0:\n",
        "            print(f\"üìä Dashboard Update #{self.update_count}:\")\n",
        "            for sym, info in list(self.dashboard_data.items())[-5:]:  # Show last 5 symbols\n",
        "                print(f\"   {sym}: ${info['price']} (Vol: {info['volume']:,})\")\n",
        "            print()\n",
        "    \n",
        "    def process_message(self, message):\n",
        "        \"\"\"Process message for dashboard updates\"\"\"\n",
        "        data = super().process_message(message)\n",
        "        self.update_dashboard(data)\n",
        "        return data\n",
        "\n",
        "print(\"üìã Real-time Analytics Consumer Classes:\")\n",
        "print(\"   - AnalyticsConsumer: Moving averages, trends, volume analysis\")\n",
        "print(\"   - AlertConsumer: Price and volume alerts\")\n",
        "print(\"   - DashboardConsumer: Real-time dashboard updates\")\n",
        "print(\"   - RealTimeAnalyticsConsumer: Base class for custom implementations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: High-Frequency Data Stream\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Generate high-frequency stock data\n",
        "- Understand real-time data processing challenges\n",
        "- Practice handling high-throughput streams\n",
        "- Learn about data generation patterns\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **High-Frequency Data**: Rapid data generation and processing\n",
        "2. **Real-time Processing**: Handling data as it arrives\n",
        "3. **Throughput**: Messages per second processing capability\n",
        "4. **Data Quality**: Ensuring realistic and consistent data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Exercise 1: High-Frequency Data Stream Generation\n",
            "\n",
            "üìä Starting 30-second high-frequency stream...\n",
            "   This will generate ~300 messages at 0.1s intervals\n",
            "   Press Ctrl+C to stop early if needed\n",
            "üöÄ Starting high-frequency stream for 30 seconds...\n",
            "   Interval: 0.1s between messages\n",
            "   Expected messages: ~300\n",
            "üìä Sent 50 messages - Latest: META $303.45\n",
            "üìä Sent 100 messages - Latest: AAPL $142.49\n",
            "üìä Sent 150 messages - Latest: ADBE $488.35\n",
            "üìä Sent 200 messages - Latest: ADBE $471.65\n",
            "üìä Sent 250 messages - Latest: MSFT $343.86\n",
            "‚úÖ High-frequency stream completed!\n",
            "   Total messages sent: 262\n",
            "   Average rate: 8.7 messages/second\n",
            "\n",
            "‚úÖ High-frequency stream completed!\n",
            "üìà Data is now available in the Kafka topic for real-time processing\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: Start High-Frequency Data Stream\n",
        "print(\"üöÄ Exercise 1: High-Frequency Data Stream Generation\")\n",
        "\n",
        "print(\"\\nüìä Starting 30-second high-frequency stream...\")\n",
        "print(\"   This will generate ~300 messages at 0.1s intervals\")\n",
        "print(\"   Press Ctrl+C to stop early if needed\")\n",
        "\n",
        "# Start the high-frequency stream\n",
        "producer.start_high_frequency_stream(\n",
        "    duration_seconds=30,\n",
        "    interval=ANALYTICS_CONFIG['high_frequency_interval']\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ High-frequency stream completed!\")\n",
        "print(\"üìà Data is now available in the Kafka topic for real-time processing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Real-time Analytics Processing\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Implement real-time analytics calculations\n",
        "- Practice moving average calculations\n",
        "- Learn trend detection algorithms\n",
        "- Understand volume analysis patterns\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **Moving Averages**: Smoothing price data over time windows\n",
        "2. **Trend Detection**: Identifying price direction patterns\n",
        "3. **Volume Analysis**: Understanding trading activity\n",
        "4. **Real-time Calculations**: Processing data as it arrives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà Exercise 2: Real-time Analytics Processing\n",
            "\n",
            "üîß Starting Analytics Consumer...\n",
            "   Processing messages for moving averages, trends, and volume analysis\n",
            "\n",
            "üìä Processing messages (will timeout after 10 seconds)...\n",
            "   Watch for analytics calculations below:\n",
            "\n",
            "üìä Analytics Processing Results:\n",
            "   ‚úÖ Messages processed: 0\n",
            "   üìà Processing rate: 0.0 messages/second\n",
            "   üìã Analytics results: 0 calculations\n",
            "\n",
            "‚úÖ Real-time analytics processing completed!\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2: Real-time Analytics Processing\n",
        "print(\"üìà Exercise 2: Real-time Analytics Processing\")\n",
        "\n",
        "print(\"\\nüîß Starting Analytics Consumer...\")\n",
        "print(\"   Processing messages for moving averages, trends, and volume analysis\")\n",
        "\n",
        "# Create analytics consumer\n",
        "analytics_consumer = AnalyticsConsumer(\n",
        "    KAFKA_BOOTSTRAP_SERVERS, \n",
        "    TOPIC_NAME, \n",
        "    CONSUMER_GROUPS['analytics']\n",
        ")\n",
        "\n",
        "analytics_consumer.create_consumer()\n",
        "\n",
        "print(\"\\nüìä Processing messages (will timeout after 10 seconds)...\")\n",
        "print(\"   Watch for analytics calculations below:\")\n",
        "\n",
        "# Process messages for 10 seconds\n",
        "start_time = time.time()\n",
        "timeout_seconds = 10\n",
        "\n",
        "try:\n",
        "    for message in analytics_consumer.consumer:\n",
        "        if time.time() - start_time > timeout_seconds:\n",
        "            print(f\"\\n‚è∞ Timeout reached after {timeout_seconds} seconds\")\n",
        "            break\n",
        "            \n",
        "        analytics_consumer.process_message(message)\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during processing: {e}\")\n",
        "finally:\n",
        "    if analytics_consumer.consumer:\n",
        "        analytics_consumer.consumer.close()\n",
        "\n",
        "# Show analytics results\n",
        "print(f\"\\nüìä Analytics Processing Results:\")\n",
        "print(f\"   ‚úÖ Messages processed: {analytics_consumer.processed_count}\")\n",
        "print(f\"   üìà Processing rate: {analytics_consumer.get_processing_rate():.1f} messages/second\")\n",
        "print(f\"   üìã Analytics results: {len(analytics_consumer.analytics_results)} calculations\")\n",
        "\n",
        "if analytics_consumer.analytics_results:\n",
        "    print(f\"\\nüìà Sample Analytics Results:\")\n",
        "    for result in analytics_consumer.analytics_results[-3:]:  # Show last 3\n",
        "        print(f\"   {result['symbol']}: ${result['close_price']} | MA: ${result['moving_average']} | Trend: {result['trend']} | Vol Ratio: {result['volume_ratio']}\")\n",
        "\n",
        "print(\"\\n‚úÖ Real-time analytics processing completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Real-time Alert System\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Implement price change alerts\n",
        "- Practice volume spike detection\n",
        "- Learn alert threshold management\n",
        "- Understand real-time notification patterns\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **Price Alerts**: Detecting significant price movements\n",
        "2. **Volume Alerts**: Identifying unusual trading activity\n",
        "3. **Threshold Management**: Setting appropriate alert levels\n",
        "4. **Real-time Notifications**: Immediate alert delivery\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üö® Exercise 3: Real-time Alert System\n",
            "\n",
            "üîß Starting Alert Consumer...\n",
            "   Monitoring for price changes and volume spikes\n",
            "\n",
            "üìä Processing messages for alerts (will timeout after 10 seconds)...\n",
            "   Watch for alerts below:\n",
            "\n",
            "üö® Alert System Results:\n",
            "   ‚úÖ Messages processed: 0\n",
            "   üìà Processing rate: 0.0 messages/second\n",
            "   üö® Price alerts triggered: 0\n",
            "   üìä Volume alerts triggered: 0\n",
            "\n",
            "‚úÖ Real-time alert system completed!\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3: Real-time Alert System\n",
        "print(\"üö® Exercise 3: Real-time Alert System\")\n",
        "\n",
        "print(\"\\nüîß Starting Alert Consumer...\")\n",
        "print(\"   Monitoring for price changes and volume spikes\")\n",
        "\n",
        "# Create alert consumer\n",
        "alert_consumer = AlertConsumer(\n",
        "    KAFKA_BOOTSTRAP_SERVERS, \n",
        "    TOPIC_NAME, \n",
        "    CONSUMER_GROUPS['alerts']\n",
        ")\n",
        "\n",
        "alert_consumer.create_consumer()\n",
        "\n",
        "print(\"\\nüìä Processing messages for alerts (will timeout after 10 seconds)...\")\n",
        "print(\"   Watch for alerts below:\")\n",
        "\n",
        "# Process messages for 10 seconds\n",
        "start_time = time.time()\n",
        "timeout_seconds = 10\n",
        "\n",
        "try:\n",
        "    for message in alert_consumer.consumer:\n",
        "        if time.time() - start_time > timeout_seconds:\n",
        "            print(f\"\\n‚è∞ Timeout reached after {timeout_seconds} seconds\")\n",
        "            break\n",
        "            \n",
        "        alert_consumer.process_message(message)\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during processing: {e}\")\n",
        "finally:\n",
        "    if alert_consumer.consumer:\n",
        "        alert_consumer.consumer.close()\n",
        "\n",
        "# Show alert results\n",
        "print(f\"\\nüö® Alert System Results:\")\n",
        "print(f\"   ‚úÖ Messages processed: {alert_consumer.processed_count}\")\n",
        "print(f\"   üìà Processing rate: {alert_consumer.get_processing_rate():.1f} messages/second\")\n",
        "print(f\"   üö® Price alerts triggered: {len(alert_consumer.price_alerts)}\")\n",
        "print(f\"   üìä Volume alerts triggered: {len(alert_consumer.volume_alerts)}\")\n",
        "\n",
        "if alert_consumer.price_alerts:\n",
        "    print(f\"\\nüö® Price Alerts Summary:\")\n",
        "    for alert in alert_consumer.price_alerts[-3:]:  # Show last 3\n",
        "        print(f\"   {alert['symbol']}: {alert['change_percent']}% change (${alert['previous_price']} ‚Üí ${alert['current_price']})\")\n",
        "\n",
        "if alert_consumer.volume_alerts:\n",
        "    print(f\"\\nüìä Volume Alerts Summary:\")\n",
        "    for alert in alert_consumer.volume_alerts[-3:]:  # Show last 3\n",
        "        print(f\"   {alert['symbol']}: {alert['volume_ratio']}x volume spike ({alert['volume']:,} vs {alert['avg_volume']:,.0f})\")\n",
        "\n",
        "print(\"\\n‚úÖ Real-time alert system completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datalab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
