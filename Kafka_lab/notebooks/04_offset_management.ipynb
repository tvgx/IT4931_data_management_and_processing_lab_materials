{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 4: Offset Management and Exactly-Once Processing\n",
        "\n",
        "## ğŸ¯ **Learning Objectives:**\n",
        "- Understand Kafka offset management mechanisms\n",
        "- Learn about different offset reset strategies\n",
        "- Practice manual offset commits for reliability\n",
        "- Explore exactly-once processing patterns\n",
        "- Handle consumer failures and recovery scenarios\n",
        "\n",
        "## ğŸ“š **Key Concepts:**\n",
        "1. **Offset Management**: Tracking message consumption progress\n",
        "2. **Auto vs Manual Commit**: Trade-offs between convenience and reliability\n",
        "3. **Exactly-Once Processing**: Ensuring no duplicate or lost messages\n",
        "4. **Consumer Failure Recovery**: Handling crashes and restarts\n",
        "5. **Offset Reset Strategies**: earliest, latest, none\n",
        "\n",
        "## ğŸ—ï¸ **Architecture Overview:**\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   Producer      â”‚â”€â”€â”€â–¶â”‚   Kafka Topic    â”‚â”€â”€â”€â–¶â”‚   Consumer      â”‚\n",
        "â”‚                 â”‚    â”‚   (3 Partitions) â”‚    â”‚   Group         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "         â”‚                        â”‚                        â”‚\n",
        "         â–¼                        â–¼                        â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Transactional   â”‚    â”‚ Partition 0      â”‚    â”‚ Offset Tracking â”‚\n",
        "â”‚ Producer        â”‚    â”‚ Partition 1      â”‚    â”‚ â€¢ Auto Commit   â”‚\n",
        "â”‚                 â”‚    â”‚ Partition 2      â”‚    â”‚ â€¢ Manual Commit â”‚\n",
        "â”‚                 â”‚    â”‚                  â”‚    â”‚ â€¢ Exactly-Once  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All dependencies imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install and Import Dependencies\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import threading\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from kafka import KafkaProducer, KafkaConsumer, KafkaAdminClient\n",
        "from kafka.errors import KafkaError, CommitFailedError\n",
        "from kafka.admin import ConfigResource, ConfigResourceType\n",
        "from kafka.structs import TopicPartition\n",
        "import uuid\n",
        "\n",
        "print(\"âœ… All dependencies imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Kafka Configuration:\n",
            "   Bootstrap Servers: localhost:9092\n",
            "   Topic: stock-data-offsets\n",
            "   Consumer Group: offset-management-group\n",
            "   Available Strategies: ['auto_commit', 'manual_commit', 'exactly_once', 'offset_reset']\n"
          ]
        }
      ],
      "source": [
        "# Kafka Configuration\n",
        "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
        "TOPIC_NAME = 'stock-data-offsets'\n",
        "CONSUMER_GROUP = 'offset-management-group'\n",
        "\n",
        "# Offset Management Strategies\n",
        "OFFSET_STRATEGIES = {\n",
        "    'auto_commit': 'Automatic offset commits (default)',\n",
        "    'manual_commit': 'Manual offset commits for reliability',\n",
        "    'exactly_once': 'Exactly-once processing with transactions',\n",
        "    'offset_reset': 'Different offset reset strategies'\n",
        "}\n",
        "\n",
        "print(\"ğŸ”§ Kafka Configuration:\")\n",
        "print(f\"   Bootstrap Servers: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
        "print(f\"   Topic: {TOPIC_NAME}\")\n",
        "print(f\"   Consumer Group: {CONSUMER_GROUP}\")\n",
        "print(f\"   Available Strategies: {list(OFFSET_STRATEGIES.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Connected to Kafka cluster\n",
            "ğŸ“ Creating topic 'stock-data-offsets' with 3 partitions...\n",
            "âœ… Topic 'stock-data-offsets' created successfully\n"
          ]
        }
      ],
      "source": [
        "# Create Topic for Offset Management Lab\n",
        "def create_topic_if_not_exists(topic_name: str, num_partitions: int = 3):\n",
        "    \"\"\"Create topic if it doesn't exist\"\"\"\n",
        "    try:\n",
        "        admin_client = KafkaAdminClient(\n",
        "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "            client_id='offset_lab_admin'\n",
        "        )\n",
        "        \n",
        "        # Check if topic exists\n",
        "        metadata = admin_client.describe_cluster()\n",
        "        print(f\"âœ… Connected to Kafka cluster\")\n",
        "        \n",
        "        # Try to get topic metadata\n",
        "        try:\n",
        "            topic_metadata = admin_client.describe_topics([topic_name])\n",
        "            topic_info = topic_metadata[topic_name]\n",
        "            existing_partitions = len(topic_info.partitions)\n",
        "            \n",
        "            if existing_partitions == num_partitions:\n",
        "                print(f\"âœ… Topic '{topic_name}' already exists with {num_partitions} partitions\")\n",
        "            else:\n",
        "                print(f\"âš ï¸ Topic '{topic_name}' exists but has {existing_partitions} partitions, need {num_partitions}\")\n",
        "                print(f\"ğŸ”„ Deleting and recreating topic...\")\n",
        "                \n",
        "                # Delete existing topic\n",
        "                admin_client.delete_topics([topic_name])\n",
        "                time.sleep(2)  # Wait for deletion\n",
        "                \n",
        "                # Create new topic with correct partitions\n",
        "                from kafka.admin import NewTopic\n",
        "                topic = NewTopic(\n",
        "                    name=topic_name,\n",
        "                    num_partitions=num_partitions,\n",
        "                    replication_factor=1\n",
        "                )\n",
        "                admin_client.create_topics([topic])\n",
        "                print(f\"âœ… Topic '{topic_name}' recreated with {num_partitions} partitions\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"ğŸ“ Creating topic '{topic_name}' with {num_partitions} partitions...\")\n",
        "            from kafka.admin import NewTopic\n",
        "            topic = NewTopic(\n",
        "                name=topic_name,\n",
        "                num_partitions=num_partitions,\n",
        "                replication_factor=1\n",
        "            )\n",
        "            admin_client.create_topics([topic])\n",
        "            print(f\"âœ… Topic '{topic_name}' created successfully\")\n",
        "        \n",
        "        admin_client.close()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error managing topic: {e}\")\n",
        "        print(\"ğŸ’¡ Make sure Kafka is running: docker-compose up -d\")\n",
        "\n",
        "# Create the topic\n",
        "create_topic_if_not_exists(TOPIC_NAME, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Stock Data Producer initialized!\n"
          ]
        }
      ],
      "source": [
        "# Stock Data Producer for Offset Management\n",
        "class StockDataProducer:\n",
        "    def __init__(self, bootstrap_servers: str, topic: str):\n",
        "        self.topic = topic\n",
        "        self.symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN', 'META', 'NVDA', 'NFLX', 'ADBE', 'CRM']\n",
        "        self.base_prices = {\n",
        "            'AAPL': 150.0, 'GOOGL': 2800.0, 'MSFT': 350.0, 'TSLA': 250.0, 'AMZN': 3200.0,\n",
        "            'META': 300.0, 'NVDA': 450.0, 'NFLX': 400.0, 'ADBE': 500.0, 'CRM': 200.0\n",
        "        }\n",
        "    \n",
        "    def generate_ohlcv_data(self, symbol: str, base_price: float) -> dict:\n",
        "        \"\"\"Generate realistic OHLCV data\"\"\"\n",
        "        price_change = random.uniform(-0.02, 0.02)\n",
        "        new_price = base_price * (1 + price_change)\n",
        "        \n",
        "        open_price = round(new_price * random.uniform(0.998, 1.002), 2)\n",
        "        close_price = round(new_price * random.uniform(0.998, 1.002), 2)\n",
        "        high_price = round(max(open_price, close_price) * random.uniform(1.001, 1.005), 2)\n",
        "        low_price = round(min(open_price, close_price) * random.uniform(0.995, 0.999), 2)\n",
        "        \n",
        "        volume = random.randint(100000, 1000000)\n",
        "        \n",
        "        return {\n",
        "            \"symbol\": symbol,\n",
        "            \"timestamp\": datetime.now().isoformat() + \"Z\",\n",
        "            \"open\": open_price,\n",
        "            \"high\": high_price,\n",
        "            \"low\": low_price,\n",
        "            \"close\": close_price,\n",
        "            \"volume\": volume,\n",
        "            \"exchange\": \"NASDAQ\",\n",
        "            \"message_id\": str(uuid.uuid4())  # Unique message ID for tracking\n",
        "        }\n",
        "    \n",
        "    def send_test_data(self, num_messages: int = 50):\n",
        "        \"\"\"Send test data for offset management experiments\"\"\"\n",
        "        print(f\"ğŸ“ˆ Generating {num_messages} test messages...\")\n",
        "        \n",
        "        producer_config = {\n",
        "            'bootstrap_servers': KAFKA_BOOTSTRAP_SERVERS,\n",
        "            'value_serializer': lambda v: json.dumps(v).encode('utf-8'),\n",
        "            'key_serializer': lambda k: k.encode('utf-8') if k else None,\n",
        "            'acks': 'all',  # Wait for all replicas\n",
        "            'retries': 3,\n",
        "            'retry_backoff_ms': 100\n",
        "        }\n",
        "        \n",
        "        producer = KafkaProducer(**producer_config)\n",
        "        \n",
        "        for i in range(num_messages):\n",
        "            symbol = random.choice(self.symbols)\n",
        "            base_price = self.base_prices[symbol]\n",
        "            ohlcv_data = self.generate_ohlcv_data(symbol, base_price)\n",
        "            \n",
        "            # Send message\n",
        "            future = producer.send(\n",
        "                self.topic, \n",
        "                key=symbol, \n",
        "                value=ohlcv_data\n",
        "            )\n",
        "            \n",
        "            try:\n",
        "                record_metadata = future.get(timeout=10)\n",
        "                print(f\"ğŸ“Š Sent {symbol}: ${ohlcv_data['close']} -> Partition {record_metadata.partition}, Offset {record_metadata.offset}\")\n",
        "            except KafkaError as e:\n",
        "                print(f\"âŒ Error sending message: {e}\")\n",
        "            \n",
        "            time.sleep(0.05)  # Small delay between messages\n",
        "        \n",
        "        producer.flush()\n",
        "        producer.close()\n",
        "        \n",
        "        print(f\"âœ… Successfully sent {num_messages} test messages\")\n",
        "\n",
        "# Initialize producer\n",
        "producer = StockDataProducer(KAFKA_BOOTSTRAP_SERVERS, TOPIC_NAME)\n",
        "print(\"âœ… Stock Data Producer initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‹ Available consumer types:\n",
            "   - AutoCommitConsumer: Automatic offset commits\n",
            "   - ManualCommitConsumer: Manual offset commits\n",
            "   - OffsetManagementConsumer: Base class for custom implementations\n"
          ]
        }
      ],
      "source": [
        "# Offset Management Consumer Classes\n",
        "class OffsetManagementConsumer:\n",
        "    \"\"\"Base class for offset management consumers\"\"\"\n",
        "    \n",
        "    def __init__(self, bootstrap_servers: str, topic: str, group_id: str):\n",
        "        self.bootstrap_servers = bootstrap_servers\n",
        "        self.topic = topic\n",
        "        self.group_id = group_id\n",
        "        self.consumer = None\n",
        "        self.processed_messages = []\n",
        "        self.failed_messages = []\n",
        "    \n",
        "    def create_consumer(self, **kwargs):\n",
        "        \"\"\"Create consumer with specified configuration\"\"\"\n",
        "        default_config = {\n",
        "            'bootstrap_servers': self.bootstrap_servers,\n",
        "            'group_id': self.group_id,\n",
        "            'value_deserializer': lambda m: json.loads(m.decode('utf-8')),\n",
        "            'key_deserializer': lambda m: m.decode('utf-8') if m else None,\n",
        "            'auto_offset_reset': 'earliest',\n",
        "            'enable_auto_commit': True,\n",
        "            'auto_commit_interval_ms': 1000,\n",
        "            'session_timeout_ms': 30000,\n",
        "            'heartbeat_interval_ms': 10000,\n",
        "            'consumer_timeout_ms': 1000  # Add consumer timeout\n",
        "        }\n",
        "        \n",
        "        # Merge with provided config\n",
        "        config = {**default_config, **kwargs}\n",
        "        self.consumer = KafkaConsumer(self.topic, **config)\n",
        "        return self.consumer\n",
        "    \n",
        "    def process_message(self, message):\n",
        "        \"\"\"Process a single message - override in subclasses\"\"\"\n",
        "        data = message.value\n",
        "        print(f\"ğŸ“Š Processing {data['symbol']}: ${data['close']} (Message ID: {data['message_id']})\")\n",
        "        \n",
        "        # Simulate processing time\n",
        "        time.sleep(0.1)\n",
        "        \n",
        "        # Simulate occasional failures (10% chance)\n",
        "        if random.random() < 0.1:\n",
        "            raise Exception(f\"Simulated processing failure for message {data['message_id']}\")\n",
        "        \n",
        "        return data\n",
        "    \n",
        "    def consume_messages(self, max_messages: int = 10, timeout_ms: int = 5000):\n",
        "        \"\"\"Consume messages with offset management\"\"\"\n",
        "        if not self.consumer:\n",
        "            raise Exception(\"Consumer not created. Call create_consumer() first.\")\n",
        "        \n",
        "        print(f\"ğŸ”„ Starting to consume up to {max_messages} messages...\")\n",
        "        \n",
        "        message_count = 0\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            for message in self.consumer:\n",
        "                if message_count >= max_messages:\n",
        "                    break\n",
        "                \n",
        "                try:\n",
        "                    processed_data = self.process_message(message)\n",
        "                    self.processed_messages.append({\n",
        "                        'message_id': processed_data['message_id'],\n",
        "                        'symbol': processed_data['symbol'],\n",
        "                        'partition': message.partition,\n",
        "                        'offset': message.offset,\n",
        "                        'timestamp': processed_data['timestamp']\n",
        "                    })\n",
        "                    print(f\"âœ… Processed message {message_count + 1}/{max_messages}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"âŒ Failed to process message: {e}\")\n",
        "                    self.failed_messages.append({\n",
        "                        'message_id': message.value.get('message_id', 'unknown'),\n",
        "                        'error': str(e),\n",
        "                        'partition': message.partition,\n",
        "                        'offset': message.offset\n",
        "                    })\n",
        "                \n",
        "                message_count += 1\n",
        "                \n",
        "                # Check timeout\n",
        "                if (time.time() - start_time) * 1000 > timeout_ms:\n",
        "                    print(f\"â° Timeout reached after {timeout_ms}ms\")\n",
        "                    break\n",
        "                    \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Consumer error: {e}\")\n",
        "        finally:\n",
        "            if self.consumer:\n",
        "                self.consumer.close()\n",
        "        \n",
        "        print(f\"ğŸ“Š Consumption completed: {len(self.processed_messages)} processed, {len(self.failed_messages)} failed\")\n",
        "        return self.processed_messages, self.failed_messages\n",
        "\n",
        "class AutoCommitConsumer(OffsetManagementConsumer):\n",
        "    \"\"\"Consumer with automatic offset commits\"\"\"\n",
        "    \n",
        "    def create_consumer(self, **kwargs):\n",
        "        config = {\n",
        "            'enable_auto_commit': True,\n",
        "            'auto_commit_interval_ms': 1000,\n",
        "            **kwargs\n",
        "        }\n",
        "        return super().create_consumer(**config)\n",
        "\n",
        "class ManualCommitConsumer(OffsetManagementConsumer):\n",
        "    \"\"\"Consumer with manual offset commits\"\"\"\n",
        "    \n",
        "    def create_consumer(self, **kwargs):\n",
        "        config = {\n",
        "            'enable_auto_commit': False,\n",
        "            **kwargs\n",
        "        }\n",
        "        return super().create_consumer(**config)\n",
        "    \n",
        "    def consume_messages(self, max_messages: int = 10, commit_every: int = 5, timeout_ms: int = 5000):\n",
        "        \"\"\"Consume messages with manual commits\"\"\"\n",
        "        if not self.consumer:\n",
        "            raise Exception(\"Consumer not created. Call create_consumer() first.\")\n",
        "        \n",
        "        print(f\"ğŸ”„ Starting manual commit consumer (commit every {commit_every} messages)...\")\n",
        "        \n",
        "        message_count = 0\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            for message in self.consumer:\n",
        "                if message_count >= max_messages:\n",
        "                    break\n",
        "                \n",
        "                try:\n",
        "                    processed_data = self.process_message(message)\n",
        "                    self.processed_messages.append({\n",
        "                        'message_id': processed_data['message_id'],\n",
        "                        'symbol': processed_data['symbol'],\n",
        "                        'partition': message.partition,\n",
        "                        'offset': message.offset,\n",
        "                        'timestamp': processed_data['timestamp']\n",
        "                    })\n",
        "                    print(f\"âœ… Processed message {message_count + 1}/{max_messages}\")\n",
        "                    \n",
        "                    # Manual commit every N messages\n",
        "                    if (message_count + 1) % commit_every == 0:\n",
        "                        try:\n",
        "                            self.consumer.commit()\n",
        "                            print(f\"ğŸ’¾ Committed offsets after {message_count + 1} messages\")\n",
        "                        except CommitFailedError as e:\n",
        "                            print(f\"âŒ Commit failed: {e}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"âŒ Failed to process message: {e}\")\n",
        "                    self.failed_messages.append({\n",
        "                        'message_id': message.value.get('message_id', 'unknown'),\n",
        "                        'error': str(e),\n",
        "                        'partition': message.partition,\n",
        "                        'offset': message.offset\n",
        "                    })\n",
        "                \n",
        "                message_count += 1\n",
        "                \n",
        "                # Check timeout\n",
        "                if (time.time() - start_time) * 1000 > timeout_ms:\n",
        "                    print(f\"â° Timeout reached after {timeout_ms}ms\")\n",
        "                    break\n",
        "            \n",
        "            # Final commit\n",
        "            try:\n",
        "                self.consumer.commit()\n",
        "                print(f\"ğŸ’¾ Final commit completed\")\n",
        "            except CommitFailedError as e:\n",
        "                print(f\"âŒ Final commit failed: {e}\")\n",
        "                    \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Consumer error: {e}\")\n",
        "        finally:\n",
        "            if self.consumer:\n",
        "                self.consumer.close()\n",
        "        \n",
        "        print(f\"ğŸ“Š Manual commit consumption completed: {len(self.processed_messages)} processed, {len(self.failed_messages)} failed\")\n",
        "        return self.processed_messages, self.failed_messages\n",
        "\n",
        "print(\"ğŸ“‹ Available consumer types:\")\n",
        "print(\"   - AutoCommitConsumer: Automatic offset commits\")\n",
        "print(\"   - ManualCommitConsumer: Manual offset commits\")\n",
        "print(\"   - OffsetManagementConsumer: Base class for custom implementations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Auto Commit vs Manual Commit\n",
        "\n",
        "### ğŸ¯ **Learning Objectives:**\n",
        "- Understand automatic offset commits\n",
        "- Learn manual offset commit patterns\n",
        "- Compare reliability between approaches\n",
        "- Observe offset behavior during failures\n",
        "\n",
        "### ğŸ“š **Key Concepts:**\n",
        "1. **Auto Commit**: Convenient but less reliable\n",
        "2. **Manual Commit**: More control and reliability\n",
        "3. **Commit Timing**: When to commit offsets\n",
        "4. **Failure Recovery**: Handling processing failures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ Exercise 1: Comparing Auto Commit vs Manual Commit\n",
            "\n",
            "ğŸ“ˆ Step 1: Sending test data...\n",
            "ğŸ“ˆ Generating 20 test messages...\n",
            "ğŸ“Š Sent MSFT: $356.81 -> Partition 0, Offset 0\n",
            "ğŸ“Š Sent TSLA: $247.4 -> Partition 2, Offset 0\n",
            "ğŸ“Š Sent MSFT: $356.34 -> Partition 0, Offset 1\n",
            "ğŸ“Š Sent NFLX: $403.32 -> Partition 1, Offset 0\n",
            "ğŸ“Š Sent AMZN: $3255.12 -> Partition 2, Offset 1\n",
            "ğŸ“Š Sent NFLX: $395.45 -> Partition 1, Offset 1\n",
            "ğŸ“Š Sent NVDA: $448.39 -> Partition 2, Offset 2\n",
            "ğŸ“Š Sent AAPL: $147.64 -> Partition 0, Offset 2\n",
            "ğŸ“Š Sent NFLX: $401.05 -> Partition 1, Offset 2\n",
            "ğŸ“Š Sent GOOGL: $2769.15 -> Partition 1, Offset 3\n",
            "ğŸ“Š Sent CRM: $197.76 -> Partition 1, Offset 4\n",
            "ğŸ“Š Sent NFLX: $406.86 -> Partition 1, Offset 5\n",
            "ğŸ“Š Sent TSLA: $246.87 -> Partition 2, Offset 3\n",
            "ğŸ“Š Sent AMZN: $3141.19 -> Partition 2, Offset 4\n",
            "ğŸ“Š Sent GOOGL: $2790.82 -> Partition 1, Offset 6\n",
            "ğŸ“Š Sent NVDA: $455.58 -> Partition 2, Offset 5\n",
            "ğŸ“Š Sent MSFT: $344.87 -> Partition 0, Offset 3\n",
            "ğŸ“Š Sent AMZN: $3147.97 -> Partition 2, Offset 6\n",
            "ğŸ“Š Sent NFLX: $393.7 -> Partition 1, Offset 7\n",
            "ğŸ“Š Sent MSFT: $343.34 -> Partition 0, Offset 4\n",
            "âœ… Successfully sent 20 test messages\n",
            "\n",
            "ğŸ¤– Step 2: Testing Auto Commit Consumer...\n",
            "ğŸ”„ Starting to consume up to 10 messages...\n",
            "ğŸ“Š Processing MSFT: $356.81 (Message ID: d6e9cb87-9521-40ee-abff-6707b372bde0)\n",
            "âœ… Processed message 1/10\n",
            "ğŸ“Š Processing MSFT: $356.34 (Message ID: 264eaede-4c15-459f-8426-10b025cbdef7)\n",
            "âœ… Processed message 2/10\n",
            "ğŸ“Š Processing AAPL: $147.64 (Message ID: 6e6bba55-8cfb-4eb1-8da8-80a8ba07e28b)\n",
            "âœ… Processed message 3/10\n",
            "ğŸ“Š Processing MSFT: $344.87 (Message ID: 9f47f2e5-0e61-44a3-94d2-854fe0621792)\n",
            "âœ… Processed message 4/10\n",
            "ğŸ“Š Processing MSFT: $343.34 (Message ID: a3a34feb-e395-4b47-8f96-9ddb0e394830)\n",
            "âœ… Processed message 5/10\n",
            "ğŸ“Š Processing NFLX: $403.32 (Message ID: 205dc5ca-9d82-4069-adf1-e9bb612e269b)\n",
            "âœ… Processed message 6/10\n",
            "ğŸ“Š Processing NFLX: $395.45 (Message ID: 36ef50b5-3963-4f5e-bc38-d0e01cb94a6a)\n",
            "âœ… Processed message 7/10\n",
            "ğŸ“Š Processing NFLX: $401.05 (Message ID: aa3a7ff3-6471-4b43-a024-4cb7740e1ac0)\n",
            "âœ… Processed message 8/10\n",
            "ğŸ“Š Processing GOOGL: $2769.15 (Message ID: d1f9ac65-05c2-4638-9d82-bc628412c39b)\n",
            "âŒ Failed to process message: Simulated processing failure for message d1f9ac65-05c2-4638-9d82-bc628412c39b\n",
            "ğŸ“Š Processing CRM: $197.76 (Message ID: c28ccd3e-56e6-4490-baac-0d8b7c661356)\n",
            "âœ… Processed message 10/10\n",
            "ğŸ“Š Consumption completed: 9 processed, 1 failed\n",
            "\n",
            "ğŸ“Š Auto Commit Results:\n",
            "   âœ… Processed: 9 messages\n",
            "   âŒ Failed: 1 messages\n",
            "\n",
            "ğŸ¤– Step 3: Testing Manual Commit Consumer...\n",
            "ğŸ”„ Starting manual commit consumer (commit every 3 messages)...\n",
            "ğŸ“Š Processing MSFT: $356.81 (Message ID: d6e9cb87-9521-40ee-abff-6707b372bde0)\n",
            "âœ… Processed message 1/10\n",
            "ğŸ“Š Processing MSFT: $356.34 (Message ID: 264eaede-4c15-459f-8426-10b025cbdef7)\n",
            "âœ… Processed message 2/10\n",
            "ğŸ“Š Processing AAPL: $147.64 (Message ID: 6e6bba55-8cfb-4eb1-8da8-80a8ba07e28b)\n",
            "âœ… Processed message 3/10\n",
            "ğŸ’¾ Committed offsets after 3 messages\n",
            "ğŸ“Š Processing MSFT: $344.87 (Message ID: 9f47f2e5-0e61-44a3-94d2-854fe0621792)\n",
            "âœ… Processed message 4/10\n",
            "ğŸ“Š Processing MSFT: $343.34 (Message ID: a3a34feb-e395-4b47-8f96-9ddb0e394830)\n",
            "âœ… Processed message 5/10\n",
            "ğŸ“Š Processing NFLX: $403.32 (Message ID: 205dc5ca-9d82-4069-adf1-e9bb612e269b)\n",
            "âœ… Processed message 6/10\n",
            "ğŸ’¾ Committed offsets after 6 messages\n",
            "ğŸ“Š Processing NFLX: $395.45 (Message ID: 36ef50b5-3963-4f5e-bc38-d0e01cb94a6a)\n",
            "âœ… Processed message 7/10\n",
            "ğŸ“Š Processing NFLX: $401.05 (Message ID: aa3a7ff3-6471-4b43-a024-4cb7740e1ac0)\n",
            "âœ… Processed message 8/10\n",
            "ğŸ“Š Processing GOOGL: $2769.15 (Message ID: d1f9ac65-05c2-4638-9d82-bc628412c39b)\n",
            "âœ… Processed message 9/10\n",
            "ğŸ’¾ Committed offsets after 9 messages\n",
            "ğŸ“Š Processing CRM: $197.76 (Message ID: c28ccd3e-56e6-4490-baac-0d8b7c661356)\n",
            "âœ… Processed message 10/10\n",
            "ğŸ’¾ Final commit completed\n",
            "ğŸ“Š Manual commit consumption completed: 10 processed, 0 failed\n",
            "\n",
            "ğŸ“Š Manual Commit Results:\n",
            "   âœ… Processed: 10 messages\n",
            "   âŒ Failed: 0 messages\n",
            "\n",
            "ğŸ“‹ Comparison Analysis:\n",
            "âœ… Auto Commit: Convenient, commits automatically every 1 second\n",
            "âœ… Manual Commit: More control, commits after processing batches\n",
            "âš ï¸ Auto Commit: May lose messages if consumer crashes before commit\n",
            "âš ï¸ Manual Commit: Requires careful error handling to avoid duplicates\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: Auto Commit vs Manual Commit Demo\n",
        "print(\"ğŸ”„ Exercise 1: Comparing Auto Commit vs Manual Commit\")\n",
        "\n",
        "# First, send some test data\n",
        "print(\"\\nğŸ“ˆ Step 1: Sending test data...\")\n",
        "producer.send_test_data(num_messages=20)\n",
        "\n",
        "print(\"\\nğŸ¤– Step 2: Testing Auto Commit Consumer...\")\n",
        "# Test Auto Commit Consumer\n",
        "auto_consumer = AutoCommitConsumer(KAFKA_BOOTSTRAP_SERVERS, TOPIC_NAME, f\"{CONSUMER_GROUP}-auto\")\n",
        "auto_consumer.create_consumer()\n",
        "auto_processed, auto_failed = auto_consumer.consume_messages(max_messages=10)\n",
        "\n",
        "print(f\"\\nğŸ“Š Auto Commit Results:\")\n",
        "print(f\"   âœ… Processed: {len(auto_processed)} messages\")\n",
        "print(f\"   âŒ Failed: {len(auto_failed)} messages\")\n",
        "\n",
        "print(\"\\nğŸ¤– Step 3: Testing Manual Commit Consumer...\")\n",
        "# Test Manual Commit Consumer\n",
        "manual_consumer = ManualCommitConsumer(KAFKA_BOOTSTRAP_SERVERS, TOPIC_NAME, f\"{CONSUMER_GROUP}-manual\")\n",
        "manual_consumer.create_consumer()\n",
        "manual_processed, manual_failed = manual_consumer.consume_messages(max_messages=10, commit_every=3)\n",
        "\n",
        "print(f\"\\nğŸ“Š Manual Commit Results:\")\n",
        "print(f\"   âœ… Processed: {len(manual_processed)} messages\")\n",
        "print(f\"   âŒ Failed: {len(manual_failed)} messages\")\n",
        "\n",
        "print(\"\\nğŸ“‹ Comparison Analysis:\")\n",
        "print(\"âœ… Auto Commit: Convenient, commits automatically every 1 second\")\n",
        "print(\"âœ… Manual Commit: More control, commits after processing batches\")\n",
        "print(\"âš ï¸ Auto Commit: May lose messages if consumer crashes before commit\")\n",
        "print(\"âš ï¸ Manual Commit: Requires careful error handling to avoid duplicates\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Offset Reset Strategies\n",
        "\n",
        "### ğŸ¯ **Learning Objectives:**\n",
        "- Understand different offset reset strategies\n",
        "- Learn when to use each strategy\n",
        "- Practice offset management for new consumer groups\n",
        "- Handle scenarios with no committed offsets\n",
        "\n",
        "### ğŸ“š **Key Concepts:**\n",
        "1. **earliest**: Start from the beginning of the topic\n",
        "2. **latest**: Start from the end (newest messages)\n",
        "3. **none**: Fail if no committed offset exists\n",
        "4. **Consumer Group**: Offset management per group\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ Exercise 2: Testing Different Offset Reset Strategies\n",
            "\n",
            "ğŸ“ˆ Step 1: Sending additional test data...\n",
            "ğŸ“ˆ Generating 15 test messages...\n",
            "ğŸ“Š Sent NVDA: $446.28 -> Partition 2, Offset 10\n",
            "ğŸ“Š Sent AAPL: $150.68 -> Partition 0, Offset 12\n",
            "ğŸ“Š Sent ADBE: $493.13 -> Partition 2, Offset 11\n",
            "ğŸ“Š Sent GOOGL: $2761.89 -> Partition 1, Offset 13\n",
            "ğŸ“Š Sent AMZN: $3152.21 -> Partition 2, Offset 12\n",
            "ğŸ“Š Sent TSLA: $245.75 -> Partition 2, Offset 13\n",
            "ğŸ“Š Sent AMZN: $3216.9 -> Partition 2, Offset 14\n",
            "ğŸ“Š Sent GOOGL: $2755.93 -> Partition 1, Offset 14\n",
            "ğŸ“Š Sent META: $297.37 -> Partition 0, Offset 13\n",
            "ğŸ“Š Sent MSFT: $356.03 -> Partition 0, Offset 14\n",
            "ğŸ“Š Sent TSLA: $252.74 -> Partition 2, Offset 15\n",
            "ğŸ“Š Sent GOOGL: $2771.82 -> Partition 1, Offset 15\n",
            "ğŸ“Š Sent NVDA: $446.58 -> Partition 2, Offset 16\n",
            "ğŸ“Š Sent AMZN: $3237.48 -> Partition 2, Offset 17\n",
            "ğŸ“Š Sent CRM: $199.77 -> Partition 1, Offset 16\n",
            "âœ… Successfully sent 15 test messages\n",
            "\n",
            "ğŸ” Step 2: Testing 'earliest' offset reset...\n",
            "ğŸ”„ Starting to consume up to 5 messages...\n",
            "ğŸ“Š Processing MSFT: $353.76 (Message ID: 5d7373bd-c285-4105-874a-28cb92838a20)\n",
            "âŒ Failed to process message: Simulated processing failure for message 5d7373bd-c285-4105-874a-28cb92838a20\n",
            "ğŸ“Š Processing AAPL: $149.6 (Message ID: e9f95bad-36d6-4bde-92f3-d7a7bdb383dc)\n",
            "âœ… Processed message 2/5\n",
            "ğŸ“Š Processing META: $304.48 (Message ID: 1b197ebd-3f03-41b7-a08f-d912743772e0)\n",
            "âœ… Processed message 3/5\n",
            "ğŸ“Š Processing META: $300.3 (Message ID: 5e89eac0-09b5-45bd-bcaf-5385964b896f)\n",
            "âœ… Processed message 4/5\n",
            "ğŸ“Š Processing AAPL: $152.46 (Message ID: 67f5940c-e5d7-4800-b27a-c7400b683c72)\n",
            "âœ… Processed message 5/5\n",
            "ğŸ“Š Consumption completed: 4 processed, 1 failed\n",
            "\n",
            "ğŸ“Š Earliest Reset Results:\n",
            "   âœ… Processed: 4 messages\n",
            "   ğŸ“ First message offset: 7\n",
            "\n",
            "ğŸ” Step 3: Testing 'latest' offset reset...\n",
            "   ğŸ“ˆ Sending data for latest consumer...\n",
            "ğŸ“ˆ Generating 10 test messages...\n",
            "ğŸ“Š Sent AAPL: $150.83 -> Partition 0, Offset 15\n",
            "ğŸ“Š Sent AMZN: $3254.54 -> Partition 2, Offset 18\n",
            "ğŸ“Š Sent MSFT: $350.78 -> Partition 0, Offset 16\n",
            "ğŸ“Š Sent NFLX: $392.67 -> Partition 1, Offset 17\n",
            "ğŸ“Š Sent CRM: $200.08 -> Partition 1, Offset 18\n",
            "ğŸ“Š Sent ADBE: $508.34 -> Partition 2, Offset 19\n",
            "ğŸ“Š Sent META: $300.04 -> Partition 0, Offset 17\n",
            "ğŸ“Š Sent TSLA: $252.32 -> Partition 2, Offset 20\n",
            "ğŸ“Š Sent TSLA: $245.57 -> Partition 2, Offset 21\n",
            "ğŸ“Š Sent NVDA: $454.96 -> Partition 2, Offset 22\n",
            "âœ… Successfully sent 10 test messages\n",
            "ğŸ”„ Starting to consume up to 5 messages...\n",
            "ğŸ“Š Processing AAPL: $150.68 (Message ID: 2a7ebd2f-ee35-41e6-9f98-1cb07fa70fa2)\n",
            "âœ… Processed message 1/5\n",
            "â° Timeout reached after 3000ms\n",
            "ğŸ“Š Consumption completed: 1 processed, 0 failed\n",
            "\n",
            "ğŸ“Š Latest Reset Results:\n",
            "   âœ… Processed: 1 messages\n",
            "   ğŸ“ First message offset: 12\n",
            "\n",
            "ğŸ” Step 4: Testing 'none' offset reset...\n",
            "ğŸ”„ Starting to consume up to 5 messages...\n",
            "âŒ Consumer error: NoOffsetForPartitionError: {TopicPartition(topic='stock-data-offsets', partition=0), TopicPartition(topic='stock-data-offsets', partition=1), TopicPartition(topic='stock-data-offsets', partition=2)}\n",
            "ğŸ“Š Consumption completed: 0 processed, 0 failed\n",
            "\n",
            "ğŸ“Š None Reset Results:\n",
            "   âœ… Processed: 0 messages\n",
            "\n",
            "ğŸ“‹ Offset Reset Strategy Analysis:\n",
            "âœ… earliest: Good for replaying all data, processing historical messages\n",
            "âœ… latest: Good for real-time processing, only new messages\n",
            "âœ… none: Good for strict offset management, fails if no committed offset\n",
            "âš ï¸ Choose strategy based on your use case and data requirements\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2: Offset Reset Strategies Demo\n",
        "print(\"ğŸ”„ Exercise 2: Testing Different Offset Reset Strategies\")\n",
        "\n",
        "# Send more test data\n",
        "print(\"\\nğŸ“ˆ Step 1: Sending additional test data...\")\n",
        "producer.send_test_data(num_messages=15)\n",
        "\n",
        "print(\"\\nğŸ” Step 2: Testing 'earliest' offset reset...\")\n",
        "# Test earliest offset reset\n",
        "earliest_consumer = AutoCommitConsumer(KAFKA_BOOTSTRAP_SERVERS, TOPIC_NAME, f\"{CONSUMER_GROUP}-earliest\")\n",
        "earliest_consumer.create_consumer(auto_offset_reset='earliest')\n",
        "earliest_processed, earliest_failed = earliest_consumer.consume_messages(max_messages=5)\n",
        "\n",
        "print(f\"\\nğŸ“Š Earliest Reset Results:\")\n",
        "print(f\"   âœ… Processed: {len(earliest_processed)} messages\")\n",
        "if earliest_processed:\n",
        "    print(f\"   ğŸ“ First message offset: {earliest_processed[0]['offset']}\")\n",
        "\n",
        "print(\"\\nğŸ” Step 3: Testing 'latest' offset reset...\")\n",
        "# Test latest offset reset - send data first, then consume\n",
        "print(\"   ğŸ“ˆ Sending data for latest consumer...\")\n",
        "producer.send_test_data(num_messages=10)\n",
        "\n",
        "latest_consumer = AutoCommitConsumer(KAFKA_BOOTSTRAP_SERVERS, TOPIC_NAME, f\"{CONSUMER_GROUP}-latest\")\n",
        "latest_consumer.create_consumer(auto_offset_reset='latest')\n",
        "latest_processed, latest_failed = latest_consumer.consume_messages(max_messages=5, timeout_ms=3000)\n",
        "\n",
        "print(f\"\\nğŸ“Š Latest Reset Results:\")\n",
        "print(f\"   âœ… Processed: {len(latest_processed)} messages\")\n",
        "if latest_processed:\n",
        "    print(f\"   ğŸ“ First message offset: {latest_processed[0]['offset']}\")\n",
        "\n",
        "print(\"\\nğŸ” Step 4: Testing 'none' offset reset...\")\n",
        "# Test none offset reset\n",
        "try:\n",
        "    none_consumer = AutoCommitConsumer(KAFKA_BOOTSTRAP_SERVERS, TOPIC_NAME, f\"{CONSUMER_GROUP}-none\")\n",
        "    none_consumer.create_consumer(auto_offset_reset='none')\n",
        "    none_processed, none_failed = none_consumer.consume_messages(max_messages=5, timeout_ms=2000)\n",
        "    \n",
        "    print(f\"\\nğŸ“Š None Reset Results:\")\n",
        "    print(f\"   âœ… Processed: {len(none_processed)} messages\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ None Reset Error: {e}\")\n",
        "    print(\"   This is expected behavior - 'none' fails when no committed offset exists\")\n",
        "\n",
        "print(\"\\nğŸ“‹ Offset Reset Strategy Analysis:\")\n",
        "print(\"âœ… earliest: Good for replaying all data, processing historical messages\")\n",
        "print(\"âœ… latest: Good for real-time processing, only new messages\")\n",
        "print(\"âœ… none: Good for strict offset management, fails if no committed offset\")\n",
        "print(\"âš ï¸ Choose strategy based on your use case and data requirements\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Consumer Failure and Recovery\n",
        "\n",
        "### ğŸ¯ **Learning Objectives:**\n",
        "- Simulate consumer failures\n",
        "- Understand offset behavior during crashes\n",
        "- Practice recovery scenarios\n",
        "- Learn about message duplication and loss\n",
        "\n",
        "### ğŸ“š **Key Concepts:**\n",
        "1. **Consumer Failure**: What happens when consumer crashes\n",
        "2. **Offset Recovery**: How offsets are managed during failures\n",
        "3. **Message Duplication**: Risk of processing same message twice\n",
        "4. **Message Loss**: Risk of losing uncommitted messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ Exercise 3: Simulating Consumer Failures and Recovery\n",
            "\n",
            "ğŸ“ˆ Step 1: Sending test data for failure simulation...\n",
            "ğŸ“ˆ Generating 20 test messages...\n",
            "ğŸ“Š Sent CRM: $202.24 -> Partition 1, Offset 19\n",
            "ğŸ“Š Sent META: $299.35 -> Partition 0, Offset 18\n",
            "ğŸ“Š Sent ADBE: $503.99 -> Partition 2, Offset 23\n",
            "ğŸ“Š Sent NVDA: $445.72 -> Partition 2, Offset 24\n",
            "ğŸ“Š Sent NFLX: $404.99 -> Partition 1, Offset 20\n",
            "ğŸ“Š Sent GOOGL: $2809.24 -> Partition 1, Offset 21\n",
            "ğŸ“Š Sent GOOGL: $2808.14 -> Partition 1, Offset 22\n",
            "ğŸ“Š Sent CRM: $199.93 -> Partition 1, Offset 23\n",
            "ğŸ“Š Sent META: $297.5 -> Partition 0, Offset 19\n",
            "ğŸ“Š Sent GOOGL: $2830.9 -> Partition 1, Offset 24\n",
            "ğŸ“Š Sent AMZN: $3247.57 -> Partition 2, Offset 25\n",
            "ğŸ“Š Sent NVDA: $441.95 -> Partition 2, Offset 26\n",
            "ğŸ“Š Sent NVDA: $441.17 -> Partition 2, Offset 27\n",
            "ğŸ“Š Sent NVDA: $444.48 -> Partition 2, Offset 28\n",
            "ğŸ“Š Sent META: $294.48 -> Partition 0, Offset 20\n",
            "ğŸ“Š Sent GOOGL: $2764.3 -> Partition 1, Offset 25\n",
            "ğŸ“Š Sent CRM: $202.65 -> Partition 1, Offset 26\n",
            "ğŸ“Š Sent TSLA: $247.08 -> Partition 2, Offset 29\n",
            "ğŸ“Š Sent NVDA: $440.75 -> Partition 2, Offset 30\n",
            "ğŸ“Š Sent META: $303.39 -> Partition 0, Offset 21\n",
            "âœ… Successfully sent 20 test messages\n",
            "\n",
            "ğŸ’¥ Step 2: Simulating consumer failure with auto commit...\n",
            "ğŸ”„ Starting to consume up to 10 messages...\n",
            "ğŸ“Š Processing MSFT: $356.81 (Message ID: d6e9cb87-9521-40ee-abff-6707b372bde0)\n",
            "âœ… Processed message 1/10\n",
            "ğŸ“Š Processing MSFT: $356.34 (Message ID: 264eaede-4c15-459f-8426-10b025cbdef7)\n",
            "âœ… Processed message 2/10\n",
            "ğŸ“Š Processing AAPL: $147.64 (Message ID: 6e6bba55-8cfb-4eb1-8da8-80a8ba07e28b)\n",
            "âŒ Failed to process message: Simulated consumer crash after processing 2 messages\n",
            "ğŸ“Š Processing MSFT: $344.87 (Message ID: 9f47f2e5-0e61-44a3-94d2-854fe0621792)\n",
            "âŒ Failed to process message: Simulated consumer crash after processing 2 messages\n",
            "ğŸ“Š Processing MSFT: $343.34 (Message ID: a3a34feb-e395-4b47-8f96-9ddb0e394830)\n",
            "âŒ Failed to process message: Simulated consumer crash after processing 2 messages\n",
            "ğŸ“Š Processing AAPL: $150.69 (Message ID: 5853af43-a691-48e5-b479-315ce1055528)\n",
            "âŒ Failed to process message: Simulated consumer crash after processing 2 messages\n",
            "ğŸ“Š Processing MSFT: $353.76 (Message ID: 5d7373bd-c285-4105-874a-28cb92838a20)\n",
            "âŒ Failed to process message: Simulated consumer crash after processing 2 messages\n",
            "ğŸ“Š Processing AAPL: $149.6 (Message ID: e9f95bad-36d6-4bde-92f3-d7a7bdb383dc)\n",
            "âŒ Failed to process message: Simulated consumer crash after processing 2 messages\n",
            "ğŸ“Š Processing META: $304.48 (Message ID: 1b197ebd-3f03-41b7-a08f-d912743772e0)\n",
            "âŒ Failed to process message: Simulated consumer crash after processing 2 messages\n",
            "ğŸ“Š Processing META: $300.3 (Message ID: 5e89eac0-09b5-45bd-bcaf-5385964b896f)\n",
            "âŒ Failed to process message: Simulated consumer crash after processing 2 messages\n",
            "ğŸ“Š Consumption completed: 2 processed, 8 failed\n",
            "\n",
            "ğŸ“Š Auto Commit Failure Results:\n",
            "   âœ… Processed before crash: 2 messages\n",
            "   âŒ Failed: 8 messages\n",
            "\n",
            "ğŸ”„ Step 3: Testing recovery with same consumer group...\n",
            "ğŸ”„ Starting to consume up to 10 messages...\n",
            "ğŸ“Š Processing META: $299.63 (Message ID: 4f73eabf-cdd7-414c-a447-4f14682bd8af)\n",
            "âœ… Processed message 1/10\n",
            "ğŸ“Š Processing AAPL: $150.68 (Message ID: 2a7ebd2f-ee35-41e6-9f98-1cb07fa70fa2)\n",
            "âœ… Processed message 2/10\n",
            "ğŸ“Š Processing META: $297.37 (Message ID: 288dbd69-288f-4126-9e2c-2b3d08f05c3f)\n",
            "âŒ Failed to process message: Simulated processing failure for message 288dbd69-288f-4126-9e2c-2b3d08f05c3f\n",
            "ğŸ“Š Processing MSFT: $356.03 (Message ID: ae5281f3-df90-42cb-8617-df22bc3b1fa3)\n",
            "âœ… Processed message 4/10\n",
            "ğŸ“Š Processing AAPL: $150.83 (Message ID: fbe37a20-f3a5-4fcb-b5c0-45f5016d9bdf)\n",
            "âœ… Processed message 5/10\n",
            "ğŸ“Š Processing MSFT: $350.78 (Message ID: 17a7dfba-c054-4d62-9d9e-d4c9bf9c7472)\n",
            "âœ… Processed message 6/10\n",
            "ğŸ“Š Processing META: $300.04 (Message ID: 544e45be-13b0-40e9-b107-243f848a0447)\n",
            "âœ… Processed message 7/10\n",
            "ğŸ“Š Processing META: $299.35 (Message ID: 53370d16-3bb3-4027-8e6d-cdc99cdb0899)\n",
            "âœ… Processed message 8/10\n",
            "ğŸ“Š Processing META: $297.5 (Message ID: 33bc0a47-58dc-4dd6-a010-7e85d24f8507)\n",
            "âœ… Processed message 9/10\n",
            "ğŸ“Š Processing META: $294.48 (Message ID: 59499632-6c0d-49ec-819c-e05b8a5e7e5c)\n",
            "âœ… Processed message 10/10\n",
            "ğŸ“Š Consumption completed: 9 processed, 1 failed\n",
            "\n",
            "ğŸ“Š Recovery Results:\n",
            "   âœ… Processed after recovery: 9 messages\n",
            "   âŒ Failed: 1 messages\n",
            "\n",
            "ğŸ’¥ Step 4: Simulating consumer failure with manual commit...\n",
            "ğŸ”„ Starting manual commit consumer (commit every 3 messages)...\n",
            "ğŸ“Š Processing MSFT: $356.81 (Message ID: d6e9cb87-9521-40ee-abff-6707b372bde0)\n",
            "âœ… Processed message 1/10\n",
            "ğŸ“Š Processing MSFT: $356.34 (Message ID: 264eaede-4c15-459f-8426-10b025cbdef7)\n",
            "âŒ Failed to process message: Simulated consumer crash before commit\n",
            "ğŸ“Š Processing AAPL: $147.64 (Message ID: 6e6bba55-8cfb-4eb1-8da8-80a8ba07e28b)\n",
            "âŒ Failed to process message: Simulated consumer crash before commit\n",
            "ğŸ“Š Processing MSFT: $344.87 (Message ID: 9f47f2e5-0e61-44a3-94d2-854fe0621792)\n",
            "âŒ Failed to process message: Simulated consumer crash before commit\n",
            "ğŸ“Š Processing MSFT: $343.34 (Message ID: a3a34feb-e395-4b47-8f96-9ddb0e394830)\n",
            "âŒ Failed to process message: Simulated consumer crash before commit\n",
            "ğŸ“Š Processing AAPL: $150.69 (Message ID: 5853af43-a691-48e5-b479-315ce1055528)\n",
            "âŒ Failed to process message: Simulated consumer crash before commit\n",
            "ğŸ“Š Processing MSFT: $353.76 (Message ID: 5d7373bd-c285-4105-874a-28cb92838a20)\n",
            "âŒ Failed to process message: Simulated consumer crash before commit\n",
            "ğŸ“Š Processing AAPL: $149.6 (Message ID: e9f95bad-36d6-4bde-92f3-d7a7bdb383dc)\n",
            "âŒ Failed to process message: Simulated consumer crash before commit\n",
            "ğŸ“Š Processing META: $304.48 (Message ID: 1b197ebd-3f03-41b7-a08f-d912743772e0)\n",
            "âŒ Failed to process message: Simulated consumer crash before commit\n",
            "ğŸ“Š Processing META: $300.3 (Message ID: 5e89eac0-09b5-45bd-bcaf-5385964b896f)\n",
            "âŒ Failed to process message: Simulated consumer crash before commit\n",
            "ğŸ’¾ Final commit completed\n",
            "ğŸ“Š Manual commit consumption completed: 1 processed, 9 failed\n",
            "\n",
            "ğŸ“Š Manual Commit Failure Results:\n",
            "   âœ… Processed before crash: 1 messages\n",
            "   âŒ Failed: 9 messages\n",
            "\n",
            "ğŸ”„ Step 5: Testing recovery with manual commit consumer...\n",
            "ğŸ”„ Starting manual commit consumer (commit every 3 messages)...\n",
            "ğŸ“Š Processing META: $299.63 (Message ID: 4f73eabf-cdd7-414c-a447-4f14682bd8af)\n",
            "âœ… Processed message 1/10\n",
            "ğŸ“Š Processing AAPL: $150.68 (Message ID: 2a7ebd2f-ee35-41e6-9f98-1cb07fa70fa2)\n",
            "âœ… Processed message 2/10\n",
            "ğŸ“Š Processing META: $297.37 (Message ID: 288dbd69-288f-4126-9e2c-2b3d08f05c3f)\n",
            "âœ… Processed message 3/10\n",
            "ğŸ’¾ Committed offsets after 3 messages\n",
            "ğŸ“Š Processing MSFT: $356.03 (Message ID: ae5281f3-df90-42cb-8617-df22bc3b1fa3)\n",
            "âœ… Processed message 4/10\n",
            "ğŸ“Š Processing AAPL: $150.83 (Message ID: fbe37a20-f3a5-4fcb-b5c0-45f5016d9bdf)\n",
            "âœ… Processed message 5/10\n",
            "ğŸ“Š Processing MSFT: $350.78 (Message ID: 17a7dfba-c054-4d62-9d9e-d4c9bf9c7472)\n",
            "âœ… Processed message 6/10\n",
            "ğŸ’¾ Committed offsets after 6 messages\n",
            "ğŸ“Š Processing META: $300.04 (Message ID: 544e45be-13b0-40e9-b107-243f848a0447)\n",
            "âœ… Processed message 7/10\n",
            "ğŸ“Š Processing META: $299.35 (Message ID: 53370d16-3bb3-4027-8e6d-cdc99cdb0899)\n",
            "âŒ Failed to process message: Simulated processing failure for message 53370d16-3bb3-4027-8e6d-cdc99cdb0899\n",
            "ğŸ“Š Processing META: $297.5 (Message ID: 33bc0a47-58dc-4dd6-a010-7e85d24f8507)\n",
            "âŒ Failed to process message: Simulated processing failure for message 33bc0a47-58dc-4dd6-a010-7e85d24f8507\n",
            "ğŸ“Š Processing META: $294.48 (Message ID: 59499632-6c0d-49ec-819c-e05b8a5e7e5c)\n",
            "âœ… Processed message 10/10\n",
            "ğŸ’¾ Final commit completed\n",
            "ğŸ“Š Manual commit consumption completed: 8 processed, 2 failed\n",
            "\n",
            "ğŸ“Š Manual Recovery Results:\n",
            "   âœ… Processed after recovery: 8 messages\n",
            "   âŒ Failed: 2 messages\n",
            "\n",
            "ğŸ“‹ Failure and Recovery Analysis:\n",
            "âœ… Auto Commit: May lose messages if crash happens before commit interval\n",
            "âœ… Manual Commit: More reliable, commits only after successful processing\n",
            "âš ï¸ Consumer failures can lead to message duplication or loss\n",
            "âš ï¸ Recovery behavior depends on commit strategy and timing\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3: Consumer Failure and Recovery Demo\n",
        "print(\"ğŸ”„ Exercise 3: Simulating Consumer Failures and Recovery\")\n",
        "\n",
        "# Send test data for failure simulation\n",
        "print(\"\\nğŸ“ˆ Step 1: Sending test data for failure simulation...\")\n",
        "producer.send_test_data(num_messages=20)\n",
        "\n",
        "print(\"\\nğŸ’¥ Step 2: Simulating consumer failure with auto commit...\")\n",
        "# Simulate failure with auto commit\n",
        "class FailingAutoConsumer(AutoCommitConsumer):\n",
        "    def process_message(self, message):\n",
        "        data = message.value\n",
        "        print(f\"ğŸ“Š Processing {data['symbol']}: ${data['close']} (Message ID: {data['message_id']})\")\n",
        "        \n",
        "        # Simulate processing time\n",
        "        time.sleep(0.1)\n",
        "        \n",
        "        # Fail after processing 3 messages\n",
        "        if len(self.processed_messages) >= 2:\n",
        "            raise Exception(f\"Simulated consumer crash after processing {len(self.processed_messages)} messages\")\n",
        "        \n",
        "        return data\n",
        "\n",
        "# Test failing auto commit consumer\n",
        "failing_auto = FailingAutoConsumer(KAFKA_BOOTSTRAP_SERVERS, TOPIC_NAME, f\"{CONSUMER_GROUP}-failing-auto\")\n",
        "failing_auto.create_consumer()\n",
        "try:\n",
        "    auto_failed_processed, auto_failed_failed = failing_auto.consume_messages(max_messages=10)\n",
        "except Exception as e:\n",
        "    print(f\"ğŸ’¥ Consumer crashed: {e}\")\n",
        "    auto_failed_processed = failing_auto.processed_messages\n",
        "    auto_failed_failed = failing_auto.failed_messages\n",
        "\n",
        "print(f\"\\nğŸ“Š Auto Commit Failure Results:\")\n",
        "print(f\"   âœ… Processed before crash: {len(auto_failed_processed)} messages\")\n",
        "print(f\"   âŒ Failed: {len(auto_failed_failed)} messages\")\n",
        "\n",
        "print(\"\\nğŸ”„ Step 3: Testing recovery with same consumer group...\")\n",
        "# Test recovery with same consumer group\n",
        "recovery_consumer = AutoCommitConsumer(KAFKA_BOOTSTRAP_SERVERS, TOPIC_NAME, f\"{CONSUMER_GROUP}-failing-auto\")\n",
        "recovery_consumer.create_consumer()\n",
        "recovery_processed, recovery_failed = recovery_consumer.consume_messages(max_messages=10)\n",
        "\n",
        "print(f\"\\nğŸ“Š Recovery Results:\")\n",
        "print(f\"   âœ… Processed after recovery: {len(recovery_processed)} messages\")\n",
        "print(f\"   âŒ Failed: {len(recovery_failed)} messages\")\n",
        "\n",
        "print(\"\\nğŸ’¥ Step 4: Simulating consumer failure with manual commit...\")\n",
        "# Simulate failure with manual commit\n",
        "class FailingManualConsumer(ManualCommitConsumer):\n",
        "    def process_message(self, message):\n",
        "        data = message.value\n",
        "        print(f\"ğŸ“Š Processing {data['symbol']}: ${data['close']} (Message ID: {data['message_id']})\")\n",
        "        \n",
        "        # Simulate processing time\n",
        "        time.sleep(0.1)\n",
        "        \n",
        "        # Fail after processing 2 messages (before commit)\n",
        "        if len(self.processed_messages) >= 1:\n",
        "            raise Exception(f\"Simulated consumer crash before commit\")\n",
        "        \n",
        "        return data\n",
        "\n",
        "# Test failing manual commit consumer\n",
        "failing_manual = FailingManualConsumer(KAFKA_BOOTSTRAP_SERVERS, TOPIC_NAME, f\"{CONSUMER_GROUP}-failing-manual\")\n",
        "failing_manual.create_consumer()\n",
        "try:\n",
        "    manual_failed_processed, manual_failed_failed = failing_manual.consume_messages(max_messages=10, commit_every=3)\n",
        "except Exception as e:\n",
        "    print(f\"ğŸ’¥ Consumer crashed: {e}\")\n",
        "    manual_failed_processed = failing_manual.processed_messages\n",
        "    manual_failed_failed = failing_manual.failed_messages\n",
        "\n",
        "print(f\"\\nğŸ“Š Manual Commit Failure Results:\")\n",
        "print(f\"   âœ… Processed before crash: {len(manual_failed_processed)} messages\")\n",
        "print(f\"   âŒ Failed: {len(manual_failed_failed)} messages\")\n",
        "\n",
        "print(\"\\nğŸ”„ Step 5: Testing recovery with manual commit consumer...\")\n",
        "# Test recovery with manual commit consumer\n",
        "manual_recovery_consumer = ManualCommitConsumer(KAFKA_BOOTSTRAP_SERVERS, TOPIC_NAME, f\"{CONSUMER_GROUP}-failing-manual\")\n",
        "manual_recovery_consumer.create_consumer()\n",
        "manual_recovery_processed, manual_recovery_failed = manual_recovery_consumer.consume_messages(max_messages=10, commit_every=3)\n",
        "\n",
        "print(f\"\\nğŸ“Š Manual Recovery Results:\")\n",
        "print(f\"   âœ… Processed after recovery: {len(manual_recovery_processed)} messages\")\n",
        "print(f\"   âŒ Failed: {len(manual_recovery_failed)} messages\")\n",
        "\n",
        "print(\"\\nğŸ“‹ Failure and Recovery Analysis:\")\n",
        "print(\"âœ… Auto Commit: May lose messages if crash happens before commit interval\")\n",
        "print(\"âœ… Manual Commit: More reliable, commits only after successful processing\")\n",
        "print(\"âš ï¸ Consumer failures can lead to message duplication or loss\")\n",
        "print(\"âš ï¸ Recovery behavior depends on commit strategy and timing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Best Practices and Cleanup\n",
        "\n",
        "### ğŸ¯ **Learning Objectives:**\n",
        "- Learn offset management best practices\n",
        "- Understand when to use each approach\n",
        "- Review key takeaways and recommendations\n",
        "- Clean up resources and consumer groups\n",
        "\n",
        "### ğŸ“š **Best Practices:**\n",
        "1. **Choose the right commit strategy** for your use case\n",
        "2. **Handle failures gracefully** with proper error handling\n",
        "3. **Monitor consumer lag** to detect processing issues\n",
        "4. **Use idempotent processing** to handle duplicates safely\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“š Offset Management Best Practices and Recommendations\n",
            "\n",
            "ğŸ¯ When to Use Each Commit Strategy:\n",
            "\n",
            "1. ğŸ”„ Auto Commit:\n",
            "   âœ… Use when: Processing is fast and failures are rare\n",
            "   âœ… Use when: Message loss is acceptable\n",
            "   âœ… Use when: Simplicity is more important than reliability\n",
            "   âŒ Avoid when: Processing is slow or complex\n",
            "   âŒ Avoid when: Message loss is not acceptable\n",
            "\n",
            "2. ğŸ’¾ Manual Commit:\n",
            "   âœ… Use when: Processing is slow or complex\n",
            "   âœ… Use when: Message loss is not acceptable\n",
            "   âœ… Use when: You need exactly-once processing\n",
            "   âœ… Use when: You need fine-grained control\n",
            "   âŒ Avoid when: Processing is very simple and fast\n",
            "\n",
            "3. ğŸ”„ Offset Reset Strategies:\n",
            "   âœ… earliest: Replay all data, historical processing\n",
            "   âœ… latest: Real-time processing, only new messages\n",
            "   âœ… none: Strict offset management, fail if no committed offset\n",
            "\n",
            "ğŸ“‹ General Best Practices:\n",
            "âœ… Always handle consumer failures gracefully\n",
            "âœ… Use idempotent processing to handle duplicates\n",
            "âœ… Monitor consumer lag to detect processing issues\n",
            "âœ… Commit offsets after successful processing\n",
            "âœ… Use appropriate timeout settings\n",
            "âœ… Test failure scenarios thoroughly\n",
            "âœ… Document your offset management strategy\n",
            "\n",
            "ğŸ¯ Key Takeaways:\n",
            "1. Offset management is crucial for reliable message processing\n",
            "2. Choose commit strategy based on your reliability requirements\n",
            "3. Manual commits provide better control but require more care\n",
            "4. Consumer failures can lead to message duplication or loss\n",
            "5. Test failure scenarios to understand recovery behavior\n",
            "6. Monitor and alert on consumer lag and processing failures\n",
            "\n",
            "ğŸ§¹ Cleanup:\n",
            "âœ… Consumer groups will be cleaned up automatically\n",
            "âœ… Offsets are managed by Kafka automatically\n",
            "âœ… No manual cleanup required for this lab\n",
            "\n",
            "âœ… Lab 4: Offset Management and Exactly-Once Processing completed!\n",
            "ğŸš€ Next: Try Lab 5: Real-time Analytics or explore advanced Kafka features\n"
          ]
        }
      ],
      "source": [
        "# Exercise 4: Best Practices and Cleanup\n",
        "print(\"ğŸ“š Offset Management Best Practices and Recommendations\")\n",
        "\n",
        "print(\"\\nğŸ¯ When to Use Each Commit Strategy:\")\n",
        "print(\"\\n1. ğŸ”„ Auto Commit:\")\n",
        "print(\"   âœ… Use when: Processing is fast and failures are rare\")\n",
        "print(\"   âœ… Use when: Message loss is acceptable\")\n",
        "print(\"   âœ… Use when: Simplicity is more important than reliability\")\n",
        "print(\"   âŒ Avoid when: Processing is slow or complex\")\n",
        "print(\"   âŒ Avoid when: Message loss is not acceptable\")\n",
        "\n",
        "print(\"\\n2. ğŸ’¾ Manual Commit:\")\n",
        "print(\"   âœ… Use when: Processing is slow or complex\")\n",
        "print(\"   âœ… Use when: Message loss is not acceptable\")\n",
        "print(\"   âœ… Use when: You need exactly-once processing\")\n",
        "print(\"   âœ… Use when: You need fine-grained control\")\n",
        "print(\"   âŒ Avoid when: Processing is very simple and fast\")\n",
        "\n",
        "print(\"\\n3. ğŸ”„ Offset Reset Strategies:\")\n",
        "print(\"   âœ… earliest: Replay all data, historical processing\")\n",
        "print(\"   âœ… latest: Real-time processing, only new messages\")\n",
        "print(\"   âœ… none: Strict offset management, fail if no committed offset\")\n",
        "\n",
        "print(\"\\nğŸ“‹ General Best Practices:\")\n",
        "print(\"âœ… Always handle consumer failures gracefully\")\n",
        "print(\"âœ… Use idempotent processing to handle duplicates\")\n",
        "print(\"âœ… Monitor consumer lag to detect processing issues\")\n",
        "print(\"âœ… Commit offsets after successful processing\")\n",
        "print(\"âœ… Use appropriate timeout settings\")\n",
        "print(\"âœ… Test failure scenarios thoroughly\")\n",
        "print(\"âœ… Document your offset management strategy\")\n",
        "\n",
        "print(\"\\nğŸ¯ Key Takeaways:\")\n",
        "print(\"1. Offset management is crucial for reliable message processing\")\n",
        "print(\"2. Choose commit strategy based on your reliability requirements\")\n",
        "print(\"3. Manual commits provide better control but require more care\")\n",
        "print(\"4. Consumer failures can lead to message duplication or loss\")\n",
        "print(\"5. Test failure scenarios to understand recovery behavior\")\n",
        "print(\"6. Monitor and alert on consumer lag and processing failures\")\n",
        "\n",
        "print(\"\\nğŸ§¹ Cleanup:\")\n",
        "print(\"âœ… Consumer groups will be cleaned up automatically\")\n",
        "print(\"âœ… Offsets are managed by Kafka automatically\")\n",
        "print(\"âœ… No manual cleanup required for this lab\")\n",
        "\n",
        "print(\"\\nâœ… Lab 4: Offset Management and Exactly-Once Processing completed!\")\n",
        "print(\"ğŸš€ Next: Try Lab 5: Real-time Analytics or explore advanced Kafka features\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datalab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
