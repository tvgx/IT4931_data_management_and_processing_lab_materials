# Data Lakehouse Lab - End-to-End Data Pipeline Integration

## ğŸ“‹ Overview

Há»‡ thá»‘ng bÃ i lab tá»•ng há»£p tÃ­ch há»£p táº¥t cáº£ cÃ¡c cÃ´ng nghá»‡ Data Engineering Ä‘Ã£ há»c:
- **Kafka**: Real-time data ingestion
- **Spark**: Big data processing
- **Iceberg**: Data lakehouse storage
- **dbt**: Data transformations
- **Great Expectations**: Data quality
- **Airflow**: Workflow orchestration

## ğŸ¯ Learning Objectives

Sau khi hoÃ n thÃ nh lab nÃ y, báº¡n sáº½ cÃ³ thá»ƒ:

- âœ… Thiáº¿t káº¿ vÃ  implement complete data lakehouse architecture
- âœ… TÃ­ch há»£p multiple technologies trong má»™t pipeline
- âœ… Build end-to-end data pipelines tá»« ingestion Ä‘áº¿n analytics
- âœ… Implement data quality checks á»Ÿ má»i stage
- âœ… Orchestrate complex workflows vá»›i Airflow
- âœ… Apply best practices cho production pipelines

## ğŸ—ï¸ Lab Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka  â”‚â”€â”€â”€â”€â–¶â”‚  Spark   â”‚â”€â”€â”€â”€â–¶â”‚ Iceberg â”‚â”€â”€â”€â”€â–¶â”‚   dbt    â”‚â”€â”€â”€â”€â–¶â”‚   GE    â”‚
â”‚(Ingest) â”‚     â”‚(Process) â”‚     â”‚(Store)  â”‚     â”‚(Transform)â”‚     â”‚(Quality)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                â”‚                â”‚                â”‚                â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚   Airflow   â”‚
                            â”‚(Orchestrate)â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Docker vÃ  Docker Compose
- Python 3.10+
- Conda hoáº·c Miniconda
- HoÃ n thÃ nh cÃ¡c labs trÆ°á»›c:
  - Kafka Lab
  - Spark Lab
  - Airflow Lab
  - dbt Lab
  - Great Expectations Lab

### Setup Steps

1. **Clone vÃ  navigate:**
```bash
cd Data_Lakehouse_lab
```

2. **Run setup script:**
```bash
chmod +x setup_lakehouse_lab.sh
./setup_lakehouse_lab.sh
```

3. **Start all services:**
```bash
docker-compose up -d
```

4. **Wait for services to be ready:**
```bash
# Check services
docker-compose ps

# Check Airflow UI: http://localhost:8080
# Check Spark Master UI: http://localhost:8081
```

5. **Open Jupyter:**
```bash
conda activate lakehouse_lab
jupyter notebook
```

## ğŸ“š Lab Content

### Lab 1: Architecture Overview
- Data Lakehouse architecture
- Technology stack overview
- Integration patterns
- Best practices

### Lab 2: Data Ingestion vá»›i Kafka
- Ingest data tá»« multiple sources
- Kafka producers vÃ  consumers
- Schema registry
- Data validation at ingestion

### Lab 3: Processing vá»›i Spark + Iceberg
- Spark Structured Streaming tá»« Kafka
- Write to Iceberg tables
- Batch processing vá»›i Spark
- Schema evolution vá»›i Iceberg

### Lab 4: Transformation vá»›i dbt
- dbt models trÃªn Iceberg tables
- Staging â†’ Intermediate â†’ Marts
- dbt transformations
- dbt tests

### Lab 5: Data Quality vá»›i Great Expectations
- GE expectations trÃªn processed data
- Checkpoints vÃ  validations
- Data quality monitoring
- Alerts vÃ  notifications

### Lab 6: Orchestration vá»›i Airflow
- Airflow DAGs cho complete pipeline
- Task dependencies
- Error handling vÃ  retries
- Monitoring vÃ  alerting

### Lab 7: End-to-End Pipeline
- Complete pipeline tá»« start to finish
- Integration testing
- Performance optimization
- Production best practices

## ğŸ³ Docker Services

Lab nÃ y cháº¡y cÃ¡c services sau:

- **Kafka**: Port 9092
- **Zookeeper**: Port 2181
- **Spark Master**: Port 8081 (UI) - Changed from 8080 to avoid conflict
- **Spark Worker**: Connected to master
- **PostgreSQL**: Port 5432 (Airflow metadata + dbt)
- **Redis**: Port 6379 (Airflow Celery broker)
- **Airflow Webserver**: Port 8080
- **Airflow Scheduler**: Background
- **Airflow Worker**: Background

## ğŸ“– Pipeline Flow

```
1. Data Sources â†’ Kafka (Ingestion)
2. Kafka â†’ Spark Streaming (Real-time processing)
3. Spark â†’ Iceberg Tables (Storage)
4. Iceberg â†’ dbt (Transformations)
5. dbt Models â†’ Great Expectations (Validation)
6. All â†’ Airflow (Orchestration)
```

## ğŸ”— Integration Points

- **Kafka â†’ Spark**: Structured Streaming
- **Spark â†’ Iceberg**: Write operations
- **Iceberg â†’ dbt**: Read tá»« Iceberg tables
- **dbt â†’ GE**: Validate dbt outputs
- **All â†’ Airflow**: Orchestrate entire pipeline

## ğŸ› Troubleshooting

### Service Issues

1. **Check all services:**
```bash
docker-compose ps
```

2. **Check logs:**
```bash
docker-compose logs <service_name>
```

3. **Restart services:**
```bash
docker-compose restart <service_name>
```

### Connection Issues

- Ensure all services are healthy before starting
- Check port conflicts
- Verify network connectivity between containers

## ğŸ“ Notes

- This lab requires significant resources (RAM, CPU)
- Start services gradually if needed
- Monitor resource usage
- Some services may take time to initialize

## ğŸ“ Next Steps

Sau khi hoÃ n thÃ nh lab nÃ y, báº¡n cÃ³ thá»ƒ:

1. Deploy similar architecture to production
2. Optimize pipeline performance
3. Scale individual components
4. Add monitoring vÃ  alerting
5. Implement CI/CD for data pipelines

---

**Happy Building! ğŸ—ï¸**

