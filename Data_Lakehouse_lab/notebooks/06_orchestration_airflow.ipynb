{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 6: Orchestration vá»›i Airflow - TÃ­ch há»£p Complete Pipeline\n",
        "\n",
        "## ðŸŽ¯ Objectives\n",
        "- Táº¡o Airflow DAGs cho complete data lakehouse pipeline\n",
        "- Orchestrate Kafka, Spark, Iceberg, dbt, vÃ  GE\n",
        "- Implement error handling vÃ  retries\n",
        "- Monitoring vÃ  alerting\n",
        "\n",
        "## ðŸ“‹ Prerequisites\n",
        "- HoÃ n thÃ nh Lab 1-5\n",
        "- Airflow Ä‘ang cháº¡y\n",
        "- Táº¥t cáº£ services Ä‘Ã£ Ä‘Æ°á»£c setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete Airflow DAG cho Data Lakehouse Pipeline\n",
        "dag_code = \"\"\"\n",
        "from airflow.sdk import DAG, task\n",
        "from airflow.operators.bash import BashOperator\n",
        "from airflow.providers.standard.operators.empty import EmptyOperator\n",
        "import pendulum\n",
        "\n",
        "@dag(\n",
        "    dag_id='data_lakehouse_pipeline',\n",
        "    schedule='@daily',\n",
        "    start_date=pendulum.datetime(2024, 1, 1, tz='UTC'),\n",
        "    catchup=False,\n",
        "    default_args={\n",
        "        'retries': 2,\n",
        "        'retry_delay': timedelta(minutes=5),\n",
        "    },\n",
        "    tags=['lakehouse', 'end-to-end'],\n",
        ")\n",
        "def data_lakehouse_pipeline():\n",
        "    start = EmptyOperator(task_id='start')\n",
        "    \n",
        "    # Stage 1: Ingest data vá»›i Kafka producer\n",
        "    ingest_task = BashOperator(\n",
        "        task_id='ingest_data',\n",
        "        bash_command='python /opt/airflow/data/scripts/kafka_producer.py',\n",
        "    )\n",
        "    \n",
        "    # Stage 2: Process vá»›i Spark Streaming\n",
        "    spark_streaming = BashOperator(\n",
        "        task_id='spark_streaming',\n",
        "        bash_command='spark-submit --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0 /opt/airflow/spark_jobs/streaming_job.py',\n",
        "    )\n",
        "    \n",
        "    # Stage 3: Transform vá»›i dbt\n",
        "    dbt_run = BashOperator(\n",
        "        task_id='dbt_transform',\n",
        "        bash_command='cd /opt/airflow/dbt_project && dbt run --profiles-dir . --project-dir .',\n",
        "    )\n",
        "    \n",
        "    # Stage 4: Validate vá»›i Great Expectations\n",
        "    ge_validate = BashOperator(\n",
        "        task_id='ge_validation',\n",
        "        bash_command='cd /opt/airflow/ge_project && great_expectations checkpoint run data_quality_checkpoint',\n",
        "    )\n",
        "    \n",
        "    # Stage 5: Generate reports\n",
        "    @task\n",
        "    def generate_report(**context):\n",
        "        print('Pipeline completed successfully!')\n",
        "        return 'Report generated'\n",
        "    \n",
        "    end = EmptyOperator(task_id='end', trigger_rule='all_done')\n",
        "    \n",
        "    # Define workflow\n",
        "    start >> ingest_task >> spark_streaming >> dbt_run >> ge_validate >> generate_report() >> end\n",
        "\n",
        "pipeline = data_lakehouse_pipeline()\n",
        "\"\"\"\n",
        "\n",
        "print(\"ðŸ“‹ Complete Airflow DAG:\")\n",
        "print(\"=\" * 60)\n",
        "print(dag_code)\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
